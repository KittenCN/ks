
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>14.8. 来自Transformers的双向编码器表示（BERT） · CoderFAN 资料库</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 6.0.3">
        <meta name="author" content="Todd Lyu">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="bert-dataset.html" />
    
    
    <link rel="prev" href="similarity-analogy.html" />
    
    <!-- MathJax 配置：唯一且完整 -->
<script>
    window.MathJax = {
      tex: {
        inlineMath:  [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        strict: "ignore",
        macros: { "\\E":"\\mathbb{E}", "\\Var":"\\operatorname{Var}" }
      },
    };
    </script>
    
    <!-- 核心脚本（defer不阻塞渲染） -->
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
    <!-- 放在 tex-chtml.js 之后 -->
    <script>
    (function () {
      function typeset() {
        if (window.MathJax && MathJax.typesetPromise) {
          MathJax.typesetPromise().catch(console.error);
        }
      }
    
      /* 第一次正文插入 */
      document.addEventListener('DOMContentLoaded', typeset);
    
      /*   关键：等待 gitbook.js 初始化成功   */
      function hookGitBook() {
        if (window.gitbook && gitbook.events) {
          gitbook.events.bind('page.change', typeset);   // 切章排版
        } else {
          /* gitbook.js 还没加载完 → 100 ms 后再试 */
          setTimeout(hookGitBook, 100);
        }
      }
      hookGitBook();   // 启动递归等待
    })();
    </script>
    
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
            
                <nav role="navigation">
                <a href="../.." class="btn"><b></b>&#128512;返回上层&#128512;</b></a>
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../d2l.md">
            
                <span>
            
                    
                    动手学深度学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../chapter_preface/">
            
                <a href="../chapter_preface/">
            
                    
                    前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../chapter_installation/">
            
                <a href="../chapter_installation/">
            
                    
                    安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../chapter_notation/">
            
                <a href="../chapter_notation/">
            
                    
                    符号
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../chapter_introduction/">
            
                <a href="../chapter_introduction/">
            
                    
                    1. 引言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../chapter_preliminaries/">
            
                <a href="../chapter_preliminaries/">
            
                    
                    2. 预备知识
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="../chapter_preliminaries/ndarray.html">
            
                <a href="../chapter_preliminaries/ndarray.html">
            
                    
                    2.1. 数据操作
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="../chapter_preliminaries/pandas.html">
            
                <a href="../chapter_preliminaries/pandas.html">
            
                    
                    2.2. 数据预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="../chapter_preliminaries/linear-algebra.html">
            
                <a href="../chapter_preliminaries/linear-algebra.html">
            
                    
                    2.3. 线性代数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="../chapter_preliminaries/calculus.html">
            
                <a href="../chapter_preliminaries/calculus.html">
            
                    
                    2.4. 微积分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.5" data-path="../chapter_preliminaries/autograd.html">
            
                <a href="../chapter_preliminaries/autograd.html">
            
                    
                    2.5. 自动微分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.6" data-path="../chapter_preliminaries/probability.html">
            
                <a href="../chapter_preliminaries/probability.html">
            
                    
                    2.6. 概率
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.7" data-path="../chapter_preliminaries/lookup-api.html">
            
                <a href="../chapter_preliminaries/lookup-api.html">
            
                    
                    2.7. 查阅文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../chapter_linear-networks/">
            
                <a href="../chapter_linear-networks/">
            
                    
                    3. 线性神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="../chapter_linear-networks/linear-regression.html">
            
                <a href="../chapter_linear-networks/linear-regression.html">
            
                    
                    3.1. 线性回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="../chapter_linear-networks/linear-regression-scratch.html">
            
                <a href="../chapter_linear-networks/linear-regression-scratch.html">
            
                    
                    3.2. 线性回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="../chapter_linear-networks/linear-regression-concise.html">
            
                <a href="../chapter_linear-networks/linear-regression-concise.html">
            
                    
                    3.3. 线性回归的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="../chapter_linear-networks/softmax-regression.html">
            
                <a href="../chapter_linear-networks/softmax-regression.html">
            
                    
                    3.4. softmax回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.5" data-path="../chapter_linear-networks/image-classification-dataset.html">
            
                <a href="../chapter_linear-networks/image-classification-dataset.html">
            
                    
                    3.5. 图像分类数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.6" data-path="../chapter_linear-networks/softmax-regression-scratch.html">
            
                <a href="../chapter_linear-networks/softmax-regression-scratch.html">
            
                    
                    3.6. softmax回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.7" data-path="../chapter_linear-networks/softmax-regression-concise.html">
            
                <a href="../chapter_linear-networks/softmax-regression-concise.html">
            
                    
                    3.7. softmax回归的简洁实现
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../chapter_multilayer-perceptrons/">
            
                <a href="../chapter_multilayer-perceptrons/">
            
                    
                    4. 多层感知机
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="../chapter_multilayer-perceptrons/mlp.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp.html">
            
                    
                    4.1. 多层感知机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                    
                    4.2. 多层感知机的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                    
                    4.3. 多层感知机的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                <a href="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                    
                    4.4. 模型选择、欠拟合和过拟合
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.5" data-path="../chapter_multilayer-perceptrons/weight-decay.html">
            
                <a href="../chapter_multilayer-perceptrons/weight-decay.html">
            
                    
                    4.5. 权重衰减
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.6" data-path="../chapter_multilayer-perceptrons/dropout.html">
            
                <a href="../chapter_multilayer-perceptrons/dropout.html">
            
                    
                    4.6. 暂退法（Dropout）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.7" data-path="../chapter_multilayer-perceptrons/backprop.html">
            
                <a href="../chapter_multilayer-perceptrons/backprop.html">
            
                    
                    4.7. 前向传播、反向传播和计算图
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.8" data-path="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                <a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                    
                    4.8. 数值稳定性和模型初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.9" data-path="../chapter_multilayer-perceptrons/environment.html">
            
                <a href="../chapter_multilayer-perceptrons/environment.html">
            
                    
                    4.9. 环境和分布偏移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.10" data-path="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                <a href="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                    
                    4.10. 实战Kaggle比赛：预测房价
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../chapter_deep-learning-computation/">
            
                <a href="../chapter_deep-learning-computation/">
            
                    
                    5. 深度学习计算
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="../chapter_deep-learning-computation/model-construction.html">
            
                <a href="../chapter_deep-learning-computation/model-construction.html">
            
                    
                    5.1. 层和块
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.2" data-path="../chapter_deep-learning-computation/parameters.html">
            
                <a href="../chapter_deep-learning-computation/parameters.html">
            
                    
                    5.2. 参数管理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.3" data-path="../chapter_deep-learning-computation/deferred-init.html">
            
                <a href="../chapter_deep-learning-computation/deferred-init.html">
            
                    
                    5.3. 延后初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.4" data-path="../chapter_deep-learning-computation/custom-layer.html">
            
                <a href="../chapter_deep-learning-computation/custom-layer.html">
            
                    
                    5.4. 自定义层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.5" data-path="../chapter_deep-learning-computation/read-write.html">
            
                <a href="../chapter_deep-learning-computation/read-write.html">
            
                    
                    5.5. 读写文件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.6" data-path="../chapter_deep-learning-computation/use-gpu.html">
            
                <a href="../chapter_deep-learning-computation/use-gpu.html">
            
                    
                    5.6. GPU计算
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../chapter_convolutional-neural-networks/">
            
                <a href="../chapter_convolutional-neural-networks/">
            
                    
                    6. 卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.9.1" data-path="../chapter_convolutional-neural-networks/why-conv.html">
            
                <a href="../chapter_convolutional-neural-networks/why-conv.html">
            
                    
                    6.1. 从全连接层到卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.2" data-path="../chapter_convolutional-neural-networks/conv-layer.html">
            
                <a href="../chapter_convolutional-neural-networks/conv-layer.html">
            
                    
                    6.2. 图像卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.3" data-path="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                <a href="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                    
                    6.3. 填充和步幅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.4" data-path="../chapter_convolutional-neural-networks/channels.html">
            
                <a href="../chapter_convolutional-neural-networks/channels.html">
            
                    
                    6.4. 多输入多输出通道
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.5" data-path="../chapter_convolutional-neural-networks/pooling.html">
            
                <a href="../chapter_convolutional-neural-networks/pooling.html">
            
                    
                    6.5. 汇聚层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.6" data-path="../chapter_convolutional-neural-networks/lenet.html">
            
                <a href="../chapter_convolutional-neural-networks/lenet.html">
            
                    
                    6.6. 卷积神经网络（LeNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../chapter_convolutional-modern/">
            
                <a href="../chapter_convolutional-modern/">
            
                    
                    7. 现代卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.10.1" data-path="../chapter_convolutional-modern/alexnet.html">
            
                <a href="../chapter_convolutional-modern/alexnet.html">
            
                    
                    7.1. 深度卷积神经网络（AlexNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.2" data-path="../chapter_convolutional-modern/vgg.html">
            
                <a href="../chapter_convolutional-modern/vgg.html">
            
                    
                    7.2. 使用块的网络（VGG）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.3" data-path="../chapter_convolutional-modern/nin.html">
            
                <a href="../chapter_convolutional-modern/nin.html">
            
                    
                    7.3. 网络中的网络（NiN）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.4" data-path="../chapter_convolutional-modern/googlenet.html">
            
                <a href="../chapter_convolutional-modern/googlenet.html">
            
                    
                    7.4. 含并行连结的网络（GoogLeNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.5" data-path="../chapter_convolutional-modern/batch-norm.html">
            
                <a href="../chapter_convolutional-modern/batch-norm.html">
            
                    
                    7.5. 批量规范化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.6" data-path="../chapter_convolutional-modern/resnet.html">
            
                <a href="../chapter_convolutional-modern/resnet.html">
            
                    
                    7.6. 残差网络（ResNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.7" data-path="../chapter_convolutional-modern/densenet.html">
            
                <a href="../chapter_convolutional-modern/densenet.html">
            
                    
                    7.7. 稠密连接网络（DenseNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../chapter_recurrent-neural-networks/">
            
                <a href="../chapter_recurrent-neural-networks/">
            
                    
                    8. 循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.11.1" data-path="../chapter_recurrent-neural-networks/sequence.html">
            
                <a href="../chapter_recurrent-neural-networks/sequence.html">
            
                    
                    8.1. 序列模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.2" data-path="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                <a href="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                    
                    8.2. 文本预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.3" data-path="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                <a href="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                    
                    8.3. 语言模型和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.4" data-path="../chapter_recurrent-neural-networks/rnn.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn.html">
            
                    
                    8.4. 循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.5" data-path="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                    
                    8.5. 循环神经网络的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.6" data-path="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                    
                    8.6. 循环神经网络的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.7" data-path="../chapter_recurrent-neural-networks/bptt.html">
            
                <a href="../chapter_recurrent-neural-networks/bptt.html">
            
                    
                    8.7. 通过时间反向传播
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../chapter_recurrent-modern/">
            
                <a href="../chapter_recurrent-modern/">
            
                    
                    9. 现代循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.12.1" data-path="../chapter_recurrent-modern/gru.html">
            
                <a href="../chapter_recurrent-modern/gru.html">
            
                    
                    9.1. 门控循环单元（GRU）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.2" data-path="../chapter_recurrent-modern/lstm.html">
            
                <a href="../chapter_recurrent-modern/lstm.html">
            
                    
                    9.2. 长短期记忆（LSTM）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.3" data-path="../chapter_recurrent-modern/deep-rnn.html">
            
                <a href="../chapter_recurrent-modern/deep-rnn.html">
            
                    
                    9.3. 深度循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.4" data-path="../chapter_recurrent-modern/bi-rnn.html">
            
                <a href="../chapter_recurrent-modern/bi-rnn.html">
            
                    
                    9.4. 双向循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.5" data-path="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                <a href="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                    
                    9.5. 机器翻译及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.6" data-path="../chapter_recurrent-modern/encoder-decoder.html">
            
                <a href="../chapter_recurrent-modern/encoder-decoder.html">
            
                    
                    9.6. 编码器—解码器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.7" data-path="../chapter_recurrent-modern/seq2seq.html">
            
                <a href="../chapter_recurrent-modern/seq2seq.html">
            
                    
                    9.7. 序列到序列学习（seq2seq）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.8" data-path="../chapter_recurrent-modern/beam-search.html">
            
                <a href="../chapter_recurrent-modern/beam-search.html">
            
                    
                    9.8. 束搜索
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="../chapter_attention-mechanisms/">
            
                <a href="../chapter_attention-mechanisms/">
            
                    
                    10. 注意力机制
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.13.1" data-path="../chapter_attention-mechanisms/attention-cues.html">
            
                <a href="../chapter_attention-mechanisms/attention-cues.html">
            
                    
                    10.1. 注意力提示
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.2" data-path="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                <a href="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                    
                    10.2. 注意力汇聚：Nadaraya-Watson 核回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.3" data-path="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                <a href="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                    
                    10.3. 注意力评分函数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.4" data-path="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                <a href="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                    
                    10.4. Bahdanau 注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.5" data-path="../chapter_attention-mechanisms/multihead-attention.html">
            
                <a href="../chapter_attention-mechanisms/multihead-attention.html">
            
                    
                    10.5. 多头注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.6" data-path="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                <a href="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                    
                    10.6. 自注意力和位置编码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.7" data-path="../chapter_attention-mechanisms/transformer.html">
            
                <a href="../chapter_attention-mechanisms/transformer.html">
            
                    
                    10.7. Transformer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.14" data-path="../chapter_optimization/">
            
                <a href="../chapter_optimization/">
            
                    
                    11. 优化算法
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.14.1" data-path="../chapter_optimization/optimization-intro.html">
            
                <a href="../chapter_optimization/optimization-intro.html">
            
                    
                    11.1. 优化与深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.2" data-path="../chapter_optimization/convexity.html">
            
                <a href="../chapter_optimization/convexity.html">
            
                    
                    11.2. 凸性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.3" data-path="../chapter_optimization/gd.html">
            
                <a href="../chapter_optimization/gd.html">
            
                    
                    11.3. 梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.4" data-path="../chapter_optimization/sgd.html">
            
                <a href="../chapter_optimization/sgd.html">
            
                    
                    11.4. 随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.5" data-path="../chapter_optimization/minibatch-sgd.html">
            
                <a href="../chapter_optimization/minibatch-sgd.html">
            
                    
                    11.5. 小批量随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.6" data-path="../chapter_optimization/momentum.html">
            
                <a href="../chapter_optimization/momentum.html">
            
                    
                    11.6. 动量法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.7" data-path="../chapter_optimization/adagrad.html">
            
                <a href="../chapter_optimization/adagrad.html">
            
                    
                    11.7. AdaGrad算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.8" data-path="../chapter_optimization/rmsprop.html">
            
                <a href="../chapter_optimization/rmsprop.html">
            
                    
                    11.8. RMSProp算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.9" data-path="../chapter_optimization/adadelta.html">
            
                <a href="../chapter_optimization/adadelta.html">
            
                    
                    11.9. Adadelta
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.10" data-path="../chapter_optimization/adam.html">
            
                <a href="../chapter_optimization/adam.html">
            
                    
                    11.10. Adam算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.11" data-path="../chapter_optimization/lr-scheduler.html">
            
                <a href="../chapter_optimization/lr-scheduler.html">
            
                    
                    11.11. 学习率调度器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.15" data-path="../chapter_computational-performance/">
            
                <a href="../chapter_computational-performance/">
            
                    
                    12. 计算性能
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.15.1" data-path="../chapter_computational-performance/hybridize.html">
            
                <a href="../chapter_computational-performance/hybridize.html">
            
                    
                    12.1. 编译器和解释器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.2" data-path="../chapter_computational-performance/async-computation.html">
            
                <a href="../chapter_computational-performance/async-computation.html">
            
                    
                    12.2. 异步计算
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.3" data-path="../chapter_computational-performance/auto-parallelism.html">
            
                <a href="../chapter_computational-performance/auto-parallelism.html">
            
                    
                    12.3. 自动并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.4" data-path="../chapter_computational-performance/hardware.html">
            
                <a href="../chapter_computational-performance/hardware.html">
            
                    
                    12.4. 硬件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.5" data-path="../chapter_computational-performance/multiple-gpus.html">
            
                <a href="../chapter_computational-performance/multiple-gpus.html">
            
                    
                    12.5. 多GPU训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.6" data-path="../chapter_computational-performance/multiple-gpus-concise.html">
            
                <a href="../chapter_computational-performance/multiple-gpus-concise.html">
            
                    
                    12.6. 多GPU的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.7" data-path="../chapter_computational-performance/parameterserver.html">
            
                <a href="../chapter_computational-performance/parameterserver.html">
            
                    
                    12.7. 参数服务器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.16" data-path="../chapter_computer-vision/">
            
                <a href="../chapter_computer-vision/">
            
                    
                    13. 计算机视觉
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.16.1" data-path="../chapter_computer-vision/image-augmentation.html">
            
                <a href="../chapter_computer-vision/image-augmentation.html">
            
                    
                    13.1. 图像增广
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.2" data-path="../chapter_computer-vision/fine-tuning.html">
            
                <a href="../chapter_computer-vision/fine-tuning.html">
            
                    
                    13.2. 微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.3" data-path="../chapter_computer-vision/bounding-box.html">
            
                <a href="../chapter_computer-vision/bounding-box.html">
            
                    
                    13.3. 目标检测和边界框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.4" data-path="../chapter_computer-vision/anchor.html">
            
                <a href="../chapter_computer-vision/anchor.html">
            
                    
                    13.4. 锚框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.5" data-path="../chapter_computer-vision/multiscale-object-detection.html">
            
                <a href="../chapter_computer-vision/multiscale-object-detection.html">
            
                    
                    13.5. 多尺度目标检测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.6" data-path="../chapter_computer-vision/object-detection-dataset.html">
            
                <a href="../chapter_computer-vision/object-detection-dataset.html">
            
                    
                    13.6. 目标检测数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.7" data-path="../chapter_computer-vision/ssd.html">
            
                <a href="../chapter_computer-vision/ssd.html">
            
                    
                    13.7. 单发多框检测（SSD）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.8" data-path="../chapter_computer-vision/rcnn.html">
            
                <a href="../chapter_computer-vision/rcnn.html">
            
                    
                    13.8. 区域卷积神经网络（R-CNN）系列
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.9" data-path="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                <a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                    
                    13.9. 语义分割和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.10" data-path="../chapter_computer-vision/transposed-conv.html">
            
                <a href="../chapter_computer-vision/transposed-conv.html">
            
                    
                    13.10. 转置卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.11" data-path="../chapter_computer-vision/fcn.html">
            
                <a href="../chapter_computer-vision/fcn.html">
            
                    
                    13.11. 全卷积网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.12" data-path="../chapter_computer-vision/neural-style.html">
            
                <a href="../chapter_computer-vision/neural-style.html">
            
                    
                    13.12. 风格迁移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.13" data-path="../chapter_computer-vision/kaggle-cifar10.html">
            
                <a href="../chapter_computer-vision/kaggle-cifar10.html">
            
                    
                    13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10.md)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.14" data-path="../chapter_computer-vision/kaggle-dog.html">
            
                <a href="../chapter_computer-vision/kaggle-dog.html">
            
                    
                    13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.17" data-path="./">
            
                <a href="./">
            
                    
                    14. 自然语言处理：预训练
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.17.1" data-path="word2vec.html">
            
                <a href="word2vec.html">
            
                    
                    14.1. 词嵌入（word2vec）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.2" data-path="approx-training.html">
            
                <a href="approx-training.html">
            
                    
                    14.2. 近似训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.3" data-path="word-embedding-dataset.html">
            
                <a href="word-embedding-dataset.html">
            
                    
                    14.3. 用于预训练词嵌入的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.4" data-path="word2vec-pretraining.html">
            
                <a href="word2vec-pretraining.html">
            
                    
                    14.4. 预训练word2vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.5" data-path="glove.html">
            
                <a href="glove.html">
            
                    
                    14.5. 全局向量的词嵌入（GloVe）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.6" data-path="subword-embedding.html">
            
                <a href="subword-embedding.html">
            
                    
                    14.6. 子词嵌入
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.7" data-path="similarity-analogy.html">
            
                <a href="similarity-analogy.html">
            
                    
                    14.7. 词的相似性和类比任务
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.17.8" data-path="bert.html">
            
                <a href="bert.html">
            
                    
                    14.8. 来自Transformers的双向编码器表示（BERT）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.9" data-path="bert-dataset.html">
            
                <a href="bert-dataset.html">
            
                    
                    14.9. 用于预训练BERT的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.10" data-path="bert-pretraining.html">
            
                <a href="bert-pretraining.html">
            
                    
                    14.10. 预训练BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.18" data-path="../chapter_natural-language-processing-applications/">
            
                <a href="../chapter_natural-language-processing-applications/">
            
                    
                    15. 自然语言处理：应用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.18.1" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                    
                    15.1. 情感分析及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.2" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                    
                    15.2. 情感分析：使用循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.3" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                    
                    15.3. 情感分析：使用卷积神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.4" data-path="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                    
                    15.4. 自然语言推断与数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.5" data-path="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                    
                    15.5. 自然语言推断：使用注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.6" data-path="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                    
                    15.6. 针对序列级和词元级应用微调BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.7" data-path="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                    
                    15.7. 自然语言推断：微调BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.19" data-path="../chapter_appendix-tools-for-deep-learning/">
            
                <a href="../chapter_appendix-tools-for-deep-learning/">
            
                    
                    16. 附录：深度学习工具
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.19.1" data-path="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                    
                    16.1. 使用Jupyter Notebook
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.2" data-path="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                    
                    16.2. 使用Amazon SageMaker
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.3" data-path="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                    
                    16.3. 使用Amazon EC2实例
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.4" data-path="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                    
                    16.4. 选择服务器和GPU
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.5" data-path="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                    
                    16.5. 为本书做贡献
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.6" data-path="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                    
                    16.6. d2l API 文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.20" data-path="../chapter_references/zreferences.html">
            
                <a href="../chapter_references/zreferences.html">
            
                    
                    参考文献
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >14.8. 来自Transformers的双向编码器表示（BERT）</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h1 id="来自transformers的双向编码器表示（bert）">来自Transformers的双向编码器表示（BERT）</h1>
<p>:label:<code>sec_bert</code></p>
<p>我们已经介绍了几种用于自然语言理解的词嵌入模型。在预训练之后，输出可以被认为是一个矩阵，其中每一行都是一个表示预定义词表中词的向量。事实上，这些词嵌入模型都是与上下文无关的。让我们先来说明这个性质。</p>
<h2 id="从上下文无关到上下文敏感">从上下文无关到上下文敏感</h2>
<p>回想一下 :numref:<code>sec_word2vec_pretraining</code>和 :numref:<code>sec_synonyms</code>中的实验。例如，word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文（如果有的话）。形式上，任何词元$x$的上下文无关表示是函数$f(x)$，其仅将$x$作为其输入。考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。例如，在“a crane is flying”（一只鹤在飞）和“a crane driver came”（一名吊车司机来了）的上下文中，“crane”一词有完全不同的含义；因此，同一个词可以根据上下文被赋予不同的表示。</p>
<p>这推动了“上下文敏感”词表示的发展，其中词的表征取决于它们的上下文。因此，词元$x$的上下文敏感表示是函数$f(x, c(x))$，其取决于$x$及其上下文$c(x)$。流行的上下文敏感表示包括TagLM（language-model-augmented sequence tagger，语言模型增强的序列标记器） :cite:<code>Peters.Ammar.Bhagavatula.ea.2017</code>、CoVe（Context Vectors，上下文向量） :cite:<code>McCann.Bradbury.Xiong.ea.2017</code>和ELMo（Embeddings from Language Models，来自语言模型的嵌入） :cite:<code>Peters.Neumann.Iyyer.ea.2018</code>。</p>
<p>例如，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示（例如GloVe）连结起来。一方面，在加入ELMo表示后，冻结了预训练的双向LSTM模型中的所有权重。另一方面，现有的监督模型是专门为给定的任务定制的。利用当时不同任务的不同最佳模型，添加ELMo改进了六种自然语言处理任务的技术水平：情感分析、自然语言推断、语义角色标注、共指消解、命名实体识别和问答。</p>
<h2 id="从特定于任务到不可知任务">从特定于任务到不可知任务</h2>
<p>尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定于任务的架构。然而，为每一个自然语言处理任务设计一个特定的架构实际上并不是一件容易的事。GPT（Generative Pre Training，生成式预训练）模型为上下文的敏感表示设计了通用的任务无关模型 :cite:<code>Radford.Narasimhan.Salimans.ea.2018</code>。GPT建立在Transformer解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推断、问答、句子相似性和分类等12项任务上进行了评估，并在对模型架构进行最小更改的情况下改善了其中9项任务的最新水平。</p>
<p>然而，由于语言模型的自回归特性，GPT只能向前看（从左到右）。在“i went to the bank to deposit cash”（我去银行存现金）和“i went to the bank to sit down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。</p>
<h2 id="bert：把两个最好的结合起来">BERT：把两个最好的结合起来</h2>
<p>如我们所见，ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双向编码，并且对于大多数的自然语言处理任务 :cite:<code>Devlin.Chang.Lee.ea.2018</code>只需要最少的架构改变。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。首先，BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型架构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。 :numref:<code>fig_elmo-gpt-bert</code> 描述了ELMo、GPT和BERT之间的差异。</p>
<p><img src="../img/elmo-gpt-bert.svg" alt="ELMo、GPT和BERT的比较"></img>
:label:<code>fig_elmo-gpt-bert</code></p>
<p>BERT进一步改进了11种自然语言处理任务的技术水平，这些任务分为以下几个大类：（1）单一文本分类（如情感分析）、（2）文本对分类（如自然语言推断）、（3）问答、（4）文本标记（如命名实体识别）。从上下文敏感的ELMo到任务不可知的GPT和BERT，它们都是在2018年提出的。概念上简单但经验上强大的自然语言深度表示预训练已经彻底改变了各种自然语言处理任务的解决方案。</p>
<p>在本章的其余部分，我们将深入了解BERT的训练前准备。当在 :numref:<code>chap_nlp_app</code>中解释自然语言处理应用时，我们将说明针对下游应用的BERT微调。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> mxnet <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">from</span> mxnet <span class="hljs-keyword">import</span> gluon, np, npx
<span class="hljs-keyword">from</span> mxnet.gluon <span class="hljs-keyword">import</span> nn

npx.set_np()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> paddle <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
<span class="hljs-keyword">import</span> paddle
<span class="hljs-keyword">from</span> paddle <span class="hljs-keyword">import</span> nn
</code></pre>
<h2 id="输入表示">输入表示</h2>
<p>:label:<code>subsec_bert_input_rep</code></p>
<p>在自然语言处理中，有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“&lt;cls&gt;”、文本序列的标记、以及特殊分隔词元“&lt;sep&gt;”的连结。当输入为文本对时，BERT输入序列是“&lt;cls&gt;”、第一个文本序列的标记、“&lt;sep&gt;”、第二个文本序列标记、以及“&lt;sep&gt;”的连结。我们将始终如一地将术语“BERT输入序列”与其他类型的“序列”区分开来。例如，一个<em>BERT输入序列</em>可以包括一个<em>文本序列</em>或两个<em>文本序列</em>。</p>
<p>为了区分文本对，根据输入序列学到的片段嵌入$\mathbf{e}_A$和$\mathbf{e}_B$分别被添加到第一序列和第二序列的词元嵌入中。对于单文本输入，仅使用$\mathbf{e}_A$。</p>
<p>下面的<code>get_tokens_and_segments</code>将一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其相应的片段索引。</p>
<pre><code class="lang-python"><span class="hljs-comment">#@tab all</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_tokens_and_segments</span><span class="hljs-params">(tokens_a, tokens_b=None)</span>:</span>
    <span class="hljs-string">"""获取输入序列的词元及其片段索引"""</span>
    tokens = [<span class="hljs-string">'&lt;cls&gt;'</span>] + tokens_a + [<span class="hljs-string">'&lt;sep&gt;'</span>]
    <span class="hljs-comment"># 0和1分别标记片段A和B</span>
    segments = [<span class="hljs-number">0</span>] * (len(tokens_a) + <span class="hljs-number">2</span>)
    <span class="hljs-keyword">if</span> tokens_b <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
        tokens += tokens_b + [<span class="hljs-string">'&lt;sep&gt;'</span>]
        segments += [<span class="hljs-number">1</span>] * (len(tokens_b) + <span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> tokens, segments
</code></pre>
<p>BERT选择Transformer编码器作为其双向架构。在Transformer编码器中常见是，位置嵌入被加入到输入序列的每个位置。然而，与原始的Transformer编码器不同，BERT使用<em>可学习的</em>位置嵌入。总之， 
:numref:<code>fig_bert-input</code>表明BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。</p>
<p><img src="../img/bert-input.svg" alt="BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和"></img>
:label:<code>fig_bert-input</code></p>
<p>下面的<code>BERTEncoder</code>类类似于 :numref:<code>sec_transformer</code>中实现的<code>TransformerEncoder</code>类。与<code>TransformerEncoder</code>不同，<code>BERTEncoder</code>使用片段嵌入和可学习的位置嵌入。</p>
<pre><code class="lang-python"><span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTEncoder</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-string">"""BERT编码器"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_layers, dropout, max_len=<span class="hljs-number">1000</span>, **kwargs)</span>:</span>
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(<span class="hljs-number">2</span>, num_hiddens)
        self.blks = nn.Sequential()
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(num_layers):
            self.blks.add(d2l.EncoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, <span class="hljs-keyword">True</span>))
        <span class="hljs-comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>
        self.pos_embedding = self.params.get(<span class="hljs-string">'pos_embedding'</span>,
                                             shape=(<span class="hljs-number">1</span>, max_len, num_hiddens))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, tokens, segments, valid_lens)</span>:</span>
        <span class="hljs-comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[<span class="hljs-number">1</span>], :]
        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:
            X = blk(X, valid_lens)
        <span class="hljs-keyword">return</span> X
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTEncoder</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">"""BERT编码器"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=<span class="hljs-number">1000</span>, key_size=<span class="hljs-number">768</span>, query_size=<span class="hljs-number">768</span>, value_size=<span class="hljs-number">768</span>,
                 **kwargs)</span>:</span>
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(<span class="hljs-number">2</span>, num_hiddens)
        self.blks = nn.Sequential()
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_layers):
            self.blks.add_module(f<span class="hljs-string">"{i}"</span>, d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span class="hljs-keyword">True</span>))
        <span class="hljs-comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>
        self.pos_embedding = nn.Parameter(torch.randn(<span class="hljs-number">1</span>, max_len,
                                                      num_hiddens))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, tokens, segments, valid_lens)</span>:</span>
        <span class="hljs-comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[<span class="hljs-number">1</span>], :]
        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:
            X = blk(X, valid_lens)
        <span class="hljs-keyword">return</span> X
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTEncoder</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-string">"""BERT编码器"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=<span class="hljs-number">1000</span>, key_size=<span class="hljs-number">768</span>, query_size=<span class="hljs-number">768</span>, value_size=<span class="hljs-number">768</span>,
                 **kwargs)</span>:</span>
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(<span class="hljs-number">2</span>, num_hiddens)
        self.blks = nn.Sequential()
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_layers):
            self.blks.add_sublayer(f<span class="hljs-string">"{i}"</span>, d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span class="hljs-keyword">True</span>))
        <span class="hljs-comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>
        x = paddle.randn([<span class="hljs-number">1</span>, max_len, num_hiddens])    
        self.pos_embedding = paddle.create_parameter(shape=x.shape, dtype=str(x.numpy().dtype),
                                                     default_initializer=paddle.nn.initializer.Assign(x))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, tokens, segments, valid_lens)</span>:</span>
        <span class="hljs-comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding[:, :X.shape[<span class="hljs-number">1</span>], :]
        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:
            X = blk(X, valid_lens)
        <span class="hljs-keyword">return</span> X
</code></pre>
<p>假设词表大小为10000，为了演示<code>BERTEncoder</code>的前向推断，让我们创建一个实例并初始化它的参数。</p>
<pre><code class="lang-python">vocab_size, num_hiddens, ffn_num_hiddens, num_heads = <span class="hljs-number">10000</span>, <span class="hljs-number">768</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">4</span>
num_layers, dropout = <span class="hljs-number">2</span>, <span class="hljs-number">0.2</span>
encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                      num_layers, dropout)
encoder.initialize()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch, paddle</span>
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = <span class="hljs-number">10000</span>, <span class="hljs-number">768</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">4</span>
norm_shape, ffn_num_input, num_layers, dropout = [<span class="hljs-number">768</span>], <span class="hljs-number">768</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.2</span>
encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
                      ffn_num_hiddens, num_heads, num_layers, dropout)
</code></pre>
<p>我们将<code>tokens</code>定义为长度为8的2个输入序列，其中每个词元是词表的索引。使用输入<code>tokens</code>的<code>BERTEncoder</code>的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数<code>num_hiddens</code>定义。此超参数通常称为Transformer编码器的<em>隐藏大小</em>（隐藏单元数）。</p>
<pre><code class="lang-python">tokens = np.random.randint(<span class="hljs-number">0</span>, vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">8</span>))
segments = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
encoded_X = encoder(tokens, segments, <span class="hljs-keyword">None</span>)
encoded_X.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
tokens = torch.randint(<span class="hljs-number">0</span>, vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">8</span>))
segments = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
encoded_X = encoder(tokens, segments, <span class="hljs-keyword">None</span>)
encoded_X.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
tokens = paddle.randint(<span class="hljs-number">0</span>, vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">8</span>))
segments = paddle.to_tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
encoded_X = encoder(tokens, segments, <span class="hljs-keyword">None</span>)
encoded_X.shape
</code></pre>
<h2 id="预训练任务">预训练任务</h2>
<p>:label:<code>subsec_bert_pretraining_tasks</code></p>
<p><code>BERTEncoder</code>的前向推断给出了输入文本的每个词元和插入的特殊标记“&lt;cls&gt;”及“&lt;seq&gt;”的BERT表示。接下来，我们将使用这些表示来计算预训练BERT的损失函数。预训练包括以下两个任务：掩蔽语言模型和下一句预测。</p>
<h3 id="掩蔽语言模型（masked-language-modeling）">掩蔽语言模型（Masked Language Modeling）</h3>
<p>:label:<code>subsec_mlm</code></p>
<p>如 :numref:<code>sec_language_model</code>所示，语言模型使用左侧的上下文预测词元。为了双向编码上下文以表示每个词元，BERT随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。此任务称为<em>掩蔽语言模型</em>。</p>
<p>在这个预训练任务中，将随机选择15%的词元作为预测的掩蔽词元。要预测一个掩蔽词元而不使用标签作弊，一个简单的方法是总是用一个特殊的“&lt;mask&gt;”替换输入序列中的词元。然而，人造特殊词元“&lt;mask&gt;”不会出现在微调中。为了避免预训练和微调之间的这种不匹配，如果为预测而屏蔽词元（例如，在“this movie is great”中选择掩蔽和预测“great”），则在输入中将其替换为：</p>
<ul>
<li>80%时间为特殊的“&lt;mask&gt;“词元（例如，“this movie is great”变为“this movie is&lt;mask&gt;”；</li>
<li>10%时间为随机词元（例如，“this movie is great”变为“this movie is drink”）；</li>
<li>10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”）。</li>
</ul>
<p>请注意，在15%的时间中，有10%的时间插入了随机词元。这种偶然的噪声鼓励BERT在其双向上下文编码中不那么偏向于掩蔽词元（尤其是当标签词元保持不变时）。</p>
<p>我们实现了下面的<code>MaskLM</code>类来预测BERT预训练的掩蔽语言模型任务中的掩蔽标记。预测使用单隐藏层的多层感知机（<code>self.mlp</code>）。在前向推断中，它需要两个输入：<code>BERTEncoder</code>的编码结果和用于预测的词元位置。输出是这些位置的预测结果。</p>
<pre><code class="lang-python"><span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MaskLM</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-string">"""BERT的掩蔽语言模型任务"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, **kwargs)</span>:</span>
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential()
        self.mlp.add(
            nn.Dense(num_hiddens, flatten=<span class="hljs-keyword">False</span>, activation=<span class="hljs-string">'relu'</span>))
        self.mlp.add(nn.LayerNorm())
        self.mlp.add(nn.Dense(vocab_size, flatten=<span class="hljs-keyword">False</span>))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X, pred_positions)</span>:</span>
        num_pred_positions = pred_positions.shape[<span class="hljs-number">1</span>]
        pred_positions = pred_positions.reshape(-<span class="hljs-number">1</span>)
        batch_size = X.shape[<span class="hljs-number">0</span>]
        batch_idx = np.arange(<span class="hljs-number">0</span>, batch_size)
        <span class="hljs-comment"># 假设batch_size=2，num_pred_positions=3</span>
        <span class="hljs-comment"># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span>
        batch_idx = np.repeat(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="hljs-number">1</span>))
        mlm_Y_hat = self.mlp(masked_X)
        <span class="hljs-keyword">return</span> mlm_Y_hat
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MaskLM</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">"""BERT的掩蔽语言模型任务"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, num_inputs=<span class="hljs-number">768</span>, **kwargs)</span>:</span>
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
                                 nn.ReLU(),
                                 nn.LayerNorm(num_hiddens),
                                 nn.Linear(num_hiddens, vocab_size))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X, pred_positions)</span>:</span>
        num_pred_positions = pred_positions.shape[<span class="hljs-number">1</span>]
        pred_positions = pred_positions.reshape(-<span class="hljs-number">1</span>)
        batch_size = X.shape[<span class="hljs-number">0</span>]
        batch_idx = torch.arange(<span class="hljs-number">0</span>, batch_size)
        <span class="hljs-comment"># 假设batch_size=2，num_pred_positions=3</span>
        <span class="hljs-comment"># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span>
        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="hljs-number">1</span>))
        mlm_Y_hat = self.mlp(masked_X)
        <span class="hljs-keyword">return</span> mlm_Y_hat
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MaskLM</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-string">"""BERT的掩蔽语言模型任务"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, num_inputs=<span class="hljs-number">768</span>, **kwargs)</span>:</span>
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
                                 nn.ReLU(),
                                 nn.LayerNorm(num_hiddens),
                                 nn.Linear(num_hiddens, vocab_size))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X, pred_positions)</span>:</span>
        num_pred_positions = pred_positions.shape[<span class="hljs-number">1</span>]
        pred_positions = pred_positions.reshape([-<span class="hljs-number">1</span>])
        batch_size = X.shape[<span class="hljs-number">0</span>]
        batch_idx = paddle.arange(<span class="hljs-number">0</span>, batch_size)
        <span class="hljs-comment"># 假设batch_size=2，num_pred_positions=3</span>
        <span class="hljs-comment"># 那么batch_idx是np.array（[0,0,0,1,1]）</span>
        batch_idx = paddle.repeat_interleave(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="hljs-number">1</span>))
        mlm_Y_hat = self.mlp(masked_X)
        <span class="hljs-keyword">return</span> mlm_Y_hat
</code></pre>
<p>为了演示<code>MaskLM</code>的前向推断，我们创建了其实例<code>mlm</code>并对其进行了初始化。回想一下，来自<code>BERTEncoder</code>的正向推断<code>encoded_X</code>表示2个BERT输入序列。我们将<code>mlm_positions</code>定义为在<code>encoded_X</code>的任一输入序列中预测的3个指示。<code>mlm</code>的前向推断返回<code>encoded_X</code>的所有掩蔽位置<code>mlm_positions</code>处的预测结果<code>mlm_Y_hat</code>。对于每个预测，结果的大小等于词表的大小。</p>
<pre><code class="lang-python">mlm = MaskLM(vocab_size, num_hiddens)
mlm.initialize()
mlm_positions = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = paddle.to_tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
</code></pre>
<p>通过掩码下的预测词元<code>mlm_Y</code>的真实标签<code>mlm_Y_hat</code>，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。</p>
<pre><code class="lang-python">mlm_Y = np.array([[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>]])
loss = gluon.loss.SoftmaxCrossEntropyLoss()
mlm_l = loss(mlm_Y_hat.reshape((-<span class="hljs-number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="hljs-number">1</span>))
mlm_l.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
mlm_Y = torch.tensor([[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>]])
loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">'none'</span>)
mlm_l = loss(mlm_Y_hat.reshape((-<span class="hljs-number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="hljs-number">1</span>))
mlm_l.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
mlm_Y = paddle.to_tensor([[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>]])
loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">'none'</span>)
mlm_l = loss(mlm_Y_hat.reshape((-<span class="hljs-number">1</span>, vocab_size)), mlm_Y.reshape([-<span class="hljs-number">1</span>]))
mlm_l.shape
</code></pre>
<h3 id="下一句预测（next-sentence-prediction）">下一句预测（Next Sentence Prediction）</h3>
<p>:label:<code>subsec_nsp</code></p>
<p>尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类任务——<em>下一句预测</em>。在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。</p>
<p>下面的<code>NextSentencePred</code>类使用单隐藏层的多层感知机来预测第二个句子是否是BERT输入序列中第一个句子的下一个句子。由于Transformer编码器中的自注意力，特殊词元“&lt;cls&gt;”的BERT表示已经对输入的两个句子进行了编码。因此，多层感知机分类器的输出层（<code>self.output</code>）以<code>X</code>作为输入，其中<code>X</code>是多层感知机隐藏层的输出，而MLP隐藏层的输入是编码后的“&lt;cls&gt;”词元。</p>
<pre><code class="lang-python"><span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NextSentencePred</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-string">"""BERT的下一句预测任务"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Dense(<span class="hljs-number">2</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># X的形状：(batchsize，num_hiddens)</span>
        <span class="hljs-keyword">return</span> self.output(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NextSentencePred</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">"""BERT的下一句预测任务"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_inputs, **kwargs)</span>:</span>
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Linear(num_inputs, <span class="hljs-number">2</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># X的形状：(batchsize,num_hiddens)</span>
        <span class="hljs-keyword">return</span> self.output(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NextSentencePred</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-string">"""BERT的下一句预测任务"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_inputs, **kwargs)</span>:</span>
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Linear(num_inputs, <span class="hljs-number">2</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># X的形状：(batchsize,num_hiddens)</span>
        <span class="hljs-keyword">return</span> self.output(X)
</code></pre>
<p>我们可以看到，<code>NextSentencePred</code>实例的前向推断返回每个BERT输入序列的二分类预测。</p>
<pre><code class="lang-python">nsp = NextSentencePred()
nsp.initialize()
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
encoded_X = torch.flatten(encoded_X, start_dim=<span class="hljs-number">1</span>)
<span class="hljs-comment"># NSP的输入形状:(batchsize，num_hiddens)</span>
nsp = NextSentencePred(encoded_X.shape[-<span class="hljs-number">1</span>])
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
encoded_X = paddle.flatten(encoded_X, start_axis=<span class="hljs-number">1</span>)
<span class="hljs-comment"># NSP的输入形状:(batchsize，num_hiddens)</span>
nsp = NextSentencePred(encoded_X.shape[-<span class="hljs-number">1</span>])
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
</code></pre>
<p>还可以计算两个二元分类的交叉熵损失。</p>
<pre><code class="lang-python">nsp_y = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
nsp_y = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
nsp_y = paddle.to_tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape
</code></pre>
<p>值得注意的是，上述两个预训练任务中的所有标签都可以从预训练语料库中获得，而无需人工标注。原始的BERT已经在图书语料库 :cite:<code>Zhu.Kiros.Zemel.ea.2015</code>和英文维基百科的连接上进行了预训练。这两个文本语料库非常庞大：它们分别有8亿个单词和25亿个单词。</p>
<h2 id="整合代码">整合代码</h2>
<p>在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。现在我们可以通过实例化三个类<code>BERTEncoder</code>、<code>MaskLM</code>和<code>NextSentencePred</code>来定义<code>BERTModel</code>类。前向推断返回编码后的BERT表示<code>encoded_X</code>、掩蔽语言模型预测<code>mlm_Y_hat</code>和下一句预测<code>nsp_Y_hat</code>。</p>
<pre><code class="lang-python"><span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTModel</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-string">"""BERT模型"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_layers, dropout, max_len=<span class="hljs-number">1000</span>)</span>:</span>
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
                                   num_heads, num_layers, dropout, max_len)
        self.hidden = nn.Dense(num_hiddens, activation=<span class="hljs-string">'tanh'</span>)
        self.mlm = MaskLM(vocab_size, num_hiddens)
        self.nsp = NextSentencePred()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, tokens, segments, valid_lens=None, 
                pred_positions=None)</span>:</span>
        encoded_X = self.encoder(tokens, segments, valid_lens)
        <span class="hljs-keyword">if</span> pred_positions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        <span class="hljs-keyword">else</span>:
            mlm_Y_hat = <span class="hljs-keyword">None</span>
        <span class="hljs-comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span>
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="hljs-number">0</span>, :]))
        <span class="hljs-keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTModel</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">"""BERT模型"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=<span class="hljs-number">1000</span>, key_size=<span class="hljs-number">768</span>, query_size=<span class="hljs-number">768</span>, value_size=<span class="hljs-number">768</span>,
                 hid_in_features=<span class="hljs-number">768</span>, mlm_in_features=<span class="hljs-number">768</span>,
                 nsp_in_features=<span class="hljs-number">768</span>)</span>:</span>
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                    dropout, max_len=max_len, key_size=key_size,
                    query_size=query_size, value_size=value_size)
        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
                                    nn.Tanh())
        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
        self.nsp = NextSentencePred(nsp_in_features)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, tokens, segments, valid_lens=None, 
                pred_positions=None)</span>:</span>
        encoded_X = self.encoder(tokens, segments, valid_lens)
        <span class="hljs-keyword">if</span> pred_positions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        <span class="hljs-keyword">else</span>:
            mlm_Y_hat = <span class="hljs-keyword">None</span>
        <span class="hljs-comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span>
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="hljs-number">0</span>, :]))
        <span class="hljs-keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTModel</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-string">"""BERT模型"""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=<span class="hljs-number">1000</span>, key_size=<span class="hljs-number">768</span>, query_size=<span class="hljs-number">768</span>, value_size=<span class="hljs-number">768</span>,
                 hid_in_features=<span class="hljs-number">768</span>, mlm_in_features=<span class="hljs-number">768</span>,
                 nsp_in_features=<span class="hljs-number">768</span>)</span>:</span>
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                    dropout, max_len=max_len, key_size=key_size,
                    query_size=query_size, value_size=value_size)
        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
                                    nn.Tanh())
        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
        self.nsp = NextSentencePred(nsp_in_features)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, tokens, segments, valid_lens=None,
                pred_positions=None)</span>:</span>
        encoded_X = self.encoder(tokens, segments, valid_lens)
        <span class="hljs-keyword">if</span> pred_positions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        <span class="hljs-keyword">else</span>:
            mlm_Y_hat = <span class="hljs-keyword">None</span>
        <span class="hljs-comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span>
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="hljs-number">0</span>, :]))
        <span class="hljs-keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat
</code></pre>
<h2 id="小结">小结</h2>
<ul>
<li>word2vec和GloVe等词嵌入模型与上下文无关。它们将相同的预训练向量赋给同一个词，而不考虑词的上下文（如果有的话）。它们很难处理好自然语言中的一词多义或复杂语义。</li>
<li>对于上下文敏感的词表示，如ELMo和GPT，词的表示依赖于它们的上下文。</li>
<li>ELMo对上下文进行双向编码，但使用特定于任务的架构（然而，为每个自然语言处理任务设计一个特定的体系架构实际上并不容易）；而GPT是任务无关的，但是从左到右编码上下文。</li>
<li>BERT结合了这两个方面的优点：它对上下文进行双向编码，并且需要对大量自然语言处理任务进行最小的架构更改。</li>
<li>BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。</li>
<li>预训练包括两个任务：掩蔽语言模型和下一句预测。前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。</li>
</ul>
<h2 id="练习">练习</h2>
<ol>
<li>为什么BERT成功了？</li>
<li>在所有其他条件相同的情况下，掩蔽语言模型比从左到右的语言模型需要更多或更少的预训练步骤来收敛吗？为什么？</li>
<li>在BERT的原始实现中，<code>BERTEncoder</code>中的位置前馈网络（通过<code>d2l.EncoderBlock</code>）和<code>MaskLM</code>中的全连接层都使用高斯误差线性单元（Gaussian error linear unit，GELU） :cite:<code>Hendrycks.Gimpel.2016</code>作为激活函数。研究GELU与ReLU之间的差异。</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/5749" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/5750" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>paddle</code>
<a href="https://discuss.d2l.ai/t/11820" target="_blank">Discussions</a>
:end_tab:</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="similarity-analogy.html" class="navigation navigation-prev " aria-label="Previous page: 14.7. 词的相似性和类比任务">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="bert-dataset.html" class="navigation navigation-next " aria-label="Next page: 14.9. 用于预训练BERT的数据集">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"14.8. 来自Transformers的双向编码器表示（BERT）","level":"1.2.17.8","depth":3,"next":{"title":"14.9. 用于预训练BERT的数据集","level":"1.2.17.9","depth":3,"path":"chapter_natural-language-processing-pretraining/bert-dataset.md","ref":"chapter_natural-language-processing-pretraining/bert-dataset.md","articles":[]},"previous":{"title":"14.7. 词的相似性和类比任务","level":"1.2.17.7","depth":3,"path":"chapter_natural-language-processing-pretraining/similarity-analogy.md","ref":"chapter_natural-language-processing-pretraining/similarity-analogy.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-search","-livereload","-lunr","-fontsettings","highlight","expandable-chapters-small","back-to-top-button","github","code","theme-default"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"highlight":{"lang":{"eval_rst":"rst","toc":"text"}},"github":{"url":"https://github.com/KittenCN"},"expandable-chapters-small":{},"back-to-top-button":{},"code":{"copyButtons":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"Todd Lyu","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"CoderFAN 资料库","gitbook":"*"},"file":{"path":"chapter_natural-language-processing-pretraining/bert.md","mtime":"2024-01-19T05:51:47.447Z","type":"markdown"},"gitbook":{"version":"6.0.3","time":"2025-05-05T05:19:45.706Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    

    </body>
</html>

