
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>2.6. 概率 · CoderFAN 资料库</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 6.0.3">
        <meta name="author" content="Todd Lyu">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="lookup-api.html" />
    
    
    <link rel="prev" href="autograd.html" />
    
    <!-- MathJax 配置：唯一且完整 -->
<script>
    window.MathJax = {
      tex: {
        inlineMath:  [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        strict: "ignore",
        macros: { "\\E":"\\mathbb{E}", "\\Var":"\\operatorname{Var}" }
      },
    };
    </script>
    
    <!-- 核心脚本（defer不阻塞渲染） -->
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
    <!-- 放在 tex-chtml.js 之后 -->
    <script>
    (function () {
      function typeset() {
        if (window.MathJax && MathJax.typesetPromise) {
          MathJax.typesetPromise().catch(console.error);
        }
      }
    
      /* 第一次正文插入 */
      document.addEventListener('DOMContentLoaded', typeset);
    
      /*   关键：等待 gitbook.js 初始化成功   */
      function hookGitBook() {
        if (window.gitbook && gitbook.events) {
          gitbook.events.bind('page.change', typeset);   // 切章排版
        } else {
          /* gitbook.js 还没加载完 → 100 ms 后再试 */
          setTimeout(hookGitBook, 100);
        }
      }
      hookGitBook();   // 启动递归等待
    })();
    </script>
    
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
            
                <nav role="navigation">
                <a href="../.." class="btn"><b></b>&#128512;返回上层&#128512;</b></a>
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../d2l.md">
            
                <span>
            
                    
                    动手学深度学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../chapter_preface/">
            
                <a href="../chapter_preface/">
            
                    
                    前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../chapter_installation/">
            
                <a href="../chapter_installation/">
            
                    
                    安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../chapter_notation/">
            
                <a href="../chapter_notation/">
            
                    
                    符号
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../chapter_introduction/">
            
                <a href="../chapter_introduction/">
            
                    
                    1. 引言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="./">
            
                <a href="./">
            
                    
                    2. 预备知识
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="ndarray.html">
            
                <a href="ndarray.html">
            
                    
                    2.1. 数据操作
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="pandas.html">
            
                <a href="pandas.html">
            
                    
                    2.2. 数据预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="linear-algebra.html">
            
                <a href="linear-algebra.html">
            
                    
                    2.3. 线性代数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="calculus.html">
            
                <a href="calculus.html">
            
                    
                    2.4. 微积分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.5" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    2.5. 自动微分
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.5.6" data-path="probability.html">
            
                <a href="probability.html">
            
                    
                    2.6. 概率
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.7" data-path="lookup-api.html">
            
                <a href="lookup-api.html">
            
                    
                    2.7. 查阅文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../chapter_linear-networks/">
            
                <a href="../chapter_linear-networks/">
            
                    
                    3. 线性神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="../chapter_linear-networks/linear-regression.html">
            
                <a href="../chapter_linear-networks/linear-regression.html">
            
                    
                    3.1. 线性回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="../chapter_linear-networks/linear-regression-scratch.html">
            
                <a href="../chapter_linear-networks/linear-regression-scratch.html">
            
                    
                    3.2. 线性回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="../chapter_linear-networks/linear-regression-concise.html">
            
                <a href="../chapter_linear-networks/linear-regression-concise.html">
            
                    
                    3.3. 线性回归的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="../chapter_linear-networks/softmax-regression.html">
            
                <a href="../chapter_linear-networks/softmax-regression.html">
            
                    
                    3.4. softmax回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.5" data-path="../chapter_linear-networks/image-classification-dataset.html">
            
                <a href="../chapter_linear-networks/image-classification-dataset.html">
            
                    
                    3.5. 图像分类数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.6" data-path="../chapter_linear-networks/softmax-regression-scratch.html">
            
                <a href="../chapter_linear-networks/softmax-regression-scratch.html">
            
                    
                    3.6. softmax回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.7" data-path="../chapter_linear-networks/softmax-regression-concise.html">
            
                <a href="../chapter_linear-networks/softmax-regression-concise.html">
            
                    
                    3.7. softmax回归的简洁实现
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../chapter_multilayer-perceptrons/">
            
                <a href="../chapter_multilayer-perceptrons/">
            
                    
                    4. 多层感知机
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="../chapter_multilayer-perceptrons/mlp.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp.html">
            
                    
                    4.1. 多层感知机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                    
                    4.2. 多层感知机的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                    
                    4.3. 多层感知机的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                <a href="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                    
                    4.4. 模型选择、欠拟合和过拟合
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.5" data-path="../chapter_multilayer-perceptrons/weight-decay.html">
            
                <a href="../chapter_multilayer-perceptrons/weight-decay.html">
            
                    
                    4.5. 权重衰减
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.6" data-path="../chapter_multilayer-perceptrons/dropout.html">
            
                <a href="../chapter_multilayer-perceptrons/dropout.html">
            
                    
                    4.6. 暂退法（Dropout）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.7" data-path="../chapter_multilayer-perceptrons/backprop.html">
            
                <a href="../chapter_multilayer-perceptrons/backprop.html">
            
                    
                    4.7. 前向传播、反向传播和计算图
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.8" data-path="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                <a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                    
                    4.8. 数值稳定性和模型初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.9" data-path="../chapter_multilayer-perceptrons/environment.html">
            
                <a href="../chapter_multilayer-perceptrons/environment.html">
            
                    
                    4.9. 环境和分布偏移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.10" data-path="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                <a href="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                    
                    4.10. 实战Kaggle比赛：预测房价
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../chapter_deep-learning-computation/">
            
                <a href="../chapter_deep-learning-computation/">
            
                    
                    5. 深度学习计算
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="../chapter_deep-learning-computation/model-construction.html">
            
                <a href="../chapter_deep-learning-computation/model-construction.html">
            
                    
                    5.1. 层和块
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.2" data-path="../chapter_deep-learning-computation/parameters.html">
            
                <a href="../chapter_deep-learning-computation/parameters.html">
            
                    
                    5.2. 参数管理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.3" data-path="../chapter_deep-learning-computation/deferred-init.html">
            
                <a href="../chapter_deep-learning-computation/deferred-init.html">
            
                    
                    5.3. 延后初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.4" data-path="../chapter_deep-learning-computation/custom-layer.html">
            
                <a href="../chapter_deep-learning-computation/custom-layer.html">
            
                    
                    5.4. 自定义层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.5" data-path="../chapter_deep-learning-computation/read-write.html">
            
                <a href="../chapter_deep-learning-computation/read-write.html">
            
                    
                    5.5. 读写文件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.6" data-path="../chapter_deep-learning-computation/use-gpu.html">
            
                <a href="../chapter_deep-learning-computation/use-gpu.html">
            
                    
                    5.6. GPU计算
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../chapter_convolutional-neural-networks/">
            
                <a href="../chapter_convolutional-neural-networks/">
            
                    
                    6. 卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.9.1" data-path="../chapter_convolutional-neural-networks/why-conv.html">
            
                <a href="../chapter_convolutional-neural-networks/why-conv.html">
            
                    
                    6.1. 从全连接层到卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.2" data-path="../chapter_convolutional-neural-networks/conv-layer.html">
            
                <a href="../chapter_convolutional-neural-networks/conv-layer.html">
            
                    
                    6.2. 图像卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.3" data-path="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                <a href="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                    
                    6.3. 填充和步幅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.4" data-path="../chapter_convolutional-neural-networks/channels.html">
            
                <a href="../chapter_convolutional-neural-networks/channels.html">
            
                    
                    6.4. 多输入多输出通道
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.5" data-path="../chapter_convolutional-neural-networks/pooling.html">
            
                <a href="../chapter_convolutional-neural-networks/pooling.html">
            
                    
                    6.5. 汇聚层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.6" data-path="../chapter_convolutional-neural-networks/lenet.html">
            
                <a href="../chapter_convolutional-neural-networks/lenet.html">
            
                    
                    6.6. 卷积神经网络（LeNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../chapter_convolutional-modern/">
            
                <a href="../chapter_convolutional-modern/">
            
                    
                    7. 现代卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.10.1" data-path="../chapter_convolutional-modern/alexnet.html">
            
                <a href="../chapter_convolutional-modern/alexnet.html">
            
                    
                    7.1. 深度卷积神经网络（AlexNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.2" data-path="../chapter_convolutional-modern/vgg.html">
            
                <a href="../chapter_convolutional-modern/vgg.html">
            
                    
                    7.2. 使用块的网络（VGG）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.3" data-path="../chapter_convolutional-modern/nin.html">
            
                <a href="../chapter_convolutional-modern/nin.html">
            
                    
                    7.3. 网络中的网络（NiN）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.4" data-path="../chapter_convolutional-modern/googlenet.html">
            
                <a href="../chapter_convolutional-modern/googlenet.html">
            
                    
                    7.4. 含并行连结的网络（GoogLeNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.5" data-path="../chapter_convolutional-modern/batch-norm.html">
            
                <a href="../chapter_convolutional-modern/batch-norm.html">
            
                    
                    7.5. 批量规范化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.6" data-path="../chapter_convolutional-modern/resnet.html">
            
                <a href="../chapter_convolutional-modern/resnet.html">
            
                    
                    7.6. 残差网络（ResNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.7" data-path="../chapter_convolutional-modern/densenet.html">
            
                <a href="../chapter_convolutional-modern/densenet.html">
            
                    
                    7.7. 稠密连接网络（DenseNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../chapter_recurrent-neural-networks/">
            
                <a href="../chapter_recurrent-neural-networks/">
            
                    
                    8. 循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.11.1" data-path="../chapter_recurrent-neural-networks/sequence.html">
            
                <a href="../chapter_recurrent-neural-networks/sequence.html">
            
                    
                    8.1. 序列模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.2" data-path="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                <a href="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                    
                    8.2. 文本预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.3" data-path="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                <a href="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                    
                    8.3. 语言模型和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.4" data-path="../chapter_recurrent-neural-networks/rnn.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn.html">
            
                    
                    8.4. 循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.5" data-path="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                    
                    8.5. 循环神经网络的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.6" data-path="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                    
                    8.6. 循环神经网络的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.7" data-path="../chapter_recurrent-neural-networks/bptt.html">
            
                <a href="../chapter_recurrent-neural-networks/bptt.html">
            
                    
                    8.7. 通过时间反向传播
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../chapter_recurrent-modern/">
            
                <a href="../chapter_recurrent-modern/">
            
                    
                    9. 现代循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.12.1" data-path="../chapter_recurrent-modern/gru.html">
            
                <a href="../chapter_recurrent-modern/gru.html">
            
                    
                    9.1. 门控循环单元（GRU）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.2" data-path="../chapter_recurrent-modern/lstm.html">
            
                <a href="../chapter_recurrent-modern/lstm.html">
            
                    
                    9.2. 长短期记忆（LSTM）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.3" data-path="../chapter_recurrent-modern/deep-rnn.html">
            
                <a href="../chapter_recurrent-modern/deep-rnn.html">
            
                    
                    9.3. 深度循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.4" data-path="../chapter_recurrent-modern/bi-rnn.html">
            
                <a href="../chapter_recurrent-modern/bi-rnn.html">
            
                    
                    9.4. 双向循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.5" data-path="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                <a href="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                    
                    9.5. 机器翻译及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.6" data-path="../chapter_recurrent-modern/encoder-decoder.html">
            
                <a href="../chapter_recurrent-modern/encoder-decoder.html">
            
                    
                    9.6. 编码器—解码器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.7" data-path="../chapter_recurrent-modern/seq2seq.html">
            
                <a href="../chapter_recurrent-modern/seq2seq.html">
            
                    
                    9.7. 序列到序列学习（seq2seq）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.8" data-path="../chapter_recurrent-modern/beam-search.html">
            
                <a href="../chapter_recurrent-modern/beam-search.html">
            
                    
                    9.8. 束搜索
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="../chapter_attention-mechanisms/">
            
                <a href="../chapter_attention-mechanisms/">
            
                    
                    10. 注意力机制
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.13.1" data-path="../chapter_attention-mechanisms/attention-cues.html">
            
                <a href="../chapter_attention-mechanisms/attention-cues.html">
            
                    
                    10.1. 注意力提示
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.2" data-path="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                <a href="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                    
                    10.2. 注意力汇聚：Nadaraya-Watson 核回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.3" data-path="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                <a href="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                    
                    10.3. 注意力评分函数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.4" data-path="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                <a href="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                    
                    10.4. Bahdanau 注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.5" data-path="../chapter_attention-mechanisms/multihead-attention.html">
            
                <a href="../chapter_attention-mechanisms/multihead-attention.html">
            
                    
                    10.5. 多头注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.6" data-path="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                <a href="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                    
                    10.6. 自注意力和位置编码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.7" data-path="../chapter_attention-mechanisms/transformer.html">
            
                <a href="../chapter_attention-mechanisms/transformer.html">
            
                    
                    10.7. Transformer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.14" data-path="../chapter_optimization/">
            
                <a href="../chapter_optimization/">
            
                    
                    11. 优化算法
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.14.1" data-path="../chapter_optimization/optimization-intro.html">
            
                <a href="../chapter_optimization/optimization-intro.html">
            
                    
                    11.1. 优化与深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.2" data-path="../chapter_optimization/convexity.html">
            
                <a href="../chapter_optimization/convexity.html">
            
                    
                    11.2. 凸性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.3" data-path="../chapter_optimization/gd.html">
            
                <a href="../chapter_optimization/gd.html">
            
                    
                    11.3. 梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.4" data-path="../chapter_optimization/sgd.html">
            
                <a href="../chapter_optimization/sgd.html">
            
                    
                    11.4. 随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.5" data-path="../chapter_optimization/minibatch-sgd.html">
            
                <a href="../chapter_optimization/minibatch-sgd.html">
            
                    
                    11.5. 小批量随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.6" data-path="../chapter_optimization/momentum.html">
            
                <a href="../chapter_optimization/momentum.html">
            
                    
                    11.6. 动量法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.7" data-path="../chapter_optimization/adagrad.html">
            
                <a href="../chapter_optimization/adagrad.html">
            
                    
                    11.7. AdaGrad算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.8" data-path="../chapter_optimization/rmsprop.html">
            
                <a href="../chapter_optimization/rmsprop.html">
            
                    
                    11.8. RMSProp算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.9" data-path="../chapter_optimization/adadelta.html">
            
                <a href="../chapter_optimization/adadelta.html">
            
                    
                    11.9. Adadelta
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.10" data-path="../chapter_optimization/adam.html">
            
                <a href="../chapter_optimization/adam.html">
            
                    
                    11.10. Adam算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.11" data-path="../chapter_optimization/lr-scheduler.html">
            
                <a href="../chapter_optimization/lr-scheduler.html">
            
                    
                    11.11. 学习率调度器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.15" data-path="../chapter_computational-performance/">
            
                <a href="../chapter_computational-performance/">
            
                    
                    12. 计算性能
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.15.1" data-path="../chapter_computational-performance/hybridize.html">
            
                <a href="../chapter_computational-performance/hybridize.html">
            
                    
                    12.1. 编译器和解释器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.2" data-path="../chapter_computational-performance/async-computation.html">
            
                <a href="../chapter_computational-performance/async-computation.html">
            
                    
                    12.2. 异步计算
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.3" data-path="../chapter_computational-performance/auto-parallelism.html">
            
                <a href="../chapter_computational-performance/auto-parallelism.html">
            
                    
                    12.3. 自动并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.4" data-path="../chapter_computational-performance/hardware.html">
            
                <a href="../chapter_computational-performance/hardware.html">
            
                    
                    12.4. 硬件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.5" data-path="../chapter_computational-performance/multiple-gpus.html">
            
                <a href="../chapter_computational-performance/multiple-gpus.html">
            
                    
                    12.5. 多GPU训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.6" data-path="../chapter_computational-performance/multiple-gpus-concise.html">
            
                <a href="../chapter_computational-performance/multiple-gpus-concise.html">
            
                    
                    12.6. 多GPU的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.7" data-path="../chapter_computational-performance/parameterserver.html">
            
                <a href="../chapter_computational-performance/parameterserver.html">
            
                    
                    12.7. 参数服务器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.16" data-path="../chapter_computer-vision/">
            
                <a href="../chapter_computer-vision/">
            
                    
                    13. 计算机视觉
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.16.1" data-path="../chapter_computer-vision/image-augmentation.html">
            
                <a href="../chapter_computer-vision/image-augmentation.html">
            
                    
                    13.1. 图像增广
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.2" data-path="../chapter_computer-vision/fine-tuning.html">
            
                <a href="../chapter_computer-vision/fine-tuning.html">
            
                    
                    13.2. 微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.3" data-path="../chapter_computer-vision/bounding-box.html">
            
                <a href="../chapter_computer-vision/bounding-box.html">
            
                    
                    13.3. 目标检测和边界框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.4" data-path="../chapter_computer-vision/anchor.html">
            
                <a href="../chapter_computer-vision/anchor.html">
            
                    
                    13.4. 锚框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.5" data-path="../chapter_computer-vision/multiscale-object-detection.html">
            
                <a href="../chapter_computer-vision/multiscale-object-detection.html">
            
                    
                    13.5. 多尺度目标检测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.6" data-path="../chapter_computer-vision/object-detection-dataset.html">
            
                <a href="../chapter_computer-vision/object-detection-dataset.html">
            
                    
                    13.6. 目标检测数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.7" data-path="../chapter_computer-vision/ssd.html">
            
                <a href="../chapter_computer-vision/ssd.html">
            
                    
                    13.7. 单发多框检测（SSD）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.8" data-path="../chapter_computer-vision/rcnn.html">
            
                <a href="../chapter_computer-vision/rcnn.html">
            
                    
                    13.8. 区域卷积神经网络（R-CNN）系列
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.9" data-path="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                <a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                    
                    13.9. 语义分割和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.10" data-path="../chapter_computer-vision/transposed-conv.html">
            
                <a href="../chapter_computer-vision/transposed-conv.html">
            
                    
                    13.10. 转置卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.11" data-path="../chapter_computer-vision/fcn.html">
            
                <a href="../chapter_computer-vision/fcn.html">
            
                    
                    13.11. 全卷积网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.12" data-path="../chapter_computer-vision/neural-style.html">
            
                <a href="../chapter_computer-vision/neural-style.html">
            
                    
                    13.12. 风格迁移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.13" data-path="../chapter_computer-vision/kaggle-cifar10.html">
            
                <a href="../chapter_computer-vision/kaggle-cifar10.html">
            
                    
                    13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10.md)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.14" data-path="../chapter_computer-vision/kaggle-dog.html">
            
                <a href="../chapter_computer-vision/kaggle-dog.html">
            
                    
                    13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.17" data-path="../chapter_natural-language-processing-pretraining/">
            
                <a href="../chapter_natural-language-processing-pretraining/">
            
                    
                    14. 自然语言处理：预训练
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.17.1" data-path="../chapter_natural-language-processing-pretraining/word2vec.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word2vec.html">
            
                    
                    14.1. 词嵌入（word2vec）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.2" data-path="../chapter_natural-language-processing-pretraining/approx-training.html">
            
                <a href="../chapter_natural-language-processing-pretraining/approx-training.html">
            
                    
                    14.2. 近似训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.3" data-path="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">
            
                    
                    14.3. 用于预训练词嵌入的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.4" data-path="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">
            
                    
                    14.4. 预训练word2vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.5" data-path="../chapter_natural-language-processing-pretraining/glove.html">
            
                <a href="../chapter_natural-language-processing-pretraining/glove.html">
            
                    
                    14.5. 全局向量的词嵌入（GloVe）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.6" data-path="../chapter_natural-language-processing-pretraining/subword-embedding.html">
            
                <a href="../chapter_natural-language-processing-pretraining/subword-embedding.html">
            
                    
                    14.6. 子词嵌入
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.7" data-path="../chapter_natural-language-processing-pretraining/similarity-analogy.html">
            
                <a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">
            
                    
                    14.7. 词的相似性和类比任务
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.8" data-path="../chapter_natural-language-processing-pretraining/bert.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert.html">
            
                    
                    14.8. 来自Transformers的双向编码器表示（BERT）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.9" data-path="../chapter_natural-language-processing-pretraining/bert-dataset.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert-dataset.html">
            
                    
                    14.9. 用于预训练BERT的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.10" data-path="../chapter_natural-language-processing-pretraining/bert-pretraining.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">
            
                    
                    14.10. 预训练BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.18" data-path="../chapter_natural-language-processing-applications/">
            
                <a href="../chapter_natural-language-processing-applications/">
            
                    
                    15. 自然语言处理：应用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.18.1" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                    
                    15.1. 情感分析及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.2" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                    
                    15.2. 情感分析：使用循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.3" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                    
                    15.3. 情感分析：使用卷积神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.4" data-path="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                    
                    15.4. 自然语言推断与数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.5" data-path="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                    
                    15.5. 自然语言推断：使用注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.6" data-path="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                    
                    15.6. 针对序列级和词元级应用微调BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.7" data-path="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                    
                    15.7. 自然语言推断：微调BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.19" data-path="../chapter_appendix-tools-for-deep-learning/">
            
                <a href="../chapter_appendix-tools-for-deep-learning/">
            
                    
                    16. 附录：深度学习工具
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.19.1" data-path="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                    
                    16.1. 使用Jupyter Notebook
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.2" data-path="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                    
                    16.2. 使用Amazon SageMaker
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.3" data-path="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                    
                    16.3. 使用Amazon EC2实例
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.4" data-path="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                    
                    16.4. 选择服务器和GPU
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.5" data-path="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                    
                    16.5. 为本书做贡献
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.6" data-path="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                    
                    16.6. d2l API 文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.20" data-path="../chapter_references/zreferences.html">
            
                <a href="../chapter_references/zreferences.html">
            
                    
                    参考文献
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >2.6. 概率</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h1 id="概率">概率</h1>
<p>:label:<code>sec_prob</code></p>
<p>简单地说，机器学习就是做出预测。</p>
<p>根据病人的临床病史，我们可能想预测他们在下一年心脏病发作的<em>概率</em>。
在飞机喷气发动机的异常检测中，我们想要评估一组发动机读数为正常运行情况的概率有多大。
在强化学习中，我们希望智能体（agent）能在一个环境中智能地行动。
这意味着我们需要考虑在每种可行的行为下获得高奖励的概率。
当我们建立推荐系统时，我们也需要考虑概率。
例如，假设我们为一家大型在线书店工作，我们可能希望估计某些用户购买特定图书的概率。
为此，我们需要使用概率学。
有完整的课程、专业、论文、职业、甚至院系，都致力于概率学的工作。
所以很自然地，我们在这部分的目标不是教授整个科目。
相反，我们希望教给读者基础的概率知识，使读者能够开始构建第一个深度学习模型，
以便读者可以开始自己探索它。</p>
<p>现在让我们更认真地考虑第一个例子：根据照片区分猫和狗。
这听起来可能很简单，但对于机器却可能是一个艰巨的挑战。
首先，问题的难度可能取决于图像的分辨率。</p>
<p><img src="../img/cat-dog-pixels.png" alt="不同分辨率的图像 ($10 \times 10$, $20 \times 20$, $40 \times 40$, $80 \times 80$, 和 $160 \times 160$ pixels)"></img>
:width:<code>300px</code>
:label:<code>fig_cat_dog</code></p>
<p>如 :numref:<code>fig_cat_dog</code>所示，虽然人类很容易以$160 \times 160$像素的分辨率识别猫和狗，
但它在$40\times40$像素上变得具有挑战性，而且在$10 \times 10$像素下几乎是不可能的。
换句话说，我们在很远的距离（从而降低分辨率）区分猫和狗的能力可能会变为猜测。
概率给了我们一种正式的途径来说明我们的确定性水平。
如果我们完全肯定图像是一只猫，我们说标签$y$是"猫"的<em>概率</em>，表示为$P(y=$"猫"$)$等于$1$。
如果我们没有证据表明$y=$“猫”或$y=$“狗”，那么我们可以说这两种可能性是相等的，
即$P(y=$"猫"$)=P(y=$"狗"$)=0.5$。
如果我们不十分确定图像描绘的是一只猫，我们可以将概率赋值为$0.5&lt;P(y=$"猫"$)&lt;1$。</p>
<p>现在考虑第二个例子：给出一些天气监测数据，我们想预测明天北京下雨的概率。
如果是夏天，下雨的概率是0.5。</p>
<p>在这两种情况下，我们都不确定结果，但这两种情况之间有一个关键区别。
在第一种情况中，图像实际上是狗或猫二选一。
在第二种情况下，结果实际上是一个随机的事件。
因此，概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。</p>
<h2 id="基本概率论">基本概率论</h2>
<p>假设我们掷骰子，想知道看到1的几率有多大，而不是看到另一个数字。
如果骰子是公平的，那么所有六个结果${1, \ldots, 6}$都有相同的可能发生，
因此我们可以说$1$发生的概率为$\frac{1}{6}$。</p>
<p>然而现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。
检查骰子的唯一方法是多次投掷并记录结果。
对于每个骰子，我们将观察到${1, \ldots, 6}$中的一个值。
对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，
即此<em>事件</em>（event）概率的<em>估计值</em>。
<em>大数定律</em>（law of large numbers）告诉我们：
随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。
让我们用代码试一试！</p>
<p>首先，我们导入必要的软件包。</p>
<pre><code class="lang-python">%matplotlib inline
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> mxnet <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">from</span> mxnet <span class="hljs-keyword">import</span> np, npx
<span class="hljs-keyword">import</span> random
npx.set_np()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
%matplotlib inline
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.distributions <span class="hljs-keyword">import</span> multinomial
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
%matplotlib inline
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_probability <span class="hljs-keyword">as</span> tfp
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
%matplotlib inline
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> paddle <span class="hljs-keyword">as</span> d2l
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
<span class="hljs-keyword">import</span> paddle
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
<p>在统计学中，我们把从概率分布中抽取样本的过程称为<em>抽样</em>（sampling）。
笼统来说，可以把<em>分布</em>（distribution）看作对事件的概率分配，
稍后我们将给出的更正式定义。
将概率分配给一些离散选择的分布称为<em>多项分布</em>（multinomial distribution）。</p>
<p>为了抽取一个样本，即掷骰子，我们只需传入一个概率向量。
输出是另一个相同长度的向量：它在索引$i$处的值是采样结果中$i$出现的次数。</p>
<pre><code class="lang-python">fair_probs = [<span class="hljs-number">1.0</span> / <span class="hljs-number">6</span>] * <span class="hljs-number">6</span>
np.random.multinomial(<span class="hljs-number">1</span>, fair_probs)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
fair_probs = torch.ones([<span class="hljs-number">6</span>]) / <span class="hljs-number">6</span>
multinomial.Multinomial(<span class="hljs-number">1</span>, fair_probs).sample()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
fair_probs = tf.ones(<span class="hljs-number">6</span>) / <span class="hljs-number">6</span>
tfp.distributions.Multinomial(<span class="hljs-number">1</span>, fair_probs).sample()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
fair_probs = [<span class="hljs-number">1.0</span> / <span class="hljs-number">6</span>] * <span class="hljs-number">6</span>
paddle.distribution.Multinomial(<span class="hljs-number">1</span>, paddle.to_tensor(fair_probs)).sample()
</code></pre>
<p>在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。
如果用Python的for循环来完成这个任务，速度会慢得惊人。
因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。</p>
<pre><code class="lang-python">np.random.multinomial(<span class="hljs-number">10</span>, fair_probs)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
multinomial.Multinomial(<span class="hljs-number">10</span>, fair_probs).sample()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
tfp.distributions.Multinomial(<span class="hljs-number">10</span>, fair_probs).sample()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
paddle.distribution.Multinomial(<span class="hljs-number">10</span>, paddle.to_tensor(fair_probs)).sample()
</code></pre>
<p>现在我们知道如何对骰子进行采样，我们可以模拟1000次投掷。
然后，我们可以统计1000次投掷后，每个数字被投中了多少次。
具体来说，我们计算相对频率，以作为真实概率的估计。</p>
<pre><code class="lang-python">counts = np.random.multinomial(<span class="hljs-number">1000</span>, fair_probs).astype(np.float32)
counts / <span class="hljs-number">1000</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-comment"># 将结果存储为32位浮点数以进行除法</span>
counts = multinomial.Multinomial(<span class="hljs-number">1000</span>, fair_probs).sample()
counts / <span class="hljs-number">1000</span>  <span class="hljs-comment"># 相对频率作为估计值</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
counts = tfp.distributions.Multinomial(<span class="hljs-number">1000</span>, fair_probs).sample()
counts / <span class="hljs-number">1000</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
counts = paddle.distribution.Multinomial(<span class="hljs-number">1000</span>, paddle.to_tensor(fair_probs)).sample()
counts / <span class="hljs-number">1000</span>
</code></pre>
<p>因为我们是从一个公平的骰子中生成的数据，我们知道每个结果都有真实的概率$\frac{1}{6}$，
大约是$0.167$，所以上面输出的估计值看起来不错。</p>
<p>我们也可以看到这些概率如何随着时间的推移收敛到真实概率。
让我们进行500组实验，每组抽取10个样本。</p>
<pre><code class="lang-python">counts = np.random.multinomial(<span class="hljs-number">10</span>, fair_probs, size=<span class="hljs-number">500</span>)
cum_counts = counts.astype(np.float32).cumsum(axis=<span class="hljs-number">0</span>)
estimates = cum_counts / cum_counts.sum(axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)

d2l.set_figsize((<span class="hljs-number">6</span>, <span class="hljs-number">4.5</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
    d2l.plt.plot(estimates[:, i].asnumpy(),
                 label=(<span class="hljs-string">"P(die="</span> + str(i + <span class="hljs-number">1</span>) + <span class="hljs-string">")"</span>))
d2l.plt.axhline(y=<span class="hljs-number">0.167</span>, color=<span class="hljs-string">'black'</span>, linestyle=<span class="hljs-string">'dashed'</span>)
d2l.plt.gca().set_xlabel(<span class="hljs-string">'Groups of experiments'</span>)
d2l.plt.gca().set_ylabel(<span class="hljs-string">'Estimated probability'</span>)
d2l.plt.legend();
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
counts = multinomial.Multinomial(<span class="hljs-number">10</span>, fair_probs).sample((<span class="hljs-number">500</span>,))
cum_counts = counts.cumsum(dim=<span class="hljs-number">0</span>)
estimates = cum_counts / cum_counts.sum(dim=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)

d2l.set_figsize((<span class="hljs-number">6</span>, <span class="hljs-number">4.5</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
    d2l.plt.plot(estimates[:, i].numpy(),
                 label=(<span class="hljs-string">"P(die="</span> + str(i + <span class="hljs-number">1</span>) + <span class="hljs-string">")"</span>))
d2l.plt.axhline(y=<span class="hljs-number">0.167</span>, color=<span class="hljs-string">'black'</span>, linestyle=<span class="hljs-string">'dashed'</span>)
d2l.plt.gca().set_xlabel(<span class="hljs-string">'Groups of experiments'</span>)
d2l.plt.gca().set_ylabel(<span class="hljs-string">'Estimated probability'</span>)
d2l.plt.legend();
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
counts = tfp.distributions.Multinomial(<span class="hljs-number">10</span>, fair_probs).sample(<span class="hljs-number">500</span>)
cum_counts = tf.cumsum(counts, axis=<span class="hljs-number">0</span>)
estimates = cum_counts / tf.reduce_sum(cum_counts, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)

d2l.set_figsize((<span class="hljs-number">6</span>, <span class="hljs-number">4.5</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
    d2l.plt.plot(estimates[:, i].numpy(),
                 label=(<span class="hljs-string">"P(die="</span> + str(i + <span class="hljs-number">1</span>) + <span class="hljs-string">")"</span>))
d2l.plt.axhline(y=<span class="hljs-number">0.167</span>, color=<span class="hljs-string">'black'</span>, linestyle=<span class="hljs-string">'dashed'</span>)
d2l.plt.gca().set_xlabel(<span class="hljs-string">'Groups of experiments'</span>)
d2l.plt.gca().set_ylabel(<span class="hljs-string">'Estimated probability'</span>)
d2l.plt.legend();
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
counts = paddle.distribution.Multinomial(<span class="hljs-number">10</span>, paddle.to_tensor(fair_probs)).sample((<span class="hljs-number">500</span>,<span class="hljs-number">1</span>))
cum_counts = counts.cumsum(axis=<span class="hljs-number">0</span>)
cum_counts = cum_counts.squeeze(axis=<span class="hljs-number">1</span>)
estimates = cum_counts / cum_counts.sum(axis=<span class="hljs-number">1</span>, keepdim=<span class="hljs-keyword">True</span>)

d2l.set_figsize((<span class="hljs-number">6</span>, <span class="hljs-number">4.5</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">6</span>):
    d2l.plt.plot(estimates[:, i],
                 label=(<span class="hljs-string">"P(die="</span> + str(i + <span class="hljs-number">1</span>) + <span class="hljs-string">")"</span>))
d2l.plt.axhline(y=<span class="hljs-number">0.167</span>, color=<span class="hljs-string">'black'</span>, linestyle=<span class="hljs-string">'dashed'</span>)
d2l.plt.gca().set_xlabel(<span class="hljs-string">'Groups of experiments'</span>)
d2l.plt.gca().set_ylabel(<span class="hljs-string">'Estimated probability'</span>)
d2l.plt.legend()
</code></pre>
<p>每条实线对应于骰子的6个值中的一个，并给出骰子在每组实验后出现值的估计概率。
当我们通过更多的实验获得更多的数据时，这$6$条实体曲线向真实概率收敛。</p>
<h3 id="概率论公理">概率论公理</h3>
<p>在处理骰子掷出时，我们将集合$\mathcal{S} = {1, 2, 3, 4, 5, 6}$
称为<em>样本空间</em>（sample space）或<em>结果空间</em>（outcome space），
其中每个元素都是<em>结果</em>（outcome）。
<em>事件</em>（event）是一组给定样本空间的随机结果。
例如，“看到$5$”（${5}$）和“看到奇数”（${1, 3, 5}$）都是掷出骰子的有效事件。
注意，如果一个随机实验的结果在$\mathcal{A}$中，则事件$\mathcal{A}$已经发生。
也就是说，如果投掷出$3$点，因为$3 \in {1, 3, 5}$，我们可以说，“看到奇数”的事件发生了。</p>
<p><em>概率</em>（probability）可以被认为是将集合映射到真实值的函数。
在给定的样本空间$\mathcal{S}$中，事件$\mathcal{A}$的概率，
表示为$P(\mathcal{A})$，满足以下属性：</p>
<ul>
<li>对于任意事件$\mathcal{A}$，其概率从不会是负数，即$P(\mathcal{A}) \geq 0$；</li>
<li>整个样本空间的概率为$1$，即$P(\mathcal{S}) = 1$；</li>
<li>对于<em>互斥</em>（mutually exclusive）事件（对于所有$i \neq j$都有$\mathcal{A}<em>i \cap \mathcal{A}_j = \emptyset$）的任意一个可数序列$\mathcal{A}_1, \mathcal{A}_2, \ldots$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(\bigcup</em>{i=1}^{\infty} \mathcal{A}<em>i) = \sum</em>{i=1}^{\infty} P(\mathcal{A}_i)$。</li>
</ul>
<p>以上也是概率论的公理，由科尔莫戈罗夫于1933年提出。
有了这个公理系统，我们可以避免任何关于随机性的哲学争论；
相反，我们可以用数学语言严格地推理。
例如，假设事件$\mathcal{A}_1$为整个样本空间，
且当所有$i &gt; 1$时的$\mathcal{A}_i = \emptyset$，
那么我们可以证明$P(\emptyset) = 0$，即不可能发生事件的概率是$0$。</p>
<h3 id="随机变量">随机变量</h3>
<p>在我们掷骰子的随机实验中，我们引入了<em>随机变量</em>（random variable）的概念。
随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。
考虑一个随机变量$X$，其值在掷骰子的样本空间$\mathcal{S}={1,2,3,4,5,6}$中。
我们可以将事件“看到一个$5$”表示为${X=5}$或$X=5$，
其概率表示为$P({X=5})$或$P(X=5)$。
通过$P(X=a)$，我们区分了随机变量$X$和$X$可以采取的值（例如$a$）。
然而，这可能会导致繁琐的表示。
为了简化符号，一方面，我们可以将$P(X)$表示为随机变量$X$上的<em>分布</em>（distribution）：
分布告诉我们$X$获得某一值的概率。
另一方面，我们可以简单用$P(a)$表示随机变量取值$a$的概率。
由于概率论中的事件是来自样本空间的一组结果，因此我们可以为随机变量指定值的可取范围。
例如，$P(1 \leq X \leq 3)$表示事件${1 \leq X \leq 3}$，
即${X = 1, 2, \text{or}, 3}$的概率。
等价地，$P(1 \leq X \leq 3)$表示随机变量$X$从${1, 2, 3}$中取值的概率。</p>
<p>请注意，<em>离散</em>（discrete）随机变量（如骰子的每一面）
和<em>连续</em>（continuous）随机变量（如人的体重和身高）之间存在微妙的区别。
现实生活中，测量两个人是否具有完全相同的身高没有太大意义。
如果我们进行足够精确的测量，最终会发现这个星球上没有两个人具有完全相同的身高。
在这种情况下，询问某人的身高是否落入给定的区间，比如是否在1.79米和1.81米之间更有意义。
在这些情况下，我们将这个看到某个数值的可能性量化为<em>密度</em>（density）。
高度恰好为1.80米的概率为0，但密度不是0。
在任何两个不同高度之间的区间，我们都有非零的概率。
在本节的其余部分中，我们将考虑离散空间中的概率。
连续随机变量的概率可以参考深度学习数学附录中<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/random-variables.html" target="_blank">随机变量</a>
的一节。</p>
<h2 id="处理多个随机变量">处理多个随机变量</h2>
<p>很多时候，我们会考虑多个随机变量。
比如，我们可能需要对疾病和症状之间的关系进行建模。
给定一个疾病和一个症状，比如“流感”和“咳嗽”，以某个概率存在或不存在于某个患者身上。
我们需要估计这些概率以及概率之间的关系，以便我们可以运用我们的推断来实现更好的医疗服务。</p>
<p>再举一个更复杂的例子：图像包含数百万像素，因此有数百万个随机变量。
在许多情况下，图像会附带一个<em>标签</em>（label），标识图像中的对象。
我们也可以将标签视为一个随机变量。
我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。
所有这些都是联合发生的随机变量。
当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。</p>
<h3 id="联合概率">联合概率</h3>
<p>第一个被称为<em>联合概率</em>（joint probability）$P(A=a,B=b)$。
给定任意值$a$和$b$，联合概率可以回答：$A=a$和$B=b$同时满足的概率是多少？
请注意，对于任何$a$和$b$的取值，$P(A = a, B=b) \leq P(A=a)$。
这点是确定的，因为要同时发生$A=a$和$B=b$，$A=a$就必须发生，$B=b$也必须发生（反之亦然）。因此，$A=a$和$B=b$同时发生的可能性不大于$A=a$或是$B=b$单独发生的可能性。</p>
<h3 id="条件概率">条件概率</h3>
<p>联合概率的不等式带给我们一个有趣的比率：
$0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1$。
我们称这个比率为<em>条件概率</em>（conditional probability），
并用$P(B=b \mid A=a)$表示它：它是$B=b$的概率，前提是$A=a$已发生。</p>
<h3 id="贝叶斯定理">贝叶斯定理</h3>
<p>使用条件概率的定义，我们可以得出统计学中最有用的方程之一：
<em>Bayes定理</em>（Bayes' theorem）。
根据<em>乘法法则</em>（multiplication rule ）可得到$P(A, B) = P(B \mid A) P(A)$。
根据对称性，可得到$P(A, B) = P(A \mid B) P(B)$。
假设$P(B)&gt;0$，求解其中一个条件变量，我们得到</p>
<p>$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.$$</p>
<p>请注意，这里我们使用紧凑的表示法：
其中$P(A, B)$是一个<em>联合分布</em>（joint distribution），
$P(A \mid B)$是一个<em>条件分布</em>（conditional distribution）。
这种分布可以在给定值$A = a, B=b$上进行求值。</p>
<h3 id="边际化">边际化</h3>
<p>为了能进行事件概率求和，我们需要<em>求和法则</em>（sum rule），
即$B$的概率相当于计算$A$的所有可能选择，并将所有选择的联合概率聚合在一起：</p>
<p>$$P(B) = \sum_{A} P(A, B),$$</p>
<p>这也称为<em>边际化</em>（marginalization）。
边际化结果的概率或分布称为<em>边际概率</em>（marginal probability）
或<em>边际分布</em>（marginal distribution）。</p>
<h3 id="独立性">独立性</h3>
<p>另一个有用属性是<em>依赖</em>（dependence）与<em>独立</em>（independence）。
如果两个随机变量$A$和$B$是独立的，意味着事件$A$的发生跟$B$事件的发生无关。
在这种情况下，统计学家通常将这一点表述为$A \perp  B$。
根据贝叶斯定理，马上就能同样得到$P(A \mid B) = P(A)$。
在所有其他情况下，我们称$A$和$B$依赖。
比如，两次连续抛出一个骰子的事件是相互独立的。
相比之下，灯开关的位置和房间的亮度并不是（因为可能存在灯泡坏掉、电源故障，或者开关故障）。</p>
<p>由于$P(A \mid B) = \frac{P(A, B)}{P(B)} = P(A)$等价于$P(A, B) = P(A)P(B)$，
因此两个随机变量是独立的，当且仅当两个随机变量的联合分布是其各自分布的乘积。
同样地，给定另一个随机变量$C$时，两个随机变量$A$和$B$是<em>条件独立的</em>（conditionally independent），
当且仅当$P(A, B \mid C) = P(A \mid C)P(B \mid C)$。
这个情况表示为$A \perp B \mid C$。</p>
<h3 id="应用">应用</h3>
<p>:label:<code>subsec_probability_hiv_app</code></p>
<p>我们实战演练一下！
假设一个医生对患者进行艾滋病病毒（HIV）测试。
这个测试是相当准确的，如果患者健康但测试显示他患病，这个概率只有1%；
如果患者真正感染HIV，它永远不会检测不出。
我们使用$D_1$来表示诊断结果（如果阳性，则为$1$，如果阴性，则为$0$），
$H$来表示感染艾滋病病毒的状态（如果阳性，则为$1$，如果阴性，则为$0$）。
在 :numref:<code>conditional_prob_D1</code>中列出了这样的条件概率。</p>
<p>:条件概率为$P(D_1 \mid H)$</p>
<table>
<thead>
<tr>
<th>条件概率</th>
<th>$H=1$</th>
<th>$H=0$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$P(D_1 = 1 \mid H)$</td>
<td>1</td>
<td>0.01</td>
</tr>
<tr>
<td>$P(D_1 = 0 \mid H)$</td>
<td>0</td>
<td>0.99</td>
</tr>
</tbody>
</table>
<p>:label:<code>conditional_prob_D1</code></p>
<p>请注意，每列的加和都是1（但每行的加和不是），因为条件概率需要总和为1，就像概率一样。
让我们计算如果测试出来呈阳性，患者感染HIV的概率，即$P(H = 1 \mid D_1 = 1)$。
显然，这将取决于疾病有多常见，因为它会影响错误警报的数量。
假设人口总体是相当健康的，例如，$P(H=1) = 0.0015$。
为了应用贝叶斯定理，我们需要运用边际化和乘法法则来确定</p>
<p>$$\begin{aligned}
&amp;P(D_1 = 1) \
=&amp; P(D_1=1, H=0) + P(D_1=1, H=1)  \
=&amp; P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \
=&amp; 0.011485.
\end{aligned}</p>
<p>$$
因此，我们得到</p>
<p>$$\begin{aligned}
&amp;P(H = 1 \mid D_1 = 1)\ =&amp; \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} \ =&amp; 0.1306 \end{aligned}.$$</p>
<p>换句话说，尽管使用了非常准确的测试，患者实际上患有艾滋病的几率只有13.06%。
正如我们所看到的，概率可能是违反直觉的。</p>
<p>患者在收到这样可怕的消息后应该怎么办？
很可能，患者会要求医生进行另一次测试来确定病情。
第二个测试具有不同的特性，它不如第一个测试那么精确，
如 :numref:<code>conditional_prob_D2</code>所示。</p>
<p>:条件概率为$P(D_2 \mid H)$</p>
<table>
<thead>
<tr>
<th>条件概率</th>
<th>$H=1$</th>
<th>$H=0$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$P(D_2 = 1 \mid H)$</td>
<td>0.98</td>
<td>0.03</td>
</tr>
<tr>
<td>$P(D_2 = 0 \mid H)$</td>
<td>0.02</td>
<td>0.97</td>
</tr>
</tbody>
</table>
<p>:label:<code>conditional_prob_D2</code></p>
<p>不幸的是，第二次测试也显示阳性。让我们通过假设条件独立性来计算出应用Bayes定理的必要概率：</p>
<p>$$\begin{aligned}
&amp;P(D_1 = 1, D_2 = 1 \mid H = 0) \
=&amp; P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)  \
=&amp; 0.0003,
\end{aligned}</p>
<p>$$</p>
<p>$$\begin{aligned}
&amp;P(D_1 = 1, D_2 = 1 \mid H = 1) \
=&amp; P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)  \
=&amp; 0.98.
\end{aligned}</p>
<p>$$
现在我们可以应用边际化和乘法规则：</p>
<p>$$\begin{aligned}
&amp;P(D_1 = 1, D_2 = 1) \
=&amp; P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \
=&amp; P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\
=&amp; 0.00176955.
\end{aligned}</p>
<p>$$</p>
<p>最后，鉴于存在两次阳性检测，患者患有艾滋病的概率为</p>
<p>$$\begin{aligned}
&amp;P(H = 1 \mid D_1 = 1, D_2 = 1)\
=&amp; \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \
=&amp; 0.8307.
\end{aligned}</p>
<p>$$</p>
<p>也就是说，第二次测试使我们能够对患病的情况获得更高的信心。
尽管第二次检验比第一次检验的准确性要低得多，但它仍然显著提高我们的预测概率。</p>
<h2 id="期望和方差">期望和方差</h2>
<p>为了概括概率分布的关键特征，我们需要一些测量方法。
一个随机变量$X$的<em>期望</em>（expectation，或平均值（average））表示为</p>
<p>$$E[X] = \sum_{x} x P(X = x).$$</p>
<p>当函数$f(x)$的输入是从分布$P$中抽取的随机变量时，$f(x)$的期望值为</p>
<p>$$E_{x \sim P}[f(x)] = \sum_x f(x) P(x).$$</p>
<p>在许多情况下，我们希望衡量随机变量$X$与其期望值的偏置。这可以通过方差来量化</p>
<p>$$\mathrm{Var}[X] = E\left[(X - E[X])^2\right] =
E[X^2] - E[X]^2.$$</p>
<p>方差的平方根被称为<em>标准差</em>（standard deviation）。
随机变量函数的方差衡量的是：当从该随机变量分布中采样不同值$x$时，
函数值偏离该函数的期望的程度：</p>
<p>$$\mathrm{Var}[f(x)] = E\left[\left(f(x) - E[f(x)]\right)^2\right].$$</p>
<h2 id="小结">小结</h2>
<ul>
<li>我们可以从概率分布中采样。</li>
<li>我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。</li>
<li>期望和方差为概率分布的关键特征的概括提供了实用的度量形式。</li>
</ul>
<h2 id="练习">练习</h2>
<ol>
<li>进行$m=500$组实验，每组抽取$n=10$个样本。改变$m$和$n$，观察和分析实验结果。</li>
<li>给定两个概率为$P(\mathcal{A})$和$P(\mathcal{B})$的事件，计算$P(\mathcal{A} \cup \mathcal{B})$和$P(\mathcal{A} \cap \mathcal{B})$的上限和下限。（提示：使用<a href="https://en.wikipedia.org/wiki/Venn_diagram" target="_blank">友元图</a>来展示这些情况。)</li>
<li>假设我们有一系列随机变量，例如$A$、$B$和$C$，其中$B$只依赖于$A$，而$C$只依赖于$B$，能简化联合概率$P(A, B, C)$吗？（提示：这是一个<a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank">马尔可夫链</a>。)</li>
<li>在 :numref:<code>subsec_probability_hiv_app</code>中，第一个测试更准确。为什么不运行第一个测试两次，而是同时运行第一个和第二个测试?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/1761" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1762" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1760" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>paddle</code>
<a href="https://discuss.d2l.ai/t/11685" target="_blank">Discussions</a>
:end_tab:</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="autograd.html" class="navigation navigation-prev " aria-label="Previous page: 2.5. 自动微分">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="lookup-api.html" class="navigation navigation-next " aria-label="Next page: 2.7. 查阅文档">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"2.6. 概率","level":"1.2.5.6","depth":3,"next":{"title":"2.7. 查阅文档","level":"1.2.5.7","depth":3,"path":"chapter_preliminaries/lookup-api.md","ref":"chapter_preliminaries/lookup-api.md","articles":[]},"previous":{"title":"2.5. 自动微分","level":"1.2.5.5","depth":3,"path":"chapter_preliminaries/autograd.md","ref":"chapter_preliminaries/autograd.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-search","-livereload","-lunr","-fontsettings","highlight","expandable-chapters-small","back-to-top-button","github","code","theme-default"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"highlight":{"lang":{"eval_rst":"rst","toc":"text"}},"github":{"url":"https://github.com/KittenCN"},"expandable-chapters-small":{},"back-to-top-button":{},"code":{"copyButtons":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"Todd Lyu","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"CoderFAN 资料库","gitbook":"*"},"file":{"path":"chapter_preliminaries/probability.md","mtime":"2024-01-19T05:51:47.455Z","type":"markdown"},"gitbook":{"version":"6.0.3","time":"2025-05-05T05:19:45.706Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    

    </body>
</html>

