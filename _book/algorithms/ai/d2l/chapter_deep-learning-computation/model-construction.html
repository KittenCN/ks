
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>5.1. 层和块 · CoderFAN 资料库</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="CoderFAN 资料库 动手学深度学习">
        <meta name="generator" content="GitBook 6.0.3">
        <meta name="author" content="Todd Lyu">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="parameters.html" />
    
    
    <link rel="prev" href="./" />
    
    <!-- MathJax 配置：唯一且完整 -->
<script>
    window.MathJax = {
      tex: {
        inlineMath:  [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        strict: "ignore",
        macros: { "\\E":"\\mathbb{E}", "\\Var":"\\operatorname{Var}" }
      },
    };
    </script>
    
    <!-- 核心脚本（defer不阻塞渲染） -->
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
    <!-- 放在 tex-chtml.js 之后 -->
    <script>
    (function () {
      function typeset() {
        if (window.MathJax && MathJax.typesetPromise) {
          MathJax.typesetPromise().catch(console.error);
        }
      }
    
      /* 第一次正文插入 */
      document.addEventListener('DOMContentLoaded', typeset);
    
      /*   关键：等待 gitbook.js 初始化成功   */
      function hookGitBook() {
        if (window.gitbook && gitbook.events) {
          gitbook.events.bind('page.change', typeset);   // 切章排版
        } else {
          /* gitbook.js 还没加载完 → 100 ms 后再试 */
          setTimeout(hookGitBook, 100);
        }
      }
      hookGitBook();   // 启动递归等待
    })();
    </script>
    
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
            
                <nav role="navigation">
                <a href="../.." class="btn"><b></b>&#128512;返回上层&#128512;</b></a>
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../d2l.md">
            
                <span>
            
                    
                    动手学深度学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../chapter_preface/">
            
                <a href="../chapter_preface/">
            
                    
                    前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../chapter_installation/">
            
                <a href="../chapter_installation/">
            
                    
                    安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../chapter_notation/">
            
                <a href="../chapter_notation/">
            
                    
                    符号
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../chapter_introduction/">
            
                <a href="../chapter_introduction/">
            
                    
                    1. 引言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../chapter_preliminaries/">
            
                <a href="../chapter_preliminaries/">
            
                    
                    2. 预备知识
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="../chapter_preliminaries/ndarray.html">
            
                <a href="../chapter_preliminaries/ndarray.html">
            
                    
                    2.1. 数据操作
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="../chapter_preliminaries/pandas.html">
            
                <a href="../chapter_preliminaries/pandas.html">
            
                    
                    2.2. 数据预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="../chapter_preliminaries/linear-algebra.html">
            
                <a href="../chapter_preliminaries/linear-algebra.html">
            
                    
                    2.3. 线性代数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="../chapter_preliminaries/calculus.html">
            
                <a href="../chapter_preliminaries/calculus.html">
            
                    
                    2.4. 微积分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.5" data-path="../chapter_preliminaries/autograd.html">
            
                <a href="../chapter_preliminaries/autograd.html">
            
                    
                    2.5. 自动微分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.6" data-path="../chapter_preliminaries/probability.html">
            
                <a href="../chapter_preliminaries/probability.html">
            
                    
                    2.6. 概率
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.7" data-path="../chapter_preliminaries/lookup-api.html">
            
                <a href="../chapter_preliminaries/lookup-api.html">
            
                    
                    2.7. 查阅文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../chapter_linear-networks/">
            
                <a href="../chapter_linear-networks/">
            
                    
                    3. 线性神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="../chapter_linear-networks/linear-regression.html">
            
                <a href="../chapter_linear-networks/linear-regression.html">
            
                    
                    3.1. 线性回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="../chapter_linear-networks/linear-regression-scratch.html">
            
                <a href="../chapter_linear-networks/linear-regression-scratch.html">
            
                    
                    3.2. 线性回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="../chapter_linear-networks/linear-regression-concise.html">
            
                <a href="../chapter_linear-networks/linear-regression-concise.html">
            
                    
                    3.3. 线性回归的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="../chapter_linear-networks/softmax-regression.html">
            
                <a href="../chapter_linear-networks/softmax-regression.html">
            
                    
                    3.4. softmax回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.5" data-path="../chapter_linear-networks/image-classification-dataset.html">
            
                <a href="../chapter_linear-networks/image-classification-dataset.html">
            
                    
                    3.5. 图像分类数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.6" data-path="../chapter_linear-networks/softmax-regression-scratch.html">
            
                <a href="../chapter_linear-networks/softmax-regression-scratch.html">
            
                    
                    3.6. softmax回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.7" data-path="../chapter_linear-networks/softmax-regression-concise.html">
            
                <a href="../chapter_linear-networks/softmax-regression-concise.html">
            
                    
                    3.7. softmax回归的简洁实现
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../chapter_multilayer-perceptrons/">
            
                <a href="../chapter_multilayer-perceptrons/">
            
                    
                    4. 多层感知机
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="../chapter_multilayer-perceptrons/mlp.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp.html">
            
                    
                    4.1. 多层感知机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                    
                    4.2. 多层感知机的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                    
                    4.3. 多层感知机的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                <a href="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                    
                    4.4. 模型选择、欠拟合和过拟合
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.5" data-path="../chapter_multilayer-perceptrons/weight-decay.html">
            
                <a href="../chapter_multilayer-perceptrons/weight-decay.html">
            
                    
                    4.5. 权重衰减
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.6" data-path="../chapter_multilayer-perceptrons/dropout.html">
            
                <a href="../chapter_multilayer-perceptrons/dropout.html">
            
                    
                    4.6. 暂退法（Dropout）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.7" data-path="../chapter_multilayer-perceptrons/backprop.html">
            
                <a href="../chapter_multilayer-perceptrons/backprop.html">
            
                    
                    4.7. 前向传播、反向传播和计算图
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.8" data-path="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                <a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                    
                    4.8. 数值稳定性和模型初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.9" data-path="../chapter_multilayer-perceptrons/environment.html">
            
                <a href="../chapter_multilayer-perceptrons/environment.html">
            
                    
                    4.9. 环境和分布偏移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.10" data-path="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                <a href="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                    
                    4.10. 实战Kaggle比赛：预测房价
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="./">
            
                <a href="./">
            
                    
                    5. 深度学习计算
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.2.8.1" data-path="model-construction.html">
            
                <a href="model-construction.html">
            
                    
                    5.1. 层和块
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.2" data-path="parameters.html">
            
                <a href="parameters.html">
            
                    
                    5.2. 参数管理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.3" data-path="deferred-init.html">
            
                <a href="deferred-init.html">
            
                    
                    5.3. 延后初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.4" data-path="custom-layer.html">
            
                <a href="custom-layer.html">
            
                    
                    5.4. 自定义层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.5" data-path="read-write.html">
            
                <a href="read-write.html">
            
                    
                    5.5. 读写文件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.6" data-path="use-gpu.html">
            
                <a href="use-gpu.html">
            
                    
                    5.6. GPU计算
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../chapter_convolutional-neural-networks/">
            
                <a href="../chapter_convolutional-neural-networks/">
            
                    
                    6. 卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.9.1" data-path="../chapter_convolutional-neural-networks/why-conv.html">
            
                <a href="../chapter_convolutional-neural-networks/why-conv.html">
            
                    
                    6.1. 从全连接层到卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.2" data-path="../chapter_convolutional-neural-networks/conv-layer.html">
            
                <a href="../chapter_convolutional-neural-networks/conv-layer.html">
            
                    
                    6.2. 图像卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.3" data-path="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                <a href="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                    
                    6.3. 填充和步幅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.4" data-path="../chapter_convolutional-neural-networks/channels.html">
            
                <a href="../chapter_convolutional-neural-networks/channels.html">
            
                    
                    6.4. 多输入多输出通道
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.5" data-path="../chapter_convolutional-neural-networks/pooling.html">
            
                <a href="../chapter_convolutional-neural-networks/pooling.html">
            
                    
                    6.5. 汇聚层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.6" data-path="../chapter_convolutional-neural-networks/lenet.html">
            
                <a href="../chapter_convolutional-neural-networks/lenet.html">
            
                    
                    6.6. 卷积神经网络（LeNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../chapter_convolutional-modern/">
            
                <a href="../chapter_convolutional-modern/">
            
                    
                    7. 现代卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.10.1" data-path="../chapter_convolutional-modern/alexnet.html">
            
                <a href="../chapter_convolutional-modern/alexnet.html">
            
                    
                    7.1. 深度卷积神经网络（AlexNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.2" data-path="../chapter_convolutional-modern/vgg.html">
            
                <a href="../chapter_convolutional-modern/vgg.html">
            
                    
                    7.2. 使用块的网络（VGG）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.3" data-path="../chapter_convolutional-modern/nin.html">
            
                <a href="../chapter_convolutional-modern/nin.html">
            
                    
                    7.3. 网络中的网络（NiN）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.4" data-path="../chapter_convolutional-modern/googlenet.html">
            
                <a href="../chapter_convolutional-modern/googlenet.html">
            
                    
                    7.4. 含并行连结的网络（GoogLeNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.5" data-path="../chapter_convolutional-modern/batch-norm.html">
            
                <a href="../chapter_convolutional-modern/batch-norm.html">
            
                    
                    7.5. 批量规范化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.6" data-path="../chapter_convolutional-modern/resnet.html">
            
                <a href="../chapter_convolutional-modern/resnet.html">
            
                    
                    7.6. 残差网络（ResNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.7" data-path="../chapter_convolutional-modern/densenet.html">
            
                <a href="../chapter_convolutional-modern/densenet.html">
            
                    
                    7.7. 稠密连接网络（DenseNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../chapter_recurrent-neural-networks/">
            
                <a href="../chapter_recurrent-neural-networks/">
            
                    
                    8. 循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.11.1" data-path="../chapter_recurrent-neural-networks/sequence.html">
            
                <a href="../chapter_recurrent-neural-networks/sequence.html">
            
                    
                    8.1. 序列模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.2" data-path="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                <a href="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                    
                    8.2. 文本预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.3" data-path="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                <a href="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                    
                    8.3. 语言模型和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.4" data-path="../chapter_recurrent-neural-networks/rnn.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn.html">
            
                    
                    8.4. 循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.5" data-path="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                    
                    8.5. 循环神经网络的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.6" data-path="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                    
                    8.6. 循环神经网络的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.7" data-path="../chapter_recurrent-neural-networks/bptt.html">
            
                <a href="../chapter_recurrent-neural-networks/bptt.html">
            
                    
                    8.7. 通过时间反向传播
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../chapter_recurrent-modern/">
            
                <a href="../chapter_recurrent-modern/">
            
                    
                    9. 现代循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.12.1" data-path="../chapter_recurrent-modern/gru.html">
            
                <a href="../chapter_recurrent-modern/gru.html">
            
                    
                    9.1. 门控循环单元（GRU）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.2" data-path="../chapter_recurrent-modern/lstm.html">
            
                <a href="../chapter_recurrent-modern/lstm.html">
            
                    
                    9.2. 长短期记忆（LSTM）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.3" data-path="../chapter_recurrent-modern/deep-rnn.html">
            
                <a href="../chapter_recurrent-modern/deep-rnn.html">
            
                    
                    9.3. 深度循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.4" data-path="../chapter_recurrent-modern/bi-rnn.html">
            
                <a href="../chapter_recurrent-modern/bi-rnn.html">
            
                    
                    9.4. 双向循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.5" data-path="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                <a href="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                    
                    9.5. 机器翻译及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.6" data-path="../chapter_recurrent-modern/encoder-decoder.html">
            
                <a href="../chapter_recurrent-modern/encoder-decoder.html">
            
                    
                    9.6. 编码器—解码器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.7" data-path="../chapter_recurrent-modern/seq2seq.html">
            
                <a href="../chapter_recurrent-modern/seq2seq.html">
            
                    
                    9.7. 序列到序列学习（seq2seq）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.8" data-path="../chapter_recurrent-modern/beam-search.html">
            
                <a href="../chapter_recurrent-modern/beam-search.html">
            
                    
                    9.8. 束搜索
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="../chapter_attention-mechanisms/">
            
                <a href="../chapter_attention-mechanisms/">
            
                    
                    10. 注意力机制
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.13.1" data-path="../chapter_attention-mechanisms/attention-cues.html">
            
                <a href="../chapter_attention-mechanisms/attention-cues.html">
            
                    
                    10.1. 注意力提示
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.2" data-path="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                <a href="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                    
                    10.2. 注意力汇聚：Nadaraya-Watson 核回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.3" data-path="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                <a href="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                    
                    10.3. 注意力评分函数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.4" data-path="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                <a href="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                    
                    10.4. Bahdanau 注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.5" data-path="../chapter_attention-mechanisms/multihead-attention.html">
            
                <a href="../chapter_attention-mechanisms/multihead-attention.html">
            
                    
                    10.5. 多头注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.6" data-path="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                <a href="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                    
                    10.6. 自注意力和位置编码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.7" data-path="../chapter_attention-mechanisms/transformer.html">
            
                <a href="../chapter_attention-mechanisms/transformer.html">
            
                    
                    10.7. Transformer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.14" data-path="../chapter_optimization/">
            
                <a href="../chapter_optimization/">
            
                    
                    11. 优化算法
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.14.1" data-path="../chapter_optimization/optimization-intro.html">
            
                <a href="../chapter_optimization/optimization-intro.html">
            
                    
                    11.1. 优化与深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.2" data-path="../chapter_optimization/convexity.html">
            
                <a href="../chapter_optimization/convexity.html">
            
                    
                    11.2. 凸性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.3" data-path="../chapter_optimization/gd.html">
            
                <a href="../chapter_optimization/gd.html">
            
                    
                    11.3. 梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.4" data-path="../chapter_optimization/sgd.html">
            
                <a href="../chapter_optimization/sgd.html">
            
                    
                    11.4. 随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.5" data-path="../chapter_optimization/minibatch-sgd.html">
            
                <a href="../chapter_optimization/minibatch-sgd.html">
            
                    
                    11.5. 小批量随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.6" data-path="../chapter_optimization/momentum.html">
            
                <a href="../chapter_optimization/momentum.html">
            
                    
                    11.6. 动量法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.7" data-path="../chapter_optimization/adagrad.html">
            
                <a href="../chapter_optimization/adagrad.html">
            
                    
                    11.7. AdaGrad算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.8" data-path="../chapter_optimization/rmsprop.html">
            
                <a href="../chapter_optimization/rmsprop.html">
            
                    
                    11.8. RMSProp算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.9" data-path="../chapter_optimization/adadelta.html">
            
                <a href="../chapter_optimization/adadelta.html">
            
                    
                    11.9. Adadelta
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.10" data-path="../chapter_optimization/adam.html">
            
                <a href="../chapter_optimization/adam.html">
            
                    
                    11.10. Adam算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.11" data-path="../chapter_optimization/lr-scheduler.html">
            
                <a href="../chapter_optimization/lr-scheduler.html">
            
                    
                    11.11. 学习率调度器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.15" data-path="../chapter_computational-performance/">
            
                <a href="../chapter_computational-performance/">
            
                    
                    12. 计算性能
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.15.1" data-path="../chapter_computational-performance/hybridize.html">
            
                <a href="../chapter_computational-performance/hybridize.html">
            
                    
                    12.1. 编译器和解释器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.2" data-path="../chapter_computational-performance/async-computation.html">
            
                <a href="../chapter_computational-performance/async-computation.html">
            
                    
                    12.2. 异步计算
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.3" data-path="../chapter_computational-performance/auto-parallelism.html">
            
                <a href="../chapter_computational-performance/auto-parallelism.html">
            
                    
                    12.3. 自动并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.4" data-path="../chapter_computational-performance/hardware.html">
            
                <a href="../chapter_computational-performance/hardware.html">
            
                    
                    12.4. 硬件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.5" data-path="../chapter_computational-performance/multiple-gpus.html">
            
                <a href="../chapter_computational-performance/multiple-gpus.html">
            
                    
                    12.5. 多GPU训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.6" data-path="../chapter_computational-performance/multiple-gpus-concise.html">
            
                <a href="../chapter_computational-performance/multiple-gpus-concise.html">
            
                    
                    12.6. 多GPU的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.7" data-path="../chapter_computational-performance/parameterserver.html">
            
                <a href="../chapter_computational-performance/parameterserver.html">
            
                    
                    12.7. 参数服务器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.16" data-path="../chapter_computer-vision/">
            
                <a href="../chapter_computer-vision/">
            
                    
                    13. 计算机视觉
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.16.1" data-path="../chapter_computer-vision/image-augmentation.html">
            
                <a href="../chapter_computer-vision/image-augmentation.html">
            
                    
                    13.1. 图像增广
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.2" data-path="../chapter_computer-vision/fine-tuning.html">
            
                <a href="../chapter_computer-vision/fine-tuning.html">
            
                    
                    13.2. 微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.3" data-path="../chapter_computer-vision/bounding-box.html">
            
                <a href="../chapter_computer-vision/bounding-box.html">
            
                    
                    13.3. 目标检测和边界框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.4" data-path="../chapter_computer-vision/anchor.html">
            
                <a href="../chapter_computer-vision/anchor.html">
            
                    
                    13.4. 锚框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.5" data-path="../chapter_computer-vision/multiscale-object-detection.html">
            
                <a href="../chapter_computer-vision/multiscale-object-detection.html">
            
                    
                    13.5. 多尺度目标检测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.6" data-path="../chapter_computer-vision/object-detection-dataset.html">
            
                <a href="../chapter_computer-vision/object-detection-dataset.html">
            
                    
                    13.6. 目标检测数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.7" data-path="../chapter_computer-vision/ssd.html">
            
                <a href="../chapter_computer-vision/ssd.html">
            
                    
                    13.7. 单发多框检测（SSD）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.8" data-path="../chapter_computer-vision/rcnn.html">
            
                <a href="../chapter_computer-vision/rcnn.html">
            
                    
                    13.8. 区域卷积神经网络（R-CNN）系列
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.9" data-path="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                <a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                    
                    13.9. 语义分割和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.10" data-path="../chapter_computer-vision/transposed-conv.html">
            
                <a href="../chapter_computer-vision/transposed-conv.html">
            
                    
                    13.10. 转置卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.11" data-path="../chapter_computer-vision/fcn.html">
            
                <a href="../chapter_computer-vision/fcn.html">
            
                    
                    13.11. 全卷积网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.12" data-path="../chapter_computer-vision/neural-style.html">
            
                <a href="../chapter_computer-vision/neural-style.html">
            
                    
                    13.12. 风格迁移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.13" data-path="../chapter_computer-vision/kaggle-cifar10.html">
            
                <a href="../chapter_computer-vision/kaggle-cifar10.html">
            
                    
                    13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10.md)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.14" data-path="../chapter_computer-vision/kaggle-dog.html">
            
                <a href="../chapter_computer-vision/kaggle-dog.html">
            
                    
                    13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.17" data-path="../chapter_natural-language-processing-pretraining/">
            
                <a href="../chapter_natural-language-processing-pretraining/">
            
                    
                    14. 自然语言处理：预训练
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.17.1" data-path="../chapter_natural-language-processing-pretraining/word2vec.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word2vec.html">
            
                    
                    14.1. 词嵌入（word2vec）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.2" data-path="../chapter_natural-language-processing-pretraining/approx-training.html">
            
                <a href="../chapter_natural-language-processing-pretraining/approx-training.html">
            
                    
                    14.2. 近似训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.3" data-path="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">
            
                    
                    14.3. 用于预训练词嵌入的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.4" data-path="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">
            
                    
                    14.4. 预训练word2vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.5" data-path="../chapter_natural-language-processing-pretraining/glove.html">
            
                <a href="../chapter_natural-language-processing-pretraining/glove.html">
            
                    
                    14.5. 全局向量的词嵌入（GloVe）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.6" data-path="../chapter_natural-language-processing-pretraining/subword-embedding.html">
            
                <a href="../chapter_natural-language-processing-pretraining/subword-embedding.html">
            
                    
                    14.6. 子词嵌入
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.7" data-path="../chapter_natural-language-processing-pretraining/similarity-analogy.html">
            
                <a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">
            
                    
                    14.7. 词的相似性和类比任务
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.8" data-path="../chapter_natural-language-processing-pretraining/bert.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert.html">
            
                    
                    14.8. 来自Transformers的双向编码器表示（BERT）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.9" data-path="../chapter_natural-language-processing-pretraining/bert-dataset.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert-dataset.html">
            
                    
                    14.9. 用于预训练BERT的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.10" data-path="../chapter_natural-language-processing-pretraining/bert-pretraining.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">
            
                    
                    14.10. 预训练BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.18" data-path="../chapter_natural-language-processing-applications/">
            
                <a href="../chapter_natural-language-processing-applications/">
            
                    
                    15. 自然语言处理：应用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.18.1" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                    
                    15.1. 情感分析及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.2" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                    
                    15.2. 情感分析：使用循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.3" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                    
                    15.3. 情感分析：使用卷积神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.4" data-path="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                    
                    15.4. 自然语言推断与数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.5" data-path="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                    
                    15.5. 自然语言推断：使用注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.6" data-path="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                    
                    15.6. 针对序列级和词元级应用微调BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.7" data-path="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                    
                    15.7. 自然语言推断：微调BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.19" data-path="../chapter_appendix-tools-for-deep-learning/">
            
                <a href="../chapter_appendix-tools-for-deep-learning/">
            
                    
                    16. 附录：深度学习工具
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.19.1" data-path="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                    
                    16.1. 使用Jupyter Notebook
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.2" data-path="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                    
                    16.2. 使用Amazon SageMaker
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.3" data-path="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                    
                    16.3. 使用Amazon EC2实例
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.4" data-path="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                    
                    16.4. 选择服务器和GPU
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.5" data-path="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                    
                    16.5. 为本书做贡献
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.6" data-path="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                    
                    16.6. d2l API 文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.20" data-path="../chapter_references/zreferences.html">
            
                <a href="../chapter_references/zreferences.html">
            
                    
                    参考文献
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >5.1. 层和块</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h2 id="层和块">层和块</h2>
<p>:label:<code>sec_model_construction</code></p>
<p>之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。
在这里，整个模型只有一个输出。
注意，单个神经网络
（1）接受一些输入；
（2）生成相应的标量输出；
（3）具有一组相关 <em>参数</em>（parameters），更新这些参数可以优化某目标函数。</p>
<p>然后，当考虑具有多个输出的网络时，
我们利用矢量化算法来描述整层神经元。
像单个神经元一样，层（1）接受一组输入，
（2）生成相应的输出，
（3）由一组可调整参数描述。
当我们使用softmax回归时，一个单层本身就是模型。
然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。</p>
<p>对于多层感知机而言，整个模型及其组成层都是这种架构。
整个模型接受原始输入（特征），生成输出（预测），
并包含一些参数（所有组成层的参数集合）。
同样，每个单独的层接收输入（由前一层提供），
生成输出（到下一层的输入），并且具有一组可调参数，
这些参数根据从下一层反向传播的信号进行更新。</p>
<p>事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。
例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，
这些层是由<em>层组</em>（groups of layers）的重复模式组成。
这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛
的识别和检测任务 :cite:<code>He.Zhang.Ren.ea.2016</code>。
目前ResNet架构仍然是许多视觉任务的首选架构。
在其他的领域，如自然语言处理和语音，
层组以各种重复模式排列的类似架构现在也是普遍存在。</p>
<p>为了实现这些复杂的网络，我们引入了神经网络<em>块</em>的概念。
<em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。
使用块进行抽象的一个好处是可以将一些块组合成更大的组件，
这一过程通常是递归的，如 :numref:<code>fig_blocks</code>所示。
通过定义代码来按需生成任意复杂度的块，
我们可以通过简洁的代码实现复杂的神经网络。</p>
<p><img src="../img/blocks.svg" alt="多个层被组合成块，形成更大的模型"></img>
:label:<code>fig_blocks</code></p>
<p>从编程的角度来看，块由<em>类</em>（class）表示。
它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，
并且必须存储任何必需的参数。
注意，有些块不需要任何参数。
最后，为了计算梯度，块必须具有反向传播函数。
在定义我们自己的块时，由于自动微分（在 :numref:<code>sec_autograd</code> 中引入）
提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。</p>
<p>在构造自定义块之前，(<strong>我们先回顾一下多层感知机</strong>)
（ :numref:<code>sec_mlp_concise</code> ）的代码。
下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，
然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> mxnet <span class="hljs-keyword">import</span> np, npx
<span class="hljs-keyword">from</span> mxnet.gluon <span class="hljs-keyword">import</span> nn
npx.set_np()

net = nn.Sequential()
net.add(nn.Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>))
net.add(nn.Dense(<span class="hljs-number">10</span>))
net.initialize()

X = np.random.uniform(size=(<span class="hljs-number">2</span>, <span class="hljs-number">20</span>))
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F

net = nn.Sequential(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>), nn.ReLU(), nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>))

X = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">20</span>)
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

net = tf.keras.models.Sequential([
    tf.keras.layers.Dense(<span class="hljs-number">256</span>, activation=tf.nn.relu),
    tf.keras.layers.Dense(<span class="hljs-number">10</span>),
])

X = tf.random.uniform((<span class="hljs-number">2</span>, <span class="hljs-number">20</span>))
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(action=<span class="hljs-string">'ignore'</span>)
<span class="hljs-keyword">import</span> paddle
<span class="hljs-keyword">from</span> paddle <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> paddle.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F

net = nn.Sequential(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>), nn.ReLU(), nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>))

X = paddle.rand([<span class="hljs-number">2</span>, <span class="hljs-number">20</span>])
net(X)
</code></pre>
<p>:begin<em>tab:<code>mxnet</code>
在这个例子中，我们通过实例化<code>nn.Sequential</code>来构建我们的模型，
返回的对象赋给<code>net</code>变量。
接下来，我们反复调用<code>net</code>变量的<code>add</code>函数，按照想要执行的顺序添加层。
简而言之，<code>nn.Sequential</code>定义了一种特殊类型的<code>Block</code>，
即在Gluon中表示块的类，它维护<code>Block</code>的有序列表。
<code>add</code>函数方便将每个连续的<code>Block</code>添加到列表中。
请注意，每层都是<code>Dense</code>类的一个实例，<code>Dense</code>类本身就是<code>Block</code>的子类。
到目前为止，我们一直在通过<code>net(X)</code>调用我们的模型来获得模型的输出。
这实际上是<code>net.forward(X)</code>的简写，
这是通过<code>Block</code>类的`<em>_call</em></em><code>函数实现的一个Python技巧。
前向传播（</code>forward<code>）函数非常简单：它将列表中的每个</code>Block<code>连接在一起，
将每个</code>Block`的输出作为输入传递给下一层。</p>
<p>:end_tab:</p>
<p>:begin<em>tab:<code>pytorch</code>
在这个例子中，我们通过实例化<code>nn.Sequential</code>来构建我们的模型，
层的执行顺序是作为参数传递的。
简而言之，(<strong><code>nn.Sequential</code>定义了一种特殊的<code>Module</code></strong>)，
即在PyTorch中表示一个块的类，
它维护了一个由<code>Module</code>组成的有序列表。
注意，两个全连接层都是<code>Linear</code>类的实例，
<code>Linear</code>类本身就是<code>Module</code>的子类。
另外，到目前为止，我们一直在通过<code>net(X)</code>调用我们的模型来获得模型的输出。
这实际上是`net.<em>_call</em></em>(X)`的简写。
这个前向传播函数非常简单：
它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。</p>
<p>:end_tab:</p>
<p>:begin<em>tab:<code>tensorflow</code>
在这个例子中，我们通过实例化<code>keras.models.Sequential</code>来构建我们的模型，
层的执行顺序是作为参数传递的。
简而言之，<code>Sequential</code>定义了一种特殊的<code>keras.Model</code>，
即在Keras中表示一个块的类。
它维护了一个由<code>Model</code>组成的有序列表，
注意两个全连接层都是<code>Model</code>类的实例，
这个类本身就是<code>Model</code>的子类。
前向传播（<code>call</code>）函数也非常简单：
它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。
注意，到目前为止，我们一直在通过<code>net(X)</code>调用我们的模型来获得模型的输出。
这实际上是<code>net.call(X)</code>的简写，
这是通过Block类的`<em>_call</em></em>`函数实现的一个Python技巧。
:end_tab:</p>
<p>:begin<em>tab:<code>paddle</code>
在这个例子中，我们通过实例化<code>nn.Sequential</code>来构建我们的模型，
层的执行顺序是作为参数传递的。
简而言之，(<strong><code>nn.Sequential</code>定义了一种特殊的<code>Layer</code></strong>)，
即在PaddlePaddle中表示一个块的类，
它维护了一个由<code>Layer</code>组成的有序列表。
注意，两个全连接层都是<code>Linear</code>类的实例，
<code>Linear</code>类本身就是<code>Layer</code>的子类。
另外，到目前为止，我们一直在通过<code>net(X)</code>调用我们的模型来获得模型的输出。
这实际上是`net.<em>_call</em></em>(X)`的简写。
这个前向传播函数非常简单：
它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。
:end_tab:</p>
<h2 id="自定义块">[<strong>自定义块</strong>]</h2>
<p>要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。
在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。</p>
<p>:begin_tab:<code>mxnet, tensorflow</code></p>
<ol>
<li>将输入数据作为其前向传播函数的参数。</li>
<li>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收任意维的输入，但是返回一个维度256的输出。</li>
<li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</li>
<li>存储和访问前向传播计算所需的参数。</li>
<li>根据需要初始化模型参数。
:end_tab:</li>
</ol>
<p>:begin_tab:<code>pytorch, paddle</code></p>
<ol>
<li>将输入数据作为其前向传播函数的参数。</li>
<li>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。</li>
<li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</li>
<li>存储和访问前向传播计算所需的参数。</li>
<li>根据需要初始化模型参数。
:end_tab:</li>
</ol>
<p>在下面的代码片段中，我们从零开始编写一个块。
它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。
注意，下面的<code>MLP</code>类继承了表示块的类。
我们的实现只需要提供我们自己的构造函数（Python中的<code>__init__</code>函数）和前向传播函数。</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>
        <span class="hljs-comment"># 调用MLP的父类Block的构造函数来执行必要的初始化。</span>
        <span class="hljs-comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span>
        super().__init__(**kwargs)
        self.hidden = nn.Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>)  <span class="hljs-comment"># 隐藏层</span>
        self.out = nn.Dense(<span class="hljs-number">10</span>)  <span class="hljs-comment"># 输出层</span>

    <span class="hljs-comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-keyword">return</span> self.out(self.hidden(X))
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span>
        <span class="hljs-comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span>
        super().__init__()
        self.hidden = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>)  <span class="hljs-comment"># 隐藏层</span>
        self.out = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)  <span class="hljs-comment"># 输出层</span>

    <span class="hljs-comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span>
        <span class="hljs-keyword">return</span> self.out(F.relu(self.hidden(X)))
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span><span class="hljs-params">(tf.keras.Model)</span>:</span>
    <span class="hljs-comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment"># 调用MLP的父类Model的构造函数来执行必要的初始化。</span>
        <span class="hljs-comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span>
        super().__init__()
        <span class="hljs-comment"># Hiddenlayer</span>
        self.hidden = tf.keras.layers.Dense(units=<span class="hljs-number">256</span>, activation=tf.nn.relu)
        self.out = tf.keras.layers.Dense(units=<span class="hljs-number">10</span>)  <span class="hljs-comment"># Outputlayer</span>

    <span class="hljs-comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-keyword">return</span> self.out(self.hidden((X)))
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLP</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment"># 调用`MLP`的父类Layer的构造函数来执行必要的初始化。</span>
        <span class="hljs-comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍）</span>
        super().__init__()
        self.hidden = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>)  <span class="hljs-comment"># 隐藏层</span>
        self.out = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)  <span class="hljs-comment"># 输出层</span>

    <span class="hljs-comment"># 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span>
        <span class="hljs-keyword">return</span> self.out(F.relu(self.hidden(X)))
</code></pre>
<p>我们首先看一下前向传播函数，它以<code>X</code>作为输入，
计算带有激活函数的隐藏表示，并输出其未规范化的输出值。
在这个<code>MLP</code>实现中，两个层都是实例变量。
要了解这为什么是合理的，可以想象实例化两个多层感知机（<code>net1</code>和<code>net2</code>），
并根据不同的数据对它们进行训练。
当然，我们希望它们学到两种不同的模型。</p>
<p>接着我们[<strong>实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层</strong>]。
注意一些关键细节：
首先，我们定制的<code>__init__</code>函数通过<code>super().__init__()</code>
调用父类的<code>__init__</code>函数，
省去了重复编写模版代码的痛苦。
然后，我们实例化两个全连接层，
分别为<code>self.hidden</code>和<code>self.out</code>。
注意，除非我们实现一个新的运算符，
否则我们不必担心反向传播函数或参数初始化，
系统将自动生成这些。</p>
<p>我们来试一下这个函数：</p>
<pre><code class="lang-python">net = MLP()
net.initialize()
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch, paddle</span>
net = MLP()
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
net = MLP()
net(X)
</code></pre>
<p>块的一个主要优点是它的多功能性。
我们可以子类化块以创建层（如全连接层的类）、
整个模型（如上面的<code>MLP</code>类）或具有中等复杂度的各种组件。
我们在接下来的章节中充分利用了这种多功能性，
比如在处理卷积神经网络时。</p>
<h2 id="顺序块">[<strong>顺序块</strong>]</h2>
<p>现在我们可以更仔细地看看<code>Sequential</code>类是如何工作的，
回想一下<code>Sequential</code>的设计是为了把其他模块串起来。
为了构建我们自己的简化的<code>MySequential</code>，
我们只需要定义两个关键函数：</p>
<ol>
<li>一种将块逐个追加到列表中的函数；</li>
<li>一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。</li>
</ol>
<p>下面的<code>MySequential</code>类提供了与默认<code>Sequential</code>类相同的功能。</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MySequential</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span><span class="hljs-params">(self, block)</span>:</span>
    <span class="hljs-comment"># 这里，block是Block子类的一个实例，我们假设它有一个唯一的名称。我们把它</span>
    <span class="hljs-comment"># 保存在'Block'类的成员变量_children中。block的类型是OrderedDict。</span>
    <span class="hljs-comment"># 当MySequential实例调用initialize函数时，系统会自动初始化_children</span>
    <span class="hljs-comment"># 的所有成员</span>
        self._children[block.name] = block

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span>
        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self._children.values():
            X = block(X)
        <span class="hljs-keyword">return</span> X
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MySequential</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, *args)</span>:</span>
        super().__init__()
        <span class="hljs-keyword">for</span> idx, module <span class="hljs-keyword">in</span> enumerate(args):
            <span class="hljs-comment"># 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员</span>
            <span class="hljs-comment"># 变量_modules中。_module的类型是OrderedDict</span>
            self._modules[str(idx)] = module

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span>
        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self._modules.values():
            X = block(X)
        <span class="hljs-keyword">return</span> X
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MySequential</span><span class="hljs-params">(tf.keras.Model)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, *args)</span>:</span>
        super().__init__()
        self.modules = []
        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> args:
            <span class="hljs-comment"># 这里，block是tf.keras.layers.Layer子类的一个实例</span>
            self.modules.append(block)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> self.modules:
            X = module(X)
        <span class="hljs-keyword">return</span> X
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MySequential</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, *layers)</span>:</span>
        super(MySequential, self).__init__()
        <span class="hljs-comment"># 如果传入的是一个tuple</span>
        <span class="hljs-keyword">if</span> len(layers) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> isinstance(layers[<span class="hljs-number">0</span>], tuple): 
            <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> layers:
                <span class="hljs-comment"># add_sublayer方法会将layer添加到self._sub_layers(一个tuple)</span>
                self.add_sublayer(name, layer)  
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">for</span> idx, layer <span class="hljs-keyword">in</span> enumerate(layers):
                self.add_sublayer(str(idx), layer)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span>
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self._sub_layers.values():
            X = layer(X)
        <span class="hljs-keyword">return</span> X
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>add</code>函数向有序字典<code>_children</code>添加一个块。
读者可能会好奇为什么每个Gluon中的<code>Block</code>都有一个<code>_children</code>属性？
以及为什么我们使用它而不是自己定义一个Python列表？
简而言之，<code>_children</code>的主要优点是：
在块的参数初始化过程中，
Gluon知道在<code>_children</code>字典中查找需要初始化参数的子块。
:end_tab:</p>
<p>:begin<em>tab:<code>pytorch</code>
`<em>_init</em></em><code>函数将每个模块逐个添加到有序字典</code>_modules<code>中。
读者可能会好奇为什么每个</code>Module<code>都有一个</code>_modules<code>属性？
以及为什么我们使用它而不是自己定义一个Python列表？
简而言之，</code>_modules<code>的主要优点是：
在模块的参数初始化过程中，
系统知道在</code>_modules`字典中查找需要初始化参数的子块。
:end_tab:</p>
<p>:begin<em>tab:<code>paddle</code>
`<em>_init</em></em><code>函数将每个模块逐个添加到有序字典</code>_sub_layers<code>中。
你可能会好奇为什么每个</code>Layer<code>都有一个</code>_sub_layers<code>属性？
以及为什么我们使用它而不是自己定义一个Python列表？
简而言之，</code>_sub_layers<code>的主要优点是：
在模块的参数初始化过程中，
系统知道在</code>_sub_layers`字典中查找需要初始化参数的子块。
:end_tab:</p>
<p>当<code>MySequential</code>的前向传播函数被调用时，
每个添加的块都按照它们被添加的顺序执行。
现在可以使用我们的<code>MySequential</code>类重新实现多层感知机。</p>
<pre><code class="lang-python">net = MySequential()
net.add(nn.Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>))
net.add(nn.Dense(<span class="hljs-number">10</span>))
net.initialize()
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch, paddle</span>
net = MySequential(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>), nn.ReLU(), nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>))
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
net = MySequential(
    tf.keras.layers.Dense(units=<span class="hljs-number">256</span>, activation=tf.nn.relu),
    tf.keras.layers.Dense(<span class="hljs-number">10</span>))
net(X)
</code></pre>
<p>请注意，<code>MySequential</code>的用法与之前为<code>Sequential</code>类编写的代码相同
（如 :numref:<code>sec_mlp_concise</code> 中所述）。</p>
<h2 id="在前向传播函数中执行代码">[<strong>在前向传播函数中执行代码</strong>]</h2>
<p><code>Sequential</code>类使模型构造变得简单，
允许我们组合新的架构，而不必定义自己的类。
然而，并不是所有的架构都是简单的顺序架构。
当需要更强的灵活性时，我们需要定义自己的块。
例如，我们可能希望在前向传播函数中执行Python的控制流。
此外，我们可能希望执行任意的数学运算，
而不是简单地依赖预定义的神经网络层。</p>
<p>到目前为止，
我们网络中的所有操作都对网络的激活值及网络的参数起作用。
然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，
我们称之为<em>常数参数</em>（constant parameter）。
例如，我们需要一个计算函数
$f(\mathbf{x},\mathbf{w}) = c \cdot \mathbf{w}^\top \mathbf{x}$的层，
其中$\mathbf{x}$是输入，
$\mathbf{w}$是参数，
$c$是某个在优化过程中没有更新的指定常量。
因此我们实现了一个<code>FixedHiddenMLP</code>类，如下所示：</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FixedHiddenMLP</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>
        super().__init__(**kwargs)
        <span class="hljs-comment"># 使用get_constant函数创建的随机权重参数在训练期间不会更新（即为常量参数）</span>
        self.rand_weight = self.params.get_constant(
            <span class="hljs-string">'rand_weight'</span>, np.random.uniform(size=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)))
        self.dense = nn.Dense(<span class="hljs-number">20</span>, activation=<span class="hljs-string">'relu'</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        X = self.dense(X)
        <span class="hljs-comment"># 使用创建的常量参数以及relu和dot函数</span>
        X = npx.relu(np.dot(X, self.rand_weight.data()) + <span class="hljs-number">1</span>)
        <span class="hljs-comment"># 复用全连接层。这相当于两个全连接层共享参数</span>
        X = self.dense(X)
        <span class="hljs-comment"># 控制流</span>
        <span class="hljs-keyword">while</span> np.abs(X).sum() &gt; <span class="hljs-number">1</span>:
            X /= <span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> X.sum()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FixedHiddenMLP</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        <span class="hljs-comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span>
        self.rand_weight = torch.rand((<span class="hljs-number">20</span>, <span class="hljs-number">20</span>), requires_grad=<span class="hljs-keyword">False</span>)
        self.linear = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        X = self.linear(X)
        <span class="hljs-comment"># 使用创建的常量参数以及relu和mm函数</span>
        X = F.relu(torch.mm(X, self.rand_weight) + <span class="hljs-number">1</span>)
        <span class="hljs-comment"># 复用全连接层。这相当于两个全连接层共享参数</span>
        X = self.linear(X)
        <span class="hljs-comment"># 控制流</span>
        <span class="hljs-keyword">while</span> X.abs().sum() &gt; <span class="hljs-number">1</span>:
            X /= <span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> X.sum()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FixedHiddenMLP</span><span class="hljs-params">(tf.keras.Model)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()
        <span class="hljs-comment"># 使用tf.constant函数创建的随机权重参数在训练期间不会更新（即为常量参数）</span>
        self.rand_weight = tf.constant(tf.random.uniform((<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)))
        self.dense = tf.keras.layers.Dense(<span class="hljs-number">20</span>, activation=tf.nn.relu)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>
        X = self.flatten(inputs)
        <span class="hljs-comment"># 使用创建的常量参数以及relu和matmul函数</span>
        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + <span class="hljs-number">1</span>)
        <span class="hljs-comment"># 复用全连接层。这相当于两个全连接层共享参数。</span>
        X = self.dense(X)
        <span class="hljs-comment"># 控制流</span>
        <span class="hljs-keyword">while</span> tf.reduce_sum(tf.math.abs(X)) &gt; <span class="hljs-number">1</span>:
            X /= <span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> tf.reduce_sum(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FixedHiddenMLP</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        <span class="hljs-comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变。</span>
        self.rand_weight = paddle.rand([<span class="hljs-number">20</span>, <span class="hljs-number">20</span>])
        self.linear = nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        X = self.linear(X)
        <span class="hljs-comment"># 使用创建的常量参数以及relu和mm函数。</span>
        X = F.relu(paddle.tensor.mm(X, self.rand_weight) + <span class="hljs-number">1</span>)
        <span class="hljs-comment"># 复用全连接层。这相当于两个全连接层共享参数。</span>
        X = self.linear(X)
        <span class="hljs-comment"># 控制流</span>
        <span class="hljs-keyword">while</span> X.abs().sum() &gt; <span class="hljs-number">1</span>:
            X /= <span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> X.sum()
</code></pre>
<p>在这个<code>FixedHiddenMLP</code>模型中，我们实现了一个隐藏层，
其权重（<code>self.rand_weight</code>）在实例化时被随机初始化，之后为常量。
这个权重不是一个模型参数，因此它永远不会被反向传播更新。
然后，神经网络将这个固定层的输出通过一个全连接层。</p>
<p>注意，在返回输出之前，模型做了一些不寻常的事情：
它运行了一个while循环，在$L_1$范数大于$1$的条件下，
将输出向量除以$2$，直到它满足条件为止。
最后，模型返回了<code>X</code>中所有项的和。
注意，此操作可能不会常用于在任何实际任务中，
我们只展示如何将任意代码集成到神经网络计算的流程中。</p>
<pre><code class="lang-python">net = FixedHiddenMLP()
net.initialize()
net(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch, tensorflow, paddle</span>
net = FixedHiddenMLP()
net(X)
</code></pre>
<p>我们可以[<strong>混合搭配各种组合块的方法</strong>]。
在下面的例子中，我们以一些想到的方法嵌套块。</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestMLP</span><span class="hljs-params">(nn.Block)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>
        super().__init__(**kwargs)
        self.net = nn.Sequential()
        self.net.add(nn.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>),
                     nn.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>))
        self.dense = nn.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">'relu'</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-keyword">return</span> self.dense(self.net(X))

chimera = nn.Sequential()
chimera.add(NestMLP(), nn.Dense(<span class="hljs-number">20</span>), FixedHiddenMLP())
chimera.initialize()
chimera(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestMLP</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        self.net = nn.Sequential(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">64</span>), nn.ReLU(),
                                 nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>), nn.ReLU())
        self.linear = nn.Linear(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-keyword">return</span> self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="hljs-number">16</span>, <span class="hljs-number">20</span>), FixedHiddenMLP())
chimera(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestMLP</span><span class="hljs-params">(tf.keras.Model)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        self.net = tf.keras.Sequential()
        self.net.add(tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=tf.nn.relu))
        self.net.add(tf.keras.layers.Dense(<span class="hljs-number">32</span>, activation=tf.nn.relu))
        self.dense = tf.keras.layers.Dense(<span class="hljs-number">16</span>, activation=tf.nn.relu)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span><span class="hljs-params">(self, inputs)</span>:</span>
        <span class="hljs-keyword">return</span> self.dense(self.net(inputs))

chimera = tf.keras.Sequential()
chimera.add(NestMLP())
chimera.add(tf.keras.layers.Dense(<span class="hljs-number">20</span>))
chimera.add(FixedHiddenMLP())
chimera(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NestMLP</span><span class="hljs-params">(nn.Layer)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        self.net = nn.Sequential(nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">64</span>), nn.ReLU(),
                                 nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>), nn.ReLU())
        self.linear = nn.Linear(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-keyword">return</span> self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="hljs-number">16</span>, <span class="hljs-number">20</span>), FixedHiddenMLP())
chimera(X)
</code></pre>
<h2 id="效率">效率</h2>
<p>:begin_tab:<code>mxnet</code>
读者可能会开始担心操作效率的问题。
毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、
代码执行和许多其他的Python代码。
Python的问题<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank">全局解释器锁</a>
是众所周知的。
在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。</p>
<p>提高Python速度的最好方法是完全避免使用Python。
Gluon这样做的一个方法是允许<em>混合式编程</em>（hybridization），这将在后面描述。
Python解释器在第一次调用块时执行它。
Gluon运行时记录正在发生的事情，以及下一次它将对Python调用加速。
在某些情况下，这可以大大加快运行速度，
但当控制流（如上所述）在不同的网络通路上引导不同的分支时，需要格外小心。
我们建议感兴趣的读者在读完本章后，阅读混合式编程部分（ :numref:<code>sec_hybridize</code> ）来了解编译。
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
读者可能会开始担心操作效率的问题。
毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、
代码执行和许多其他的Python代码。
Python的问题<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank">全局解释器锁</a>
是众所周知的。
在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
读者可能会开始担心操作效率的问题。
毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、
代码执行和许多其他的Python代码。
Python的问题<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank">全局解释器锁</a>
是众所周知的。
在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。
:end_tab:</p>
<p>:begin_tab:<code>paddle</code>
你可能会开始担心操作效率的问题。
毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、
代码执行和许多其他的Python代码。
Python的问题<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank">全局解释器锁</a>
是众所周知的。
在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。
:end_tab:</p>
<h2 id="小结">小结</h2>
<ul>
<li>一个块可以由许多层组成；一个块可以由许多块组成。</li>
<li>块可以包含代码。</li>
<li>块负责大量的内部处理，包括参数初始化和反向传播。</li>
<li>层和块的顺序连接由<code>Sequential</code>块处理。</li>
</ul>
<h2 id="练习">练习</h2>
<ol>
<li>如果将<code>MySequential</code>中存储块的方式更改为Python列表，会出现什么样的问题？</li>
<li>实现一个块，它以两个块为参数，例如<code>net1</code>和<code>net2</code>，并返回前向传播中两个网络的串联输出。这也被称为平行块。</li>
<li>假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/1828" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1827" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1826" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>paddle</code>
<a href="https://discuss.d2l.ai/t/11777" target="_blank">Discussions</a>
:end_tab:</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="./" class="navigation navigation-prev " aria-label="Previous page: 5. 深度学习计算">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="parameters.html" class="navigation navigation-next " aria-label="Next page: 5.2. 参数管理">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"description":"CoderFAN 资料库 动手学深度学习","title":"5.1. 层和块","level":"1.2.8.1","depth":3,"next":{"title":"5.2. 参数管理","level":"1.2.8.2","depth":3,"path":"chapter_deep-learning-computation/parameters.md","ref":"chapter_deep-learning-computation/parameters.md","articles":[]},"previous":{"title":"5. 深度学习计算","level":"1.2.8","depth":2,"path":"chapter_deep-learning-computation/index.md","ref":"chapter_deep-learning-computation/index.md","articles":[{"title":"5.1. 层和块","level":"1.2.8.1","depth":3,"path":"chapter_deep-learning-computation/model-construction.md","ref":"chapter_deep-learning-computation/model-construction.md","articles":[]},{"title":"5.2. 参数管理","level":"1.2.8.2","depth":3,"path":"chapter_deep-learning-computation/parameters.md","ref":"chapter_deep-learning-computation/parameters.md","articles":[]},{"title":"5.3. 延后初始化","level":"1.2.8.3","depth":3,"path":"chapter_deep-learning-computation/deferred-init.md","ref":"chapter_deep-learning-computation/deferred-init.md","articles":[]},{"title":"5.4. 自定义层","level":"1.2.8.4","depth":3,"path":"chapter_deep-learning-computation/custom-layer.md","ref":"chapter_deep-learning-computation/custom-layer.md","articles":[]},{"title":"5.5. 读写文件","level":"1.2.8.5","depth":3,"path":"chapter_deep-learning-computation/read-write.md","ref":"chapter_deep-learning-computation/read-write.md","articles":[]},{"title":"5.6. GPU计算","level":"1.2.8.6","depth":3,"path":"chapter_deep-learning-computation/use-gpu.md","ref":"chapter_deep-learning-computation/use-gpu.md","articles":[]}]},"dir":"ltr"},"config":{"plugins":["-search","-livereload","-lunr","-fontsettings","highlight","expandable-chapters-small","back-to-top-button","github","code","theme-default"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"highlight":{"lang":{"eval_rst":"rst","toc":"text"}},"github":{"url":"https://github.com/KittenCN"},"expandable-chapters-small":{},"back-to-top-button":{},"code":{"copyButtons":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"Todd Lyu","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"CoderFAN 资料库","gitbook":"*","description":"CoderFAN 资料库 动手学深度学习"},"file":{"path":"chapter_deep-learning-computation/model-construction.md","mtime":"2025-05-12T03:21:13.163Z","type":"markdown"},"gitbook":{"version":"6.0.3","time":"2025-05-12T03:23:46.668Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    

    </body>
</html>

