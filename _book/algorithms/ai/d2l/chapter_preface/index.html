
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>前言 · CoderFAN 资料库</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="CoderFAN 资料库 动手学深度学习">
        <meta name="generator" content="GitBook 6.0.3">
        <meta name="author" content="Todd Lyu">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../chapter_installation/" />
    
    
    <link rel="prev" href="../d2l.html" />
    
    <!-- MathJax 配置：唯一且完整 -->
<script>
    window.MathJax = {
      tex: {
        inlineMath:  [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        strict: "ignore",
        macros: { "\\E":"\\mathbb{E}", "\\Var":"\\operatorname{Var}" }
      },
    };
    </script>
    
    <!-- 核心脚本（defer不阻塞渲染） -->
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
    <!-- 放在 tex-chtml.js 之后 -->
    <script>
    (function () {
      function typeset() {
        if (window.MathJax && MathJax.typesetPromise) {
          MathJax.typesetPromise().catch(console.error);
        }
      }
    
      /* 第一次正文插入 */
      document.addEventListener('DOMContentLoaded', typeset);
    
      /*   关键：等待 gitbook.js 初始化成功   */
      function hookGitBook() {
        if (window.gitbook && gitbook.events) {
          gitbook.events.bind('page.change', typeset);   // 切章排版
        } else {
          /* gitbook.js 还没加载完 → 100 ms 后再试 */
          setTimeout(hookGitBook, 100);
        }
      }
      hookGitBook();   // 启动递归等待
    })();
    </script>
    
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
            
                <nav role="navigation">
                <a href="../.." class="btn"><b></b>&#128512;返回上层&#128512;</b></a>
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../d2l.md">
            
                <span>
            
                    
                    动手学深度学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.2.1" data-path="./">
            
                <a href="./">
            
                    
                    前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../chapter_installation/">
            
                <a href="../chapter_installation/">
            
                    
                    安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../chapter_notation/">
            
                <a href="../chapter_notation/">
            
                    
                    符号
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../chapter_introduction/">
            
                <a href="../chapter_introduction/">
            
                    
                    1. 引言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../chapter_preliminaries/">
            
                <a href="../chapter_preliminaries/">
            
                    
                    2. 预备知识
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="../chapter_preliminaries/ndarray.html">
            
                <a href="../chapter_preliminaries/ndarray.html">
            
                    
                    2.1. 数据操作
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.2" data-path="../chapter_preliminaries/pandas.html">
            
                <a href="../chapter_preliminaries/pandas.html">
            
                    
                    2.2. 数据预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.3" data-path="../chapter_preliminaries/linear-algebra.html">
            
                <a href="../chapter_preliminaries/linear-algebra.html">
            
                    
                    2.3. 线性代数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.4" data-path="../chapter_preliminaries/calculus.html">
            
                <a href="../chapter_preliminaries/calculus.html">
            
                    
                    2.4. 微积分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.5" data-path="../chapter_preliminaries/autograd.html">
            
                <a href="../chapter_preliminaries/autograd.html">
            
                    
                    2.5. 自动微分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.6" data-path="../chapter_preliminaries/probability.html">
            
                <a href="../chapter_preliminaries/probability.html">
            
                    
                    2.6. 概率
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5.7" data-path="../chapter_preliminaries/lookup-api.html">
            
                <a href="../chapter_preliminaries/lookup-api.html">
            
                    
                    2.7. 查阅文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../chapter_linear-networks/">
            
                <a href="../chapter_linear-networks/">
            
                    
                    3. 线性神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="../chapter_linear-networks/linear-regression.html">
            
                <a href="../chapter_linear-networks/linear-regression.html">
            
                    
                    3.1. 线性回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="../chapter_linear-networks/linear-regression-scratch.html">
            
                <a href="../chapter_linear-networks/linear-regression-scratch.html">
            
                    
                    3.2. 线性回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="../chapter_linear-networks/linear-regression-concise.html">
            
                <a href="../chapter_linear-networks/linear-regression-concise.html">
            
                    
                    3.3. 线性回归的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.4" data-path="../chapter_linear-networks/softmax-regression.html">
            
                <a href="../chapter_linear-networks/softmax-regression.html">
            
                    
                    3.4. softmax回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.5" data-path="../chapter_linear-networks/image-classification-dataset.html">
            
                <a href="../chapter_linear-networks/image-classification-dataset.html">
            
                    
                    3.5. 图像分类数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.6" data-path="../chapter_linear-networks/softmax-regression-scratch.html">
            
                <a href="../chapter_linear-networks/softmax-regression-scratch.html">
            
                    
                    3.6. softmax回归的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.7" data-path="../chapter_linear-networks/softmax-regression-concise.html">
            
                <a href="../chapter_linear-networks/softmax-regression-concise.html">
            
                    
                    3.7. softmax回归的简洁实现
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../chapter_multilayer-perceptrons/">
            
                <a href="../chapter_multilayer-perceptrons/">
            
                    
                    4. 多层感知机
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="../chapter_multilayer-perceptrons/mlp.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp.html">
            
                    
                    4.1. 多层感知机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-scratch.html">
            
                    
                    4.2. 多层感知机的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                <a href="../chapter_multilayer-perceptrons/mlp-concise.html">
            
                    
                    4.3. 多层感知机的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                <a href="../chapter_multilayer-perceptrons/underfit-overfit.html">
            
                    
                    4.4. 模型选择、欠拟合和过拟合
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.5" data-path="../chapter_multilayer-perceptrons/weight-decay.html">
            
                <a href="../chapter_multilayer-perceptrons/weight-decay.html">
            
                    
                    4.5. 权重衰减
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.6" data-path="../chapter_multilayer-perceptrons/dropout.html">
            
                <a href="../chapter_multilayer-perceptrons/dropout.html">
            
                    
                    4.6. 暂退法（Dropout）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.7" data-path="../chapter_multilayer-perceptrons/backprop.html">
            
                <a href="../chapter_multilayer-perceptrons/backprop.html">
            
                    
                    4.7. 前向传播、反向传播和计算图
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.8" data-path="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                <a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">
            
                    
                    4.8. 数值稳定性和模型初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.9" data-path="../chapter_multilayer-perceptrons/environment.html">
            
                <a href="../chapter_multilayer-perceptrons/environment.html">
            
                    
                    4.9. 环境和分布偏移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.10" data-path="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                <a href="../chapter_multilayer-perceptrons/kaggle-house-price.html">
            
                    
                    4.10. 实战Kaggle比赛：预测房价
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../chapter_deep-learning-computation/">
            
                <a href="../chapter_deep-learning-computation/">
            
                    
                    5. 深度学习计算
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="../chapter_deep-learning-computation/model-construction.html">
            
                <a href="../chapter_deep-learning-computation/model-construction.html">
            
                    
                    5.1. 层和块
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.2" data-path="../chapter_deep-learning-computation/parameters.html">
            
                <a href="../chapter_deep-learning-computation/parameters.html">
            
                    
                    5.2. 参数管理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.3" data-path="../chapter_deep-learning-computation/deferred-init.html">
            
                <a href="../chapter_deep-learning-computation/deferred-init.html">
            
                    
                    5.3. 延后初始化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.4" data-path="../chapter_deep-learning-computation/custom-layer.html">
            
                <a href="../chapter_deep-learning-computation/custom-layer.html">
            
                    
                    5.4. 自定义层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.5" data-path="../chapter_deep-learning-computation/read-write.html">
            
                <a href="../chapter_deep-learning-computation/read-write.html">
            
                    
                    5.5. 读写文件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8.6" data-path="../chapter_deep-learning-computation/use-gpu.html">
            
                <a href="../chapter_deep-learning-computation/use-gpu.html">
            
                    
                    5.6. GPU计算
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../chapter_convolutional-neural-networks/">
            
                <a href="../chapter_convolutional-neural-networks/">
            
                    
                    6. 卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.9.1" data-path="../chapter_convolutional-neural-networks/why-conv.html">
            
                <a href="../chapter_convolutional-neural-networks/why-conv.html">
            
                    
                    6.1. 从全连接层到卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.2" data-path="../chapter_convolutional-neural-networks/conv-layer.html">
            
                <a href="../chapter_convolutional-neural-networks/conv-layer.html">
            
                    
                    6.2. 图像卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.3" data-path="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                <a href="../chapter_convolutional-neural-networks/padding-and-strides.html">
            
                    
                    6.3. 填充和步幅
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.4" data-path="../chapter_convolutional-neural-networks/channels.html">
            
                <a href="../chapter_convolutional-neural-networks/channels.html">
            
                    
                    6.4. 多输入多输出通道
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.5" data-path="../chapter_convolutional-neural-networks/pooling.html">
            
                <a href="../chapter_convolutional-neural-networks/pooling.html">
            
                    
                    6.5. 汇聚层
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9.6" data-path="../chapter_convolutional-neural-networks/lenet.html">
            
                <a href="../chapter_convolutional-neural-networks/lenet.html">
            
                    
                    6.6. 卷积神经网络（LeNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../chapter_convolutional-modern/">
            
                <a href="../chapter_convolutional-modern/">
            
                    
                    7. 现代卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.10.1" data-path="../chapter_convolutional-modern/alexnet.html">
            
                <a href="../chapter_convolutional-modern/alexnet.html">
            
                    
                    7.1. 深度卷积神经网络（AlexNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.2" data-path="../chapter_convolutional-modern/vgg.html">
            
                <a href="../chapter_convolutional-modern/vgg.html">
            
                    
                    7.2. 使用块的网络（VGG）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.3" data-path="../chapter_convolutional-modern/nin.html">
            
                <a href="../chapter_convolutional-modern/nin.html">
            
                    
                    7.3. 网络中的网络（NiN）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.4" data-path="../chapter_convolutional-modern/googlenet.html">
            
                <a href="../chapter_convolutional-modern/googlenet.html">
            
                    
                    7.4. 含并行连结的网络（GoogLeNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.5" data-path="../chapter_convolutional-modern/batch-norm.html">
            
                <a href="../chapter_convolutional-modern/batch-norm.html">
            
                    
                    7.5. 批量规范化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.6" data-path="../chapter_convolutional-modern/resnet.html">
            
                <a href="../chapter_convolutional-modern/resnet.html">
            
                    
                    7.6. 残差网络（ResNet）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10.7" data-path="../chapter_convolutional-modern/densenet.html">
            
                <a href="../chapter_convolutional-modern/densenet.html">
            
                    
                    7.7. 稠密连接网络（DenseNet）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../chapter_recurrent-neural-networks/">
            
                <a href="../chapter_recurrent-neural-networks/">
            
                    
                    8. 循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.11.1" data-path="../chapter_recurrent-neural-networks/sequence.html">
            
                <a href="../chapter_recurrent-neural-networks/sequence.html">
            
                    
                    8.1. 序列模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.2" data-path="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                <a href="../chapter_recurrent-neural-networks/text-preprocessing.html">
            
                    
                    8.2. 文本预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.3" data-path="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                <a href="../chapter_recurrent-neural-networks/language-models-and-dataset.html">
            
                    
                    8.3. 语言模型和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.4" data-path="../chapter_recurrent-neural-networks/rnn.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn.html">
            
                    
                    8.4. 循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.5" data-path="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-scratch.html">
            
                    
                    8.5. 循环神经网络的从零开始实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.6" data-path="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                <a href="../chapter_recurrent-neural-networks/rnn-concise.html">
            
                    
                    8.6. 循环神经网络的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11.7" data-path="../chapter_recurrent-neural-networks/bptt.html">
            
                <a href="../chapter_recurrent-neural-networks/bptt.html">
            
                    
                    8.7. 通过时间反向传播
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../chapter_recurrent-modern/">
            
                <a href="../chapter_recurrent-modern/">
            
                    
                    9. 现代循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.12.1" data-path="../chapter_recurrent-modern/gru.html">
            
                <a href="../chapter_recurrent-modern/gru.html">
            
                    
                    9.1. 门控循环单元（GRU）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.2" data-path="../chapter_recurrent-modern/lstm.html">
            
                <a href="../chapter_recurrent-modern/lstm.html">
            
                    
                    9.2. 长短期记忆（LSTM）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.3" data-path="../chapter_recurrent-modern/deep-rnn.html">
            
                <a href="../chapter_recurrent-modern/deep-rnn.html">
            
                    
                    9.3. 深度循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.4" data-path="../chapter_recurrent-modern/bi-rnn.html">
            
                <a href="../chapter_recurrent-modern/bi-rnn.html">
            
                    
                    9.4. 双向循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.5" data-path="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                <a href="../chapter_recurrent-modern/machine-translation-and-dataset.html">
            
                    
                    9.5. 机器翻译及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.6" data-path="../chapter_recurrent-modern/encoder-decoder.html">
            
                <a href="../chapter_recurrent-modern/encoder-decoder.html">
            
                    
                    9.6. 编码器—解码器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.7" data-path="../chapter_recurrent-modern/seq2seq.html">
            
                <a href="../chapter_recurrent-modern/seq2seq.html">
            
                    
                    9.7. 序列到序列学习（seq2seq）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12.8" data-path="../chapter_recurrent-modern/beam-search.html">
            
                <a href="../chapter_recurrent-modern/beam-search.html">
            
                    
                    9.8. 束搜索
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="../chapter_attention-mechanisms/">
            
                <a href="../chapter_attention-mechanisms/">
            
                    
                    10. 注意力机制
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.13.1" data-path="../chapter_attention-mechanisms/attention-cues.html">
            
                <a href="../chapter_attention-mechanisms/attention-cues.html">
            
                    
                    10.1. 注意力提示
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.2" data-path="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                <a href="../chapter_attention-mechanisms/nadaraya-waston.html">
            
                    
                    10.2. 注意力汇聚：Nadaraya-Watson 核回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.3" data-path="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                <a href="../chapter_attention-mechanisms/attention-scoring-functions.html">
            
                    
                    10.3. 注意力评分函数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.4" data-path="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                <a href="../chapter_attention-mechanisms/bahdanau-attention.html">
            
                    
                    10.4. Bahdanau 注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.5" data-path="../chapter_attention-mechanisms/multihead-attention.html">
            
                <a href="../chapter_attention-mechanisms/multihead-attention.html">
            
                    
                    10.5. 多头注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.6" data-path="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                <a href="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">
            
                    
                    10.6. 自注意力和位置编码
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13.7" data-path="../chapter_attention-mechanisms/transformer.html">
            
                <a href="../chapter_attention-mechanisms/transformer.html">
            
                    
                    10.7. Transformer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.14" data-path="../chapter_optimization/">
            
                <a href="../chapter_optimization/">
            
                    
                    11. 优化算法
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.14.1" data-path="../chapter_optimization/optimization-intro.html">
            
                <a href="../chapter_optimization/optimization-intro.html">
            
                    
                    11.1. 优化与深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.2" data-path="../chapter_optimization/convexity.html">
            
                <a href="../chapter_optimization/convexity.html">
            
                    
                    11.2. 凸性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.3" data-path="../chapter_optimization/gd.html">
            
                <a href="../chapter_optimization/gd.html">
            
                    
                    11.3. 梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.4" data-path="../chapter_optimization/sgd.html">
            
                <a href="../chapter_optimization/sgd.html">
            
                    
                    11.4. 随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.5" data-path="../chapter_optimization/minibatch-sgd.html">
            
                <a href="../chapter_optimization/minibatch-sgd.html">
            
                    
                    11.5. 小批量随机梯度下降
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.6" data-path="../chapter_optimization/momentum.html">
            
                <a href="../chapter_optimization/momentum.html">
            
                    
                    11.6. 动量法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.7" data-path="../chapter_optimization/adagrad.html">
            
                <a href="../chapter_optimization/adagrad.html">
            
                    
                    11.7. AdaGrad算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.8" data-path="../chapter_optimization/rmsprop.html">
            
                <a href="../chapter_optimization/rmsprop.html">
            
                    
                    11.8. RMSProp算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.9" data-path="../chapter_optimization/adadelta.html">
            
                <a href="../chapter_optimization/adadelta.html">
            
                    
                    11.9. Adadelta
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.10" data-path="../chapter_optimization/adam.html">
            
                <a href="../chapter_optimization/adam.html">
            
                    
                    11.10. Adam算法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14.11" data-path="../chapter_optimization/lr-scheduler.html">
            
                <a href="../chapter_optimization/lr-scheduler.html">
            
                    
                    11.11. 学习率调度器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.15" data-path="../chapter_computational-performance/">
            
                <a href="../chapter_computational-performance/">
            
                    
                    12. 计算性能
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.15.1" data-path="../chapter_computational-performance/hybridize.html">
            
                <a href="../chapter_computational-performance/hybridize.html">
            
                    
                    12.1. 编译器和解释器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.2" data-path="../chapter_computational-performance/async-computation.html">
            
                <a href="../chapter_computational-performance/async-computation.html">
            
                    
                    12.2. 异步计算
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.3" data-path="../chapter_computational-performance/auto-parallelism.html">
            
                <a href="../chapter_computational-performance/auto-parallelism.html">
            
                    
                    12.3. 自动并行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.4" data-path="../chapter_computational-performance/hardware.html">
            
                <a href="../chapter_computational-performance/hardware.html">
            
                    
                    12.4. 硬件
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.5" data-path="../chapter_computational-performance/multiple-gpus.html">
            
                <a href="../chapter_computational-performance/multiple-gpus.html">
            
                    
                    12.5. 多GPU训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.6" data-path="../chapter_computational-performance/multiple-gpus-concise.html">
            
                <a href="../chapter_computational-performance/multiple-gpus-concise.html">
            
                    
                    12.6. 多GPU的简洁实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15.7" data-path="../chapter_computational-performance/parameterserver.html">
            
                <a href="../chapter_computational-performance/parameterserver.html">
            
                    
                    12.7. 参数服务器
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.16" data-path="../chapter_computer-vision/">
            
                <a href="../chapter_computer-vision/">
            
                    
                    13. 计算机视觉
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.16.1" data-path="../chapter_computer-vision/image-augmentation.html">
            
                <a href="../chapter_computer-vision/image-augmentation.html">
            
                    
                    13.1. 图像增广
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.2" data-path="../chapter_computer-vision/fine-tuning.html">
            
                <a href="../chapter_computer-vision/fine-tuning.html">
            
                    
                    13.2. 微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.3" data-path="../chapter_computer-vision/bounding-box.html">
            
                <a href="../chapter_computer-vision/bounding-box.html">
            
                    
                    13.3. 目标检测和边界框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.4" data-path="../chapter_computer-vision/anchor.html">
            
                <a href="../chapter_computer-vision/anchor.html">
            
                    
                    13.4. 锚框
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.5" data-path="../chapter_computer-vision/multiscale-object-detection.html">
            
                <a href="../chapter_computer-vision/multiscale-object-detection.html">
            
                    
                    13.5. 多尺度目标检测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.6" data-path="../chapter_computer-vision/object-detection-dataset.html">
            
                <a href="../chapter_computer-vision/object-detection-dataset.html">
            
                    
                    13.6. 目标检测数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.7" data-path="../chapter_computer-vision/ssd.html">
            
                <a href="../chapter_computer-vision/ssd.html">
            
                    
                    13.7. 单发多框检测（SSD）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.8" data-path="../chapter_computer-vision/rcnn.html">
            
                <a href="../chapter_computer-vision/rcnn.html">
            
                    
                    13.8. 区域卷积神经网络（R-CNN）系列
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.9" data-path="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                <a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">
            
                    
                    13.9. 语义分割和数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.10" data-path="../chapter_computer-vision/transposed-conv.html">
            
                <a href="../chapter_computer-vision/transposed-conv.html">
            
                    
                    13.10. 转置卷积
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.11" data-path="../chapter_computer-vision/fcn.html">
            
                <a href="../chapter_computer-vision/fcn.html">
            
                    
                    13.11. 全卷积网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.12" data-path="../chapter_computer-vision/neural-style.html">
            
                <a href="../chapter_computer-vision/neural-style.html">
            
                    
                    13.12. 风格迁移
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.13" data-path="../chapter_computer-vision/kaggle-cifar10.html">
            
                <a href="../chapter_computer-vision/kaggle-cifar10.html">
            
                    
                    13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10.md)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.16.14" data-path="../chapter_computer-vision/kaggle-dog.html">
            
                <a href="../chapter_computer-vision/kaggle-dog.html">
            
                    
                    13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.17" data-path="../chapter_natural-language-processing-pretraining/">
            
                <a href="../chapter_natural-language-processing-pretraining/">
            
                    
                    14. 自然语言处理：预训练
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.17.1" data-path="../chapter_natural-language-processing-pretraining/word2vec.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word2vec.html">
            
                    
                    14.1. 词嵌入（word2vec）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.2" data-path="../chapter_natural-language-processing-pretraining/approx-training.html">
            
                <a href="../chapter_natural-language-processing-pretraining/approx-training.html">
            
                    
                    14.2. 近似训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.3" data-path="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">
            
                    
                    14.3. 用于预训练词嵌入的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.4" data-path="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">
            
                <a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">
            
                    
                    14.4. 预训练word2vec
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.5" data-path="../chapter_natural-language-processing-pretraining/glove.html">
            
                <a href="../chapter_natural-language-processing-pretraining/glove.html">
            
                    
                    14.5. 全局向量的词嵌入（GloVe）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.6" data-path="../chapter_natural-language-processing-pretraining/subword-embedding.html">
            
                <a href="../chapter_natural-language-processing-pretraining/subword-embedding.html">
            
                    
                    14.6. 子词嵌入
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.7" data-path="../chapter_natural-language-processing-pretraining/similarity-analogy.html">
            
                <a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">
            
                    
                    14.7. 词的相似性和类比任务
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.8" data-path="../chapter_natural-language-processing-pretraining/bert.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert.html">
            
                    
                    14.8. 来自Transformers的双向编码器表示（BERT）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.9" data-path="../chapter_natural-language-processing-pretraining/bert-dataset.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert-dataset.html">
            
                    
                    14.9. 用于预训练BERT的数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.17.10" data-path="../chapter_natural-language-processing-pretraining/bert-pretraining.html">
            
                <a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">
            
                    
                    14.10. 预训练BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.18" data-path="../chapter_natural-language-processing-applications/">
            
                <a href="../chapter_natural-language-processing-applications/">
            
                    
                    15. 自然语言处理：应用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.18.1" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">
            
                    
                    15.1. 情感分析及数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.2" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">
            
                    
                    15.2. 情感分析：使用循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.3" data-path="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                <a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">
            
                    
                    15.3. 情感分析：使用卷积神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.4" data-path="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">
            
                    
                    15.4. 自然语言推断与数据集
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.5" data-path="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">
            
                    
                    15.5. 自然语言推断：使用注意力
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.6" data-path="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/finetuning-bert.html">
            
                    
                    15.6. 针对序列级和词元级应用微调BERT
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.18.7" data-path="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                <a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">
            
                    
                    15.7. 自然语言推断：微调BERT
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.19" data-path="../chapter_appendix-tools-for-deep-learning/">
            
                <a href="../chapter_appendix-tools-for-deep-learning/">
            
                    
                    16. 附录：深度学习工具
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.19.1" data-path="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/jupyter.html">
            
                    
                    16.1. 使用Jupyter Notebook
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.2" data-path="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">
            
                    
                    16.2. 使用Amazon SageMaker
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.3" data-path="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/aws.html">
            
                    
                    16.3. 使用Amazon EC2实例
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.4" data-path="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">
            
                    
                    16.4. 选择服务器和GPU
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.5" data-path="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/contributing.html">
            
                    
                    16.5. 为本书做贡献
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.19.6" data-path="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                <a href="../chapter_appendix-tools-for-deep-learning/d2l.html">
            
                    
                    16.6. d2l API 文档
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.20" data-path="../chapter_references/zreferences.html">
            
                <a href="../chapter_references/zreferences.html">
            
                    
                    参考文献
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >前言</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h2 id="前言">前言</h2>
<p>几年前，在大公司和初创公司中，并没有大量的深度学习科学家开发智能产品和服务。我们中年轻人（作者）进入这个领域时，机器学习并没有在报纸上获得头条新闻。我们的父母根本不知道什么是机器学习，更不用说为什么我们可能更喜欢机器学习，而不是从事医学或法律职业。机器学习是一门具有前瞻性的学科，在现实世界的应用范围很窄。而那些应用，例如语音识别和计算机视觉，需要大量的领域知识，以至于它们通常被认为是完全独立的领域，而机器学习对这些领域来说只是一个小组件。因此，神经网络——我们在本书中关注的深度学习模型的前身，被认为是过时的工具。</p>
<p>就在过去的五年里，深度学习给世界带来了惊喜，推动了计算机视觉、自然语言处理、自动语音识别、强化学习和统计建模等领域的快速发展。有了这些进步，我们现在可以制造比以往任何时候都更自主的汽车（不过可能没有一些公司试图让大家相信的那么自主），可以自动起草普通邮件的智能回复系统，帮助人们从令人压抑的大收件箱中解放出来。在围棋等棋类游戏中，软件超越了世界上最优秀的人，这曾被认为是几十年后的事。这些工具已经对工业和社会产生了越来越广泛的影响，改变了电影的制作方式、疾病的诊断方式，并在基础科学中扮演着越来越重要的角色——从天体物理学到生物学。</p>
<h2 id="关于本书">关于本书</h2>
<p>这本书代表了我们的尝试——让深度学习可平易近人，教会人们<em>概念</em>、<em>背景</em>和<em>代码</em>。</p>
<h3 id="一种结合了代码、数学和html的媒介">一种结合了代码、数学和HTML的媒介</h3>
<p>任何一种计算技术要想发挥其全部影响力，都必须得到充分的理解、充分的文档记录，并得到成熟的、维护良好的工具的支持。关键思想应该被清楚地提炼出来，尽可能减少需要让新的从业者跟上时代的入门时间。成熟的库应该自动化常见的任务，示例代码应该使从业者可以轻松地修改、应用和扩展常见的应用程序，以满足他们的需求。以动态网页应用为例。尽管许多公司，如亚马逊，在20世纪90年代开发了成功的数据库驱动网页应用程序。但在过去的10年里，这项技术在帮助创造性企业家方面的潜力已经得到了更大程度的发挥，部分原因是开发了功能强大、文档完整的框架。</p>
<p>测试深度学习的潜力带来了独特的挑战，因为任何一个应用都会将不同的学科结合在一起。应用深度学习需要同时了解（1）以特定方式提出问题的动机；（2）给定建模方法的数学；（3）将模型拟合数据的优化算法；（4）能够有效训练模型、克服数值计算缺陷并最大限度地利用现有硬件的工程方法。同时教授表述问题所需的批判性思维技能、解决问题所需的数学知识，以及实现这些解决方案所需的软件工具，这是一个巨大的挑战。</p>
<p>在我们开始写这本书的时候，没有资源能够同时满足一些条件：（1）是最新的；（2）涵盖了现代机器学习的所有领域，技术深度丰富；（3）在一本引人入胜的教科书中，人们可以在实践教程中找到干净的可运行代码，并从中穿插高质量的阐述。我们发现了大量关于如何使用给定的深度学习框架（例如，如何对TensorFlow中的矩阵进行基本的数值计算)或实现特定技术的代码示例（例如，LeNet、AlexNet、ResNet的代码片段），这些代码示例分散在各种博客帖子和GitHub库中。但是，这些示例通常关注如何实现给定的方法，但忽略了为什么做出某些算法决策的讨论。虽然一些互动资源已经零星地出现以解决特定主题。例如，在网站<a href="http://distill.pub" target="_blank">Distill</a>上发布的引人入胜的博客帖子或个人博客，但它们仅覆盖深度学习中的选定主题，并且通常缺乏相关代码。另一方面，虽然已经出现了几本教科书，其中最著名的是 :cite:<code>Goodfellow.Bengio.Courville.2016</code>（中文名《深度学习》），它对深度学习背后的概念进行了全面的调查，但这些资源并没有将这些概念的描述与这些概念的代码实现结合起来。有时会让读者对如何实现它们一无所知。此外，太多的资源隐藏在商业课程提供商的付费壁垒后面。</p>
<p>我们着手创建的资源可以：（1）每个人都可以免费获得；（2）提供足够的技术深度，为真正成为一名应用机器学习科学家提供起步；（3）包括可运行的代码，向读者展示如何解决实践中的问题；（4）允许我们和社区的快速更新;（5）由一个<a href="http://discuss.d2l.ai" target="_blank">论坛</a>作为补充，用于技术细节的互动讨论和回答问题。</p>
<p>这些目标经常是相互冲突的。公式、定理和引用最好用LaTeX来管理和布局。代码最好用Python描述。网页原生是HTML和JavaScript的。此外，我们希望内容既可以作为可执行代码访问、作为纸质书访问，作为可下载的PDF访问，也可以作为网站在互联网上访问。目前还没有完全适合这些需求的工具和工作流程，所以我们不得不自行组装。我们在 :numref:<code>sec_how_to_contribute</code> 中详细描述了我们的方法。我们选择GitHub来共享源代码并允许编辑，选择Jupyter记事本来混合代码、公式和文本，选择Sphinx作为渲染引擎来生成多个输出，并为论坛提供讨论。虽然我们的体系尚不完善，但这些选择在相互冲突的问题之间提供了一个很好的妥协。我们相信，这可能是第一本使用这种集成工作流程出版的书。</p>
<h3 id="在实践中学习">在实践中学习</h3>
<p>许多教科书教授一系列的主题，每一个都非常详细。例如，Chris Bishop的优秀教科书 :cite:<code>Bishop.2006</code> ，对每个主题都教得很透彻，以至于要读到线性回归这一章需要大量的工作。虽然专家们喜欢这本书正是因为它的透彻性，但对初学者来说，这一特性限制了它作为介绍性文本的实用性。</p>
<p>在这本书中，我们将适时教授大部分概念。换句话说，你将在实现某些实际目的所需的非常时刻学习概念。虽然我们在开始时花了一些时间来教授基础的背景知识，如线性代数和概率，但我们希望你在思考更深奥的概率分布之前，先体会一下训练模型的满足感。</p>
<p>除了提供基本数学背景速成课程的几节初步课程外，后续的每一章都介绍了适量的新概念，并提供可独立工作的例子——使用真实的数据集。这带来了组织上的挑战。某些模型可能在逻辑上组合在单节中。而一些想法可能最好是通过连续允许几个模型来传授。另一方面，坚持“一个工作例子一节”的策略有一个很大的好处：这使你可以通过利用我们的代码尽可能轻松地启动你自己的研究项目。只需复制这一节的内容并开始修改即可。</p>
<p>我们将根据需要将可运行代码与背景材料交错。通常，在充分解释工具之前，我们常常会在提供工具这一方面犯错误（我们将在稍后解释背景）。例如，在充分解释<em>随机梯度下降</em>为什么有用或为什么有效之前，我们可以使用它。这有助于给从业者提供快速解决问题所需的弹药，同时需要读者相信我们的一些决定。</p>
<p>这本书将从头开始教授深度学习的概念。有时，我们想深入研究模型的细节，这些的细节通常会被深度学习框架的高级抽象隐藏起来。特别是在基础教程中，我们希望读者了解在给定层或优化器中发生的一切。在这些情况下，我们通常会提供两个版本的示例：一个是我们从零开始实现一切，仅依赖张量操作和自动微分；另一个是更实际的示例，我们使用深度学习框架的高级API编写简洁的代码。一旦我们教了您一些组件是如何工作的，我们就可以在随后的教程中使用高级API了。</p>
<h3 id="内容和结构">内容和结构</h3>
<p>全书大致可分为三个部分，在 :numref:<code>fig_book_org</code> 中用不同的颜色呈现：</p>
<p><img src="../img/book-org.svg" alt="全书结构"></img>
:label:<code>fig_book_org</code></p>
<ul>
<li><p>第一部分包括基础知识和预备知识。
:numref:<code>chap_introduction</code> 提供深度学习的入门课程。然后在 :numref:<code>chap_preliminaries</code> 中，我们将快速介绍实践深度学习所需的前提条件，例如如何存储和处理数据，以及如何应用基于线性代数、微积分和概率基本概念的各种数值运算。 :numref:<code>chap_linear</code> 和 :numref:<code>chap_perceptrons</code> 涵盖了深度学习的最基本概念和技术，例如线性回归、多层感知机和正则化。</p>
</li>
<li><p>接下来的五章集中讨论现代深度学习技术。
:numref:<code>chap_computation</code> 描述了深度学习计算的各种关键组件，并为我们随后实现更复杂的模型奠定了基础。接下来，在 :numref:<code>chap_cnn</code> 和 :numref:<code>chap_modern_cnn</code> 中，我们介绍了卷积神经网络（convolutional neural network，CNN），这是构成大多数现代计算机视觉系统骨干的强大工具。随后，在 :numref:<code>chap_rnn</code> 和 :numref:<code>chap_modern_rnn</code> 中，我们引入了循环神经网络(recurrent neural network，RNN)，这是一种利用数据中的时间或序列结构的模型，通常用于自然语言处理和时间序列预测。在 :numref:<code>chap_attention</code> 中，我们介绍了一类新的模型，它采用了一种称为注意力机制的技术，最近它们已经开始在自然语言处理中取代循环神经网络。这一部分将帮助读者快速了解大多数现代深度学习应用背后的基本工具。</p>
</li>
<li><p>第三部分讨论可伸缩性、效率和应用程序。
首先，在 :numref:<code>chap_optimization</code> 中，我们讨论了用于训练深度学习模型的几种常用优化算法。下一章 :numref:<code>chap_performance</code> 将探讨影响深度学习代码计算性能的几个关键因素。在 :numref:<code>chap_cv</code> 中，我们展示了深度学习在计算机视觉中的主要应用。在 :numref:<code>chap_nlp_pretrain</code> 和 :numref:<code>chap_nlp_app</code> 中，我们展示了如何预训练语言表示模型并将其应用于自然语言处理任务。</p>
</li>
</ul>
<h3 id="代码">代码</h3>
<p>:label:<code>sec_code</code></p>
<p>本书的大部分章节都以可执行代码为特色，因为我们相信交互式学习体验在深度学习中的重要性。目前，某些直觉只能通过试错、小幅调整代码并观察结果来发展。理想情况下，一个优雅的数学理论可能会精确地告诉我们如何调整代码以达到期望的结果。不幸的是，这种优雅的理论目前还没有出现。尽管我们尽了最大努力，但仍然缺乏对各种技术的正式解释，这既是因为描述这些模型的数学可能非常困难，也是因为对这些主题的认真研究最近才进入高潮。我们希望随着深度学习理论的发展，这本书的未来版本将能够在当前版本无法提供的地方提供见解。</p>
<p>有时，为了避免不必要的重复，我们将本书中经常导入和引用的函数、类等封装在<code>d2l</code>包中。对于要保存到包中的任何代码块，比如一个函数、一个类或者多个导入，我们都会标记为<code>#@save</code>。我们在 :numref:<code>sec_d2l</code> 中提供了这些函数和类的详细描述。<code>d2l</code>软件包是轻量级的，仅需要以下软件包和模块作为依赖项：</p>
<pre><code class="lang-python"><span class="hljs-comment">#@tab all</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
<span class="hljs-keyword">from</span> IPython <span class="hljs-keyword">import</span> display
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> matplotlib_inline <span class="hljs-keyword">import</span> backend_inline
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> tarfile
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> zipfile
<span class="hljs-keyword">import</span> hashlib
d2l = sys.modules[__name__]
</code></pre>
<p>:begin_tab:<code>mxnet</code>
本书中的大部分代码都是基于Apache MXNet的。MXNet是深度学习的开源框架，是亚马逊以及许多大学和公司的首选。本书中的所有代码都通过了最新MXNet版本的测试。但是，由于深度学习的快速发展，一些在印刷版中代码
可能在MXNet的未来版本无法正常工作。
但是，我们计划使在线版本保持最新。如果读者遇到任何此类问题，请查看 :ref:<code>chap_installation</code> 以更新代码和运行时环境。</p>
<p>下面是我们如何从MXNet导入模块。
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
本书中的大部分代码都是基于PyTorch的。PyTorch是一个开源的深度学习框架，在研究界非常受欢迎。本书中的所有代码都在最新版本的PyTorch下通过了测试。但是，由于深度学习的快速发展，一些在印刷版中代码可能在PyTorch的未来版本无法正常工作。
但是，我们计划使在线版本保持最新。如果读者遇到任何此类问题，请查看 :ref:<code>chap_installation</code> 以更新代码和运行时环境。</p>
<p>下面是我们如何从PyTorch导入模块。
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
本书中的大部分代码都是基于TensorFlow的。TensorFlow是一个开源的深度学习框架，在研究界和产业界都非常受欢迎。本书中的所有代码都在最新版本的TensorFlow下通过了测试。但是，由于深度学习的快速发展，一些在印刷版中代码可能在TensorFlow的未来版本无法正常工作。
但是，我们计划使在线版本保持最新。如果读者遇到任何此类问题，请查看 :ref:<code>chap_installation</code> 以更新代码和运行时环境。</p>
<p>下面是我们如何从TensorFlow导入模块。
:end_tab:</p>
<pre><code class="lang-python"><span class="hljs-comment">#@save</span>
<span class="hljs-keyword">from</span> mxnet <span class="hljs-keyword">import</span> autograd, context, gluon, image, init, np, npx
<span class="hljs-keyword">from</span> mxnet.gluon <span class="hljs-keyword">import</span> nn, rnn
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab pytorch</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab tensorflow</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#@tab paddle</span>
<span class="hljs-comment">#@save</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
<span class="hljs-keyword">import</span> paddle
<span class="hljs-keyword">from</span> paddle <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> paddle.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">from</span> paddle.vision <span class="hljs-keyword">import</span> transforms
<span class="hljs-keyword">import</span> paddle.vision <span class="hljs-keyword">as</span> paddlevision
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
paddle.disable_signal_handler()
</code></pre>
<h3 id="目标受众">目标受众</h3>
<p>本书面向学生（本科生或研究生）、工程师和研究人员，他们希望扎实掌握深度学习的实用技术。因为我们从头开始解释每个概念，所以不需要过往的深度学习或机器学习背景。全面解释深度学习的方法需要一些数学和编程，但我们只假设读者了解一些基础知识，包括线性代数、微积分、概率和非常基础的Python编程。此外，在附录中，我们提供了本书所涵盖的大多数数学知识的复习。大多数时候，我们会优先考虑直觉和想法，而不是数学的严谨性。有许多很棒的书可以引导感兴趣的读者走得更远。Bela Bollobas的《线性分析》 :cite:<code>Bollobas.1999</code> 对线性代数和函数分析进行了深入的研究。 :cite:<code>Wasserman.2013</code> 是一本很好的统计学指南。如果读者以前没有使用过Python语言，那么可以仔细阅读这个<a href="http://learnpython.org/" target="_blank">Python教程</a>。</p>
<h3 id="论坛">论坛</h3>
<p>与本书相关，我们已经启动了一个论坛，在<a href="https://discuss.d2l.ai/" target="_blank">discuss.d2l.ai</a>。当对本书的任何一节有疑问时，请在每一节的末尾找到相关的讨论页链接。</p>
<h2 id="致谢">致谢</h2>
<p>感谢中英文草稿的数百位撰稿人。他们帮助改进了内容并提供了宝贵的反馈。
感谢Anirudh Dagar和唐源将部分较早版本的MXNet实现分别改编为PyTorch和TensorFlow实现。
感谢百度团队将较新的PyTorch实现改编为PaddlePaddle实现。
感谢张帅将更新的LaTeX样式集成进PDF文件的编译。</p>
<p>特别地，我们要感谢这份中文稿的每一位撰稿人，是他们的无私奉献让这本书变得更好。他们的GitHub ID或姓名是(没有特定顺序)：alxnorden, avinashingit, bowen0701, brettkoonce, Chaitanya Prakash Bapat,
cryptonaut, Davide Fiocco, edgarroman, gkutiel, John Mitro, Liang Pu,
Rahul Agarwal, Mohamed Ali Jamaoui, Michael (Stu) Stewart, Mike Müller,
NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sundeepteki,
topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB, Yuhong Chen,
Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen, pmuens,
Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Abhishek Gupta,
uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Buddareddygari, brianhendee,
mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete Lüer, Surbhi Vijayvargeeya,
Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pedro Larroy,
lgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner,
Maximilian Böther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong,
Steve Sedlmeyer, Ruslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas,
ati-ozgur, qbaza, dchoi77, Adam Gerson, Phuc Le, Mark Atwood, christabella, vn09,
Haibin Lin, jjangga0214, RichyChen, noelo, hansent, Giel Dops, dvincent1337, WhiteD3vil,
Peter Kulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle, Peter Goetz, rigtorp,
Tiep Vu, sfilip, mlxd, Kale-ab Tessera, Sanjar Adilov, MatteoFerrara, hsneto,
Katarzyna Biesialska, Gregory Bruss, Duy–Thanh Doan, paulaurel, graytowne, Duc Pham,
sl7423, Jaedong Hwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums,
Cuong V. Nguyen, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic,
the-great-shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, biagiom,
abhinavsp0730, jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang Kang,
StevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl, SportsTHU,
Hoa Nguyen, manuel-arno-korfmann-webentwicklung, aterzis-personal, nxby, Xiaoting He, Josiah Yoder,
mathresearch, mzz2017, jroberayalas, iluu, ghejc, BSharmi, vkramdev, simonwardjones, LakshKD,
TalNeoran, djliden, Nikhil95, Oren Barkan, guoweis, haozhu233, pratikhack, 315930399, tayfununal,
steinsag, charleybeller, Andrew Lumsdaine, Jiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates,
Adriaan Tijsseling, Ron Medina, Gaurav Saha, Murat Semerci, Lei Mao, Zhu Yuanxiang,
thebesttv, Quanshangze Du, Yanbo Chen。</p>
<p>我们感谢Amazon Web Services，特别是Swami Sivasubramanian、Peter DeSantis、Adam Selipsky和Andrew Jassy对撰写本书的慷慨支持。如果没有可用的时间、资源、与同事的讨论和不断的鼓励，这本书就不会出版。</p>
<h2 id="小结">小结</h2>
<ul>
<li>深度学习已经彻底改变了模式识别，引入了一系列技术，包括计算机视觉、自然语言处理、自动语音识别。</li>
<li>要成功地应用深度学习，必须知道如何抛出一个问题、建模的数学方法、将模型与数据拟合的算法，以及实现所有这些的工程技术。</li>
<li>这本书提供了一个全面的资源，包括文本、图表、数学和代码，都集中在一个地方。</li>
<li>要回答与本书相关的问题，请访问我们的论坛<a href="https://discuss.d2l.ai/" target="_blank">discuss.d2l.ai</a>.</li>
<li>所有Jupyter记事本都可以在GitHub上下载。</li>
</ul>
<h2 id="练习">练习</h2>
<ol>
<li>在本书<a href="https://discuss.d2l.ai/" target="_blank">discuss.d2l.ai</a>的论坛上注册帐户。</li>
<li>在计算机上安装Python。</li>
<li>沿着本节底部的链接进入论坛，在那里可以寻求帮助、讨论这本书，并通过与作者和社区接触来找到问题的答案。</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/2085" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/2086" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/2087" target="_blank">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>paddle</code>
<a href="https://discuss.d2l.ai/t/11678" target="_blank">Discussions</a>
:end_tab:</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="../d2l.html" class="navigation navigation-prev " aria-label="Previous page: 动手学深度学习">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../chapter_installation/" class="navigation navigation-next " aria-label="Next page: 安装">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"description":"CoderFAN 资料库 动手学深度学习","title":"前言","level":"1.2.1","depth":2,"next":{"title":"安装","level":"1.2.2","depth":2,"path":"chapter_installation/index.md","ref":"chapter_installation/index.md","articles":[]},"previous":{"title":"动手学深度学习","level":"1.2","depth":1,"path":"d2l.md","ref":"d2l.md","articles":[{"title":"前言","level":"1.2.1","depth":2,"path":"chapter_preface/index.md","ref":"chapter_preface/index.md","articles":[]},{"title":"安装","level":"1.2.2","depth":2,"path":"chapter_installation/index.md","ref":"chapter_installation/index.md","articles":[]},{"title":"符号","level":"1.2.3","depth":2,"path":"chapter_notation/index.md","ref":"chapter_notation/index.md","articles":[]},{"title":"1. 引言","level":"1.2.4","depth":2,"path":"chapter_introduction/index.md","ref":"chapter_introduction/index.md","articles":[]},{"title":"2. 预备知识","level":"1.2.5","depth":2,"path":"chapter_preliminaries/index.md","ref":"chapter_preliminaries/index.md","articles":[{"title":"2.1. 数据操作","level":"1.2.5.1","depth":3,"path":"chapter_preliminaries/ndarray.md","ref":"chapter_preliminaries/ndarray.md","articles":[]},{"title":"2.2. 数据预处理","level":"1.2.5.2","depth":3,"path":"chapter_preliminaries/pandas.md","ref":"chapter_preliminaries/pandas.md","articles":[]},{"title":"2.3. 线性代数","level":"1.2.5.3","depth":3,"path":"chapter_preliminaries/linear-algebra.md","ref":"chapter_preliminaries/linear-algebra.md","articles":[]},{"title":"2.4. 微积分","level":"1.2.5.4","depth":3,"path":"chapter_preliminaries/calculus.md","ref":"chapter_preliminaries/calculus.md","articles":[]},{"title":"2.5. 自动微分","level":"1.2.5.5","depth":3,"path":"chapter_preliminaries/autograd.md","ref":"chapter_preliminaries/autograd.md","articles":[]},{"title":"2.6. 概率","level":"1.2.5.6","depth":3,"path":"chapter_preliminaries/probability.md","ref":"chapter_preliminaries/probability.md","articles":[]},{"title":"2.7. 查阅文档","level":"1.2.5.7","depth":3,"path":"chapter_preliminaries/lookup-api.md","ref":"chapter_preliminaries/lookup-api.md","articles":[]}]},{"title":"3. 线性神经网络","level":"1.2.6","depth":2,"path":"chapter_linear-networks/index.md","ref":"chapter_linear-networks/index.md","articles":[{"title":"3.1. 线性回归","level":"1.2.6.1","depth":3,"path":"chapter_linear-networks/linear-regression.md","ref":"chapter_linear-networks/linear-regression.md","articles":[]},{"title":"3.2. 线性回归的从零开始实现","level":"1.2.6.2","depth":3,"path":"chapter_linear-networks/linear-regression-scratch.md","ref":"chapter_linear-networks/linear-regression-scratch.md","articles":[]},{"title":"3.3. 线性回归的简洁实现","level":"1.2.6.3","depth":3,"path":"chapter_linear-networks/linear-regression-concise.md","ref":"chapter_linear-networks/linear-regression-concise.md","articles":[]},{"title":"3.4. softmax回归","level":"1.2.6.4","depth":3,"path":"chapter_linear-networks/softmax-regression.md","ref":"chapter_linear-networks/softmax-regression.md","articles":[]},{"title":"3.5. 图像分类数据集","level":"1.2.6.5","depth":3,"path":"chapter_linear-networks/image-classification-dataset.md","ref":"chapter_linear-networks/image-classification-dataset.md","articles":[]},{"title":"3.6. softmax回归的从零开始实现","level":"1.2.6.6","depth":3,"path":"chapter_linear-networks/softmax-regression-scratch.md","ref":"chapter_linear-networks/softmax-regression-scratch.md","articles":[]},{"title":"3.7. softmax回归的简洁实现","level":"1.2.6.7","depth":3,"path":"chapter_linear-networks/softmax-regression-concise.md","ref":"chapter_linear-networks/softmax-regression-concise.md","articles":[]}]},{"title":"4. 多层感知机","level":"1.2.7","depth":2,"path":"chapter_multilayer-perceptrons/index.md","ref":"chapter_multilayer-perceptrons/index.md","articles":[{"title":"4.1. 多层感知机","level":"1.2.7.1","depth":3,"path":"chapter_multilayer-perceptrons/mlp.md","ref":"chapter_multilayer-perceptrons/mlp.md","articles":[]},{"title":"4.2. 多层感知机的从零开始实现","level":"1.2.7.2","depth":3,"path":"chapter_multilayer-perceptrons/mlp-scratch.md","ref":"chapter_multilayer-perceptrons/mlp-scratch.md","articles":[]},{"title":"4.3. 多层感知机的简洁实现","level":"1.2.7.3","depth":3,"path":"chapter_multilayer-perceptrons/mlp-concise.md","ref":"chapter_multilayer-perceptrons/mlp-concise.md","articles":[]},{"title":"4.4. 模型选择、欠拟合和过拟合","level":"1.2.7.4","depth":3,"path":"chapter_multilayer-perceptrons/underfit-overfit.md","ref":"chapter_multilayer-perceptrons/underfit-overfit.md","articles":[]},{"title":"4.5. 权重衰减","level":"1.2.7.5","depth":3,"path":"chapter_multilayer-perceptrons/weight-decay.md","ref":"chapter_multilayer-perceptrons/weight-decay.md","articles":[]},{"title":"4.6. 暂退法（Dropout）","level":"1.2.7.6","depth":3,"path":"chapter_multilayer-perceptrons/dropout.md","ref":"chapter_multilayer-perceptrons/dropout.md","articles":[]},{"title":"4.7. 前向传播、反向传播和计算图","level":"1.2.7.7","depth":3,"path":"chapter_multilayer-perceptrons/backprop.md","ref":"chapter_multilayer-perceptrons/backprop.md","articles":[]},{"title":"4.8. 数值稳定性和模型初始化","level":"1.2.7.8","depth":3,"path":"chapter_multilayer-perceptrons/numerical-stability-and-init.md","ref":"chapter_multilayer-perceptrons/numerical-stability-and-init.md","articles":[]},{"title":"4.9. 环境和分布偏移","level":"1.2.7.9","depth":3,"path":"chapter_multilayer-perceptrons/environment.md","ref":"chapter_multilayer-perceptrons/environment.md","articles":[]},{"title":"4.10. 实战Kaggle比赛：预测房价","level":"1.2.7.10","depth":3,"path":"chapter_multilayer-perceptrons/kaggle-house-price.md","ref":"chapter_multilayer-perceptrons/kaggle-house-price.md","articles":[]}]},{"title":"5. 深度学习计算","level":"1.2.8","depth":2,"path":"chapter_deep-learning-computation/index.md","ref":"chapter_deep-learning-computation/index.md","articles":[{"title":"5.1. 层和块","level":"1.2.8.1","depth":3,"path":"chapter_deep-learning-computation/model-construction.md","ref":"chapter_deep-learning-computation/model-construction.md","articles":[]},{"title":"5.2. 参数管理","level":"1.2.8.2","depth":3,"path":"chapter_deep-learning-computation/parameters.md","ref":"chapter_deep-learning-computation/parameters.md","articles":[]},{"title":"5.3. 延后初始化","level":"1.2.8.3","depth":3,"path":"chapter_deep-learning-computation/deferred-init.md","ref":"chapter_deep-learning-computation/deferred-init.md","articles":[]},{"title":"5.4. 自定义层","level":"1.2.8.4","depth":3,"path":"chapter_deep-learning-computation/custom-layer.md","ref":"chapter_deep-learning-computation/custom-layer.md","articles":[]},{"title":"5.5. 读写文件","level":"1.2.8.5","depth":3,"path":"chapter_deep-learning-computation/read-write.md","ref":"chapter_deep-learning-computation/read-write.md","articles":[]},{"title":"5.6. GPU计算","level":"1.2.8.6","depth":3,"path":"chapter_deep-learning-computation/use-gpu.md","ref":"chapter_deep-learning-computation/use-gpu.md","articles":[]}]},{"title":"6. 卷积神经网络","level":"1.2.9","depth":2,"path":"chapter_convolutional-neural-networks/index.md","ref":"chapter_convolutional-neural-networks/index.md","articles":[{"title":"6.1. 从全连接层到卷积","level":"1.2.9.1","depth":3,"path":"chapter_convolutional-neural-networks/why-conv.md","ref":"chapter_convolutional-neural-networks/why-conv.md","articles":[]},{"title":"6.2. 图像卷积","level":"1.2.9.2","depth":3,"path":"chapter_convolutional-neural-networks/conv-layer.md","ref":"chapter_convolutional-neural-networks/conv-layer.md","articles":[]},{"title":"6.3. 填充和步幅","level":"1.2.9.3","depth":3,"path":"chapter_convolutional-neural-networks/padding-and-strides.md","ref":"chapter_convolutional-neural-networks/padding-and-strides.md","articles":[]},{"title":"6.4. 多输入多输出通道","level":"1.2.9.4","depth":3,"path":"chapter_convolutional-neural-networks/channels.md","ref":"chapter_convolutional-neural-networks/channels.md","articles":[]},{"title":"6.5. 汇聚层","level":"1.2.9.5","depth":3,"path":"chapter_convolutional-neural-networks/pooling.md","ref":"chapter_convolutional-neural-networks/pooling.md","articles":[]},{"title":"6.6. 卷积神经网络（LeNet）","level":"1.2.9.6","depth":3,"path":"chapter_convolutional-neural-networks/lenet.md","ref":"chapter_convolutional-neural-networks/lenet.md","articles":[]}]},{"title":"7. 现代卷积神经网络","level":"1.2.10","depth":2,"path":"chapter_convolutional-modern/index.md","ref":"chapter_convolutional-modern/index.md","articles":[{"title":"7.1. 深度卷积神经网络（AlexNet）","level":"1.2.10.1","depth":3,"path":"chapter_convolutional-modern/alexnet.md","ref":"chapter_convolutional-modern/alexnet.md","articles":[]},{"title":"7.2. 使用块的网络（VGG）","level":"1.2.10.2","depth":3,"path":"chapter_convolutional-modern/vgg.md","ref":"chapter_convolutional-modern/vgg.md","articles":[]},{"title":"7.3. 网络中的网络（NiN）","level":"1.2.10.3","depth":3,"path":"chapter_convolutional-modern/nin.md","ref":"chapter_convolutional-modern/nin.md","articles":[]},{"title":"7.4. 含并行连结的网络（GoogLeNet）","level":"1.2.10.4","depth":3,"path":"chapter_convolutional-modern/googlenet.md","ref":"chapter_convolutional-modern/googlenet.md","articles":[]},{"title":"7.5. 批量规范化","level":"1.2.10.5","depth":3,"path":"chapter_convolutional-modern/batch-norm.md","ref":"chapter_convolutional-modern/batch-norm.md","articles":[]},{"title":"7.6. 残差网络（ResNet）","level":"1.2.10.6","depth":3,"path":"chapter_convolutional-modern/resnet.md","ref":"chapter_convolutional-modern/resnet.md","articles":[]},{"title":"7.7. 稠密连接网络（DenseNet）","level":"1.2.10.7","depth":3,"path":"chapter_convolutional-modern/densenet.md","ref":"chapter_convolutional-modern/densenet.md","articles":[]}]},{"title":"8. 循环神经网络","level":"1.2.11","depth":2,"path":"chapter_recurrent-neural-networks/index.md","ref":"chapter_recurrent-neural-networks/index.md","articles":[{"title":"8.1. 序列模型","level":"1.2.11.1","depth":3,"path":"chapter_recurrent-neural-networks/sequence.md","ref":"chapter_recurrent-neural-networks/sequence.md","articles":[]},{"title":"8.2. 文本预处理","level":"1.2.11.2","depth":3,"path":"chapter_recurrent-neural-networks/text-preprocessing.md","ref":"chapter_recurrent-neural-networks/text-preprocessing.md","articles":[]},{"title":"8.3. 语言模型和数据集","level":"1.2.11.3","depth":3,"path":"chapter_recurrent-neural-networks/language-models-and-dataset.md","ref":"chapter_recurrent-neural-networks/language-models-and-dataset.md","articles":[]},{"title":"8.4. 循环神经网络","level":"1.2.11.4","depth":3,"path":"chapter_recurrent-neural-networks/rnn.md","ref":"chapter_recurrent-neural-networks/rnn.md","articles":[]},{"title":"8.5. 循环神经网络的从零开始实现","level":"1.2.11.5","depth":3,"path":"chapter_recurrent-neural-networks/rnn-scratch.md","ref":"chapter_recurrent-neural-networks/rnn-scratch.md","articles":[]},{"title":"8.6. 循环神经网络的简洁实现","level":"1.2.11.6","depth":3,"path":"chapter_recurrent-neural-networks/rnn-concise.md","ref":"chapter_recurrent-neural-networks/rnn-concise.md","articles":[]},{"title":"8.7. 通过时间反向传播","level":"1.2.11.7","depth":3,"path":"chapter_recurrent-neural-networks/bptt.md","ref":"chapter_recurrent-neural-networks/bptt.md","articles":[]}]},{"title":"9. 现代循环神经网络","level":"1.2.12","depth":2,"path":"chapter_recurrent-modern/index.md","ref":"chapter_recurrent-modern/index.md","articles":[{"title":"9.1. 门控循环单元（GRU）","level":"1.2.12.1","depth":3,"path":"chapter_recurrent-modern/gru.md","ref":"chapter_recurrent-modern/gru.md","articles":[]},{"title":"9.2. 长短期记忆（LSTM）","level":"1.2.12.2","depth":3,"path":"chapter_recurrent-modern/lstm.md","ref":"chapter_recurrent-modern/lstm.md","articles":[]},{"title":"9.3. 深度循环神经网络","level":"1.2.12.3","depth":3,"path":"chapter_recurrent-modern/deep-rnn.md","ref":"chapter_recurrent-modern/deep-rnn.md","articles":[]},{"title":"9.4. 双向循环神经网络","level":"1.2.12.4","depth":3,"path":"chapter_recurrent-modern/bi-rnn.md","ref":"chapter_recurrent-modern/bi-rnn.md","articles":[]},{"title":"9.5. 机器翻译及数据集","level":"1.2.12.5","depth":3,"path":"chapter_recurrent-modern/machine-translation-and-dataset.md","ref":"chapter_recurrent-modern/machine-translation-and-dataset.md","articles":[]},{"title":"9.6. 编码器—解码器","level":"1.2.12.6","depth":3,"path":"chapter_recurrent-modern/encoder-decoder.md","ref":"chapter_recurrent-modern/encoder-decoder.md","articles":[]},{"title":"9.7. 序列到序列学习（seq2seq）","level":"1.2.12.7","depth":3,"path":"chapter_recurrent-modern/seq2seq.md","ref":"chapter_recurrent-modern/seq2seq.md","articles":[]},{"title":"9.8. 束搜索","level":"1.2.12.8","depth":3,"path":"chapter_recurrent-modern/beam-search.md","ref":"chapter_recurrent-modern/beam-search.md","articles":[]}]},{"title":"10. 注意力机制","level":"1.2.13","depth":2,"path":"chapter_attention-mechanisms/index.md","ref":"chapter_attention-mechanisms/index.md","articles":[{"title":"10.1. 注意力提示","level":"1.2.13.1","depth":3,"path":"chapter_attention-mechanisms/attention-cues.md","ref":"chapter_attention-mechanisms/attention-cues.md","articles":[]},{"title":"10.2. 注意力汇聚：Nadaraya-Watson 核回归","level":"1.2.13.2","depth":3,"path":"chapter_attention-mechanisms/nadaraya-waston.md","ref":"chapter_attention-mechanisms/nadaraya-waston.md","articles":[]},{"title":"10.3. 注意力评分函数","level":"1.2.13.3","depth":3,"path":"chapter_attention-mechanisms/attention-scoring-functions.md","ref":"chapter_attention-mechanisms/attention-scoring-functions.md","articles":[]},{"title":"10.4. Bahdanau 注意力","level":"1.2.13.4","depth":3,"path":"chapter_attention-mechanisms/bahdanau-attention.md","ref":"chapter_attention-mechanisms/bahdanau-attention.md","articles":[]},{"title":"10.5. 多头注意力","level":"1.2.13.5","depth":3,"path":"chapter_attention-mechanisms/multihead-attention.md","ref":"chapter_attention-mechanisms/multihead-attention.md","articles":[]},{"title":"10.6. 自注意力和位置编码","level":"1.2.13.6","depth":3,"path":"chapter_attention-mechanisms/self-attention-and-positional-encoding.md","ref":"chapter_attention-mechanisms/self-attention-and-positional-encoding.md","articles":[]},{"title":"10.7. Transformer","level":"1.2.13.7","depth":3,"path":"chapter_attention-mechanisms/transformer.md","ref":"chapter_attention-mechanisms/transformer.md","articles":[]}]},{"title":"11. 优化算法","level":"1.2.14","depth":2,"path":"chapter_optimization/index.md","ref":"chapter_optimization/index.md","articles":[{"title":"11.1. 优化与深度学习","level":"1.2.14.1","depth":3,"path":"chapter_optimization/optimization-intro.md","ref":"chapter_optimization/optimization-intro.md","articles":[]},{"title":"11.2. 凸性","level":"1.2.14.2","depth":3,"path":"chapter_optimization/convexity.md","ref":"chapter_optimization/convexity.md","articles":[]},{"title":"11.3. 梯度下降","level":"1.2.14.3","depth":3,"path":"chapter_optimization/gd.md","ref":"chapter_optimization/gd.md","articles":[]},{"title":"11.4. 随机梯度下降","level":"1.2.14.4","depth":3,"path":"chapter_optimization/sgd.md","ref":"chapter_optimization/sgd.md","articles":[]},{"title":"11.5. 小批量随机梯度下降","level":"1.2.14.5","depth":3,"path":"chapter_optimization/minibatch-sgd.md","ref":"chapter_optimization/minibatch-sgd.md","articles":[]},{"title":"11.6. 动量法","level":"1.2.14.6","depth":3,"path":"chapter_optimization/momentum.md","ref":"chapter_optimization/momentum.md","articles":[]},{"title":"11.7. AdaGrad算法","level":"1.2.14.7","depth":3,"path":"chapter_optimization/adagrad.md","ref":"chapter_optimization/adagrad.md","articles":[]},{"title":"11.8. RMSProp算法","level":"1.2.14.8","depth":3,"path":"chapter_optimization/rmsprop.md","ref":"chapter_optimization/rmsprop.md","articles":[]},{"title":"11.9. Adadelta","level":"1.2.14.9","depth":3,"path":"chapter_optimization/adadelta.md","ref":"chapter_optimization/adadelta.md","articles":[]},{"title":"11.10. Adam算法","level":"1.2.14.10","depth":3,"path":"chapter_optimization/adam.md","ref":"chapter_optimization/adam.md","articles":[]},{"title":"11.11. 学习率调度器","level":"1.2.14.11","depth":3,"path":"chapter_optimization/lr-scheduler.md","ref":"chapter_optimization/lr-scheduler.md","articles":[]}]},{"title":"12. 计算性能","level":"1.2.15","depth":2,"path":"chapter_computational-performance/index.md","ref":"chapter_computational-performance/index.md","articles":[{"title":"12.1. 编译器和解释器","level":"1.2.15.1","depth":3,"path":"chapter_computational-performance/hybridize.md","ref":"chapter_computational-performance/hybridize.md","articles":[]},{"title":"12.2. 异步计算","level":"1.2.15.2","depth":3,"path":"chapter_computational-performance/async-computation.md","ref":"chapter_computational-performance/async-computation.md","articles":[]},{"title":"12.3. 自动并行","level":"1.2.15.3","depth":3,"path":"chapter_computational-performance/auto-parallelism.md","ref":"chapter_computational-performance/auto-parallelism.md","articles":[]},{"title":"12.4. 硬件","level":"1.2.15.4","depth":3,"path":"chapter_computational-performance/hardware.md","ref":"chapter_computational-performance/hardware.md","articles":[]},{"title":"12.5. 多GPU训练","level":"1.2.15.5","depth":3,"path":"chapter_computational-performance/multiple-gpus.md","ref":"chapter_computational-performance/multiple-gpus.md","articles":[]},{"title":"12.6. 多GPU的简洁实现","level":"1.2.15.6","depth":3,"path":"chapter_computational-performance/multiple-gpus-concise.md","ref":"chapter_computational-performance/multiple-gpus-concise.md","articles":[]},{"title":"12.7. 参数服务器","level":"1.2.15.7","depth":3,"path":"chapter_computational-performance/parameterserver.md","ref":"chapter_computational-performance/parameterserver.md","articles":[]}]},{"title":"13. 计算机视觉","level":"1.2.16","depth":2,"path":"chapter_computer-vision/index.md","ref":"chapter_computer-vision/index.md","articles":[{"title":"13.1. 图像增广","level":"1.2.16.1","depth":3,"path":"chapter_computer-vision/image-augmentation.md","ref":"chapter_computer-vision/image-augmentation.md","articles":[]},{"title":"13.2. 微调","level":"1.2.16.2","depth":3,"path":"chapter_computer-vision/fine-tuning.md","ref":"chapter_computer-vision/fine-tuning.md","articles":[]},{"title":"13.3. 目标检测和边界框","level":"1.2.16.3","depth":3,"path":"chapter_computer-vision/bounding-box.md","ref":"chapter_computer-vision/bounding-box.md","articles":[]},{"title":"13.4. 锚框","level":"1.2.16.4","depth":3,"path":"chapter_computer-vision/anchor.md","ref":"chapter_computer-vision/anchor.md","articles":[]},{"title":"13.5. 多尺度目标检测","level":"1.2.16.5","depth":3,"path":"chapter_computer-vision/multiscale-object-detection.md","ref":"chapter_computer-vision/multiscale-object-detection.md","articles":[]},{"title":"13.6. 目标检测数据集","level":"1.2.16.6","depth":3,"path":"chapter_computer-vision/object-detection-dataset.md","ref":"chapter_computer-vision/object-detection-dataset.md","articles":[]},{"title":"13.7. 单发多框检测（SSD）","level":"1.2.16.7","depth":3,"path":"chapter_computer-vision/ssd.md","ref":"chapter_computer-vision/ssd.md","articles":[]},{"title":"13.8. 区域卷积神经网络（R-CNN）系列","level":"1.2.16.8","depth":3,"path":"chapter_computer-vision/rcnn.md","ref":"chapter_computer-vision/rcnn.md","articles":[]},{"title":"13.9. 语义分割和数据集","level":"1.2.16.9","depth":3,"path":"chapter_computer-vision/semantic-segmentation-and-dataset.md","ref":"chapter_computer-vision/semantic-segmentation-and-dataset.md","articles":[]},{"title":"13.10. 转置卷积","level":"1.2.16.10","depth":3,"path":"chapter_computer-vision/transposed-conv.md","ref":"chapter_computer-vision/transposed-conv.md","articles":[]},{"title":"13.11. 全卷积网络","level":"1.2.16.11","depth":3,"path":"chapter_computer-vision/fcn.md","ref":"chapter_computer-vision/fcn.md","articles":[]},{"title":"13.12. 风格迁移","level":"1.2.16.12","depth":3,"path":"chapter_computer-vision/neural-style.md","ref":"chapter_computer-vision/neural-style.md","articles":[]},{"title":"13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10.md)","level":"1.2.16.13","depth":3,"path":"chapter_computer-vision/kaggle-cifar10.md","ref":"chapter_computer-vision/kaggle-cifar10.md","articles":[]},{"title":"13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）","level":"1.2.16.14","depth":3,"path":"chapter_computer-vision/kaggle-dog.md","ref":"chapter_computer-vision/kaggle-dog.md","articles":[]}]},{"title":"14. 自然语言处理：预训练","level":"1.2.17","depth":2,"path":"chapter_natural-language-processing-pretraining/index.md","ref":"chapter_natural-language-processing-pretraining/index.md","articles":[{"title":"14.1. 词嵌入（word2vec）","level":"1.2.17.1","depth":3,"path":"chapter_natural-language-processing-pretraining/word2vec.md","ref":"chapter_natural-language-processing-pretraining/word2vec.md","articles":[]},{"title":"14.2. 近似训练","level":"1.2.17.2","depth":3,"path":"chapter_natural-language-processing-pretraining/approx-training.md","ref":"chapter_natural-language-processing-pretraining/approx-training.md","articles":[]},{"title":"14.3. 用于预训练词嵌入的数据集","level":"1.2.17.3","depth":3,"path":"chapter_natural-language-processing-pretraining/word-embedding-dataset.md","ref":"chapter_natural-language-processing-pretraining/word-embedding-dataset.md","articles":[]},{"title":"14.4. 预训练word2vec","level":"1.2.17.4","depth":3,"path":"chapter_natural-language-processing-pretraining/word2vec-pretraining.md","ref":"chapter_natural-language-processing-pretraining/word2vec-pretraining.md","articles":[]},{"title":"14.5. 全局向量的词嵌入（GloVe）","level":"1.2.17.5","depth":3,"path":"chapter_natural-language-processing-pretraining/glove.md","ref":"chapter_natural-language-processing-pretraining/glove.md","articles":[]},{"title":"14.6. 子词嵌入","level":"1.2.17.6","depth":3,"path":"chapter_natural-language-processing-pretraining/subword-embedding.md","ref":"chapter_natural-language-processing-pretraining/subword-embedding.md","articles":[]},{"title":"14.7. 词的相似性和类比任务","level":"1.2.17.7","depth":3,"path":"chapter_natural-language-processing-pretraining/similarity-analogy.md","ref":"chapter_natural-language-processing-pretraining/similarity-analogy.md","articles":[]},{"title":"14.8. 来自Transformers的双向编码器表示（BERT）","level":"1.2.17.8","depth":3,"path":"chapter_natural-language-processing-pretraining/bert.md","ref":"chapter_natural-language-processing-pretraining/bert.md","articles":[]},{"title":"14.9. 用于预训练BERT的数据集","level":"1.2.17.9","depth":3,"path":"chapter_natural-language-processing-pretraining/bert-dataset.md","ref":"chapter_natural-language-processing-pretraining/bert-dataset.md","articles":[]},{"title":"14.10. 预训练BERT","level":"1.2.17.10","depth":3,"path":"chapter_natural-language-processing-pretraining/bert-pretraining.md","ref":"chapter_natural-language-processing-pretraining/bert-pretraining.md","articles":[]}]},{"title":"15. 自然语言处理：应用","level":"1.2.18","depth":2,"path":"chapter_natural-language-processing-applications/index.md","ref":"chapter_natural-language-processing-applications/index.md","articles":[{"title":"15.1. 情感分析及数据集","level":"1.2.18.1","depth":3,"path":"chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.md","ref":"chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.md","articles":[]},{"title":"15.2. 情感分析：使用循环神经网络","level":"1.2.18.2","depth":3,"path":"chapter_natural-language-processing-applications/sentiment-analysis-rnn.md","ref":"chapter_natural-language-processing-applications/sentiment-analysis-rnn.md","articles":[]},{"title":"15.3. 情感分析：使用卷积神经网络","level":"1.2.18.3","depth":3,"path":"chapter_natural-language-processing-applications/sentiment-analysis-cnn.md","ref":"chapter_natural-language-processing-applications/sentiment-analysis-cnn.md","articles":[]},{"title":"15.4. 自然语言推断与数据集","level":"1.2.18.4","depth":3,"path":"chapter_natural-language-processing-applications/natural-language-inference-and-dataset.md","ref":"chapter_natural-language-processing-applications/natural-language-inference-and-dataset.md","articles":[]},{"title":"15.5. 自然语言推断：使用注意力","level":"1.2.18.5","depth":3,"path":"chapter_natural-language-processing-applications/natural-language-inference-attention.md","ref":"chapter_natural-language-processing-applications/natural-language-inference-attention.md","articles":[]},{"title":"15.6. 针对序列级和词元级应用微调BERT","level":"1.2.18.6","depth":3,"path":"chapter_natural-language-processing-applications/finetuning-bert.md","ref":"chapter_natural-language-processing-applications/finetuning-bert.md","articles":[]},{"title":"15.7. 自然语言推断：微调BERT","level":"1.2.18.7","depth":3,"path":"chapter_natural-language-processing-applications/natural-language-inference-bert.md","ref":"chapter_natural-language-processing-applications/natural-language-inference-bert.md","articles":[]}]},{"title":"16. 附录：深度学习工具","level":"1.2.19","depth":2,"path":"chapter_appendix-tools-for-deep-learning/index.md","ref":"chapter_appendix-tools-for-deep-learning/index.md","articles":[{"title":"16.1. 使用Jupyter Notebook","level":"1.2.19.1","depth":3,"path":"chapter_appendix-tools-for-deep-learning/jupyter.md","ref":"chapter_appendix-tools-for-deep-learning/jupyter.md","articles":[]},{"title":"16.2. 使用Amazon SageMaker","level":"1.2.19.2","depth":3,"path":"chapter_appendix-tools-for-deep-learning/sagemaker.md","ref":"chapter_appendix-tools-for-deep-learning/sagemaker.md","articles":[]},{"title":"16.3. 使用Amazon EC2实例","level":"1.2.19.3","depth":3,"path":"chapter_appendix-tools-for-deep-learning/aws.md","ref":"chapter_appendix-tools-for-deep-learning/aws.md","articles":[]},{"title":"16.4. 选择服务器和GPU","level":"1.2.19.4","depth":3,"path":"chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.md","ref":"chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.md","articles":[]},{"title":"16.5. 为本书做贡献","level":"1.2.19.5","depth":3,"path":"chapter_appendix-tools-for-deep-learning/contributing.md","ref":"chapter_appendix-tools-for-deep-learning/contributing.md","articles":[]},{"title":"16.6. d2l API 文档","level":"1.2.19.6","depth":3,"path":"chapter_appendix-tools-for-deep-learning/d2l.md","ref":"chapter_appendix-tools-for-deep-learning/d2l.md","articles":[]}]},{"title":"参考文献","level":"1.2.20","depth":2,"path":"chapter_references/zreferences.md","ref":"chapter_references/zreferences.md","articles":[]}]},"dir":"ltr"},"config":{"plugins":["-search","-livereload","-lunr","-fontsettings","highlight","expandable-chapters-small","back-to-top-button","github","code","theme-default"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"highlight":{"lang":{"eval_rst":"rst","toc":"text"}},"github":{"url":"https://github.com/KittenCN"},"expandable-chapters-small":{},"back-to-top-button":{},"code":{"copyButtons":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"Todd Lyu","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"CoderFAN 资料库","gitbook":"*","description":"CoderFAN 资料库 动手学深度学习"},"file":{"path":"chapter_preface/index.md","mtime":"2025-05-12T03:21:13.223Z","type":"markdown"},"gitbook":{"version":"6.0.3","time":"2025-05-12T03:23:46.668Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    

    </body>
</html>

