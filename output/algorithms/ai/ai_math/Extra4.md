# 番外-正则化、距离与范数
> 这篇番外来自于自己研究生期间《机器学习》课程的一次L1和L2正则化区别的探讨，将内容粘贴过来
## 一、求导问题：L2优于L1
1. 从求导的难度来看，L2正则化有连续可微的性质，容易求导
2. 而L1并不是处处连续可微，所以求导过程需要优化。
3. 一般使用「次梯度」进行优化，但是次梯度的求解速度很慢，而且优化后的解通常不会是稀疏的（相当于放弃了L1正则的最大优势），所以目前主要使用Proximal Algorithm算法进行优化。
4. 总体来说，L2正则化可以直接求导进行梯度下降，而L1正则需要进行优化，难度略高。

## 二、稀疏性讨论：L1优于L2
1. 从两种正则化的定义上看，L2正则化是参数平方和的平方根。这样看，L2正则化是先将参数的平方和看做一个整体，如此每个参数（的绝对值）可以很小，这样并没有减少参数的个数。原则上并没有降低模型的复杂度（如果将参数的个数看作模型的复杂度的话），只是可以将某些参数相对于另一些参数变得不重要了。
2. 而L1正则化是参数绝对值的和，每个参数都需要“独立”的减小，这样的结果会导致最后大量的参数会直接衰减到0，可以直观意义上减少参数的个数。
3. 补充说明一下L0正则，根据范数的定义，L0正则就是非零参数的个数，但是L0在优化过程中几乎难以实现，所以使用L1正则达到这个效果更为普遍。或者从另一个角度说，L0正则是我们的目的，L1正则是实现的方法，L2正则是退而求其次的方案（从减少参数个数以减小模型复杂度的角度来说）。

## 三、应用：LASSO和Ridge
1. LASSO回归和岭回归（Ridge）是对普通的线性回归做的扩展，其中LASSO回归是在普通的线性回归中添加了L1正则化项，正如我们在第二点中讨论的那样，L1正则化项有减少参数量（倾向于让某些参数直接衰减为0）的作用，LASSO回归在最小二乘的线性回归基础上兼具选择特征的作用（LASSO的全称，least absolute shrinkage and selection operator，其中的selection operator「选择算子」正是选择特征的意思）。
2. 而正如您所猜的那样，岭回归（Ridge）是在普通的线性回归中添加了L2正则化项，在参数较多的情况下，画出来的图像会有微小的「抖动｜变化，就像绵延的山岭一样（事实上，这也是Ridge Regression名称的来源）
3. 从名称本身上看，LASSO回归和岭回归（Ridge）的英文似乎也有L1和L2正则化的特点：LASSO给人一种「干净利落」的感觉，Ridge给人一种「绵延不绝」的感觉。

## 四、范数与距离：欧几里得距离和曼哈顿距离
1. 在讨论第二点稀疏性的时候，我们提到了范数的概念，实际上L1和L2正则化项就是1-范数和2-范数在机器学习中防止模型过拟合发展的方案。
2. 而范数，线性代数和数学意义上的范数，是指空间中向量的投影。说到投影我们可能想到了距离，范数最常见的应用也确实是距离上的表示。
3. 我们常提到的闵可夫斯基距离就是范数的距离表示，而当p=2时我们就得到了欧几里得距离，也就是最常说的距离（平方和开根号），这也就是L2正则化项的表示。
4. 而当p=1时，我们就得到了曼哈顿距离（取名自曼哈顿横平竖直的街区，如果是中国人命名可能会是“田字距离”），这也是L1正则化项的表示。

