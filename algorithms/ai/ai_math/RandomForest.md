---
description: CoderFAN 资料库 常见机器学习算法的数学推导
---


# 随机森林
**约定：**
- <font color=F66A65>红色：</font>对名词进行解释的重点
- <font color=FDA63E>黄色：</font>各部分总结中的重点
- <font color=62D257>绿色：</font>算法基本假设与解释

**摘要：**
1. 随机森林的两部分【重点是「随机」的部分】（[随机](#21-随机)、[森林](#22-森林)）
1. 集成学习+随机带来的额外的评价方法（[OOB](#三算法评价（OOB）)）

**名词解释：**``集成学习`、`Bagging/Stacking/Boosting`、`OOB`
> 1. 集成学习：表示一类算法，该算法并不强调某种算法，而是某种思想：<font color=F66A65>通过将多个弱分类器组合成为一个较强的分类器</font>
> 2. Bagging/Stacking/Boosting：三种集成学习的「组装」思想，通过将<font color=F66A65>弱分类器怎样的架构排列得到最终的结果</font>的三种方法
> 3. OOB：Out of Bag，对于Bagging算法特有的一种<font color=F66A65>选取样本做测试的方法</font>（选那些未被选取作为训练数据的样本）
* 注：Bagging、Stacking和Boosting都是集成学习中组合若分类器的一种方式在[番外-集成学习的讨论](/algorithms/ai/ai_math/Extra5.md)中详细解释了三种思想的对比。

## 一、一句话概括
集成学习中最出名的算法，也是一种典型的bagging方法：通过随机的方法构建多个决策树组成森林，进行投票表决的方法得到最终的结果。有放回抽样随机选择样本数据，随机选择特征，通过这两种随机保证各个基础的弱分类器之间的差异。

## 二、随机森林
关于随机森林有一个有意思的假设：其假设<font color=62D257>各个基础模型的预测值“错落有致”地分布在真实值的周围</font>，也就是说把这些预测值平均一下，就可以稳定地得到一个比较准确的预测值。
### 2.1 随机
1. 有放回的随机选取样本(bootstrap过程)
2. 随机选取特征
### 2.2 森林
1. 标准的随机森林中选取CART树（既能回归也能分类，而且计算量相对较小）作为基础分类器
2. 理论上来说也可以选取其他的弱分类器，bagging的思想就是“广撒网”希望多捞鱼
### 2.3 算法过程
1. 样本随机选：有放回的抽样（n）这个过程叫做boostrap
2. 特征随机选：k个  k一般是 log2（总特征个数）
3. 重复m次，得到m个CART（不剪枝），进行投票
## 三、算法评价（OOB）
### 3.1 OOB问题
OOB：是一种由于有限次的有放回随机取样导致有一部分样本永远不会被取到（概率上）的问题（1/3左右）
### 3.2 通过OOB挑选“测试集”
> OOB包外估计：是一种效率高无偏估计，近似于k折交叉验证
1. 以树为单位的计算\
    a. 树的OOB误分率：每棵树没有选中的那些样本的错误分类的占比\
    b. 随机森林的误分率：所有树误分率的平均
2. 以样本为单位的计算
    a. 对任何一个样本来说，有2/3的树选了它，1/3的树没选它\
    b. 没选它的那 1/3个数 对这个样本做分类

**OOB在随机森林中的总结**
> 1. 由于OOB问题的存在，可以全部数据用于训练，直接使用OOB问题中没有见过的样本测试就行
## 四、总结
> 1. 两种<font color=FDA63E>「随机」保证各个树不一样</font>，<font color=FDA63E>「森林」说明很多决策树</font>
> 2. 随机森林属于bagging，bagging类的算法说明各个弱分类器之间是没有关联的（<font color=FDA63E>可以并行训练）