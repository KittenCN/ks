## 一、Bagging
1. Bagging = boostrap(有放回抽样) + aggregation(聚集)\
    把(相同的)若干基础模型简单的“装起来”: 基础模型独立训练
2. 随机森林可以理解为Bagging的一种具体实现，所以后面的讨论也可以看到随机森林的影子
### 1.1 有趣的假设
假设1. 各个基础模型的预测值“错落有致”，分布在真实值的周围——把这些预测值平均一下，就可以稳定地得到一个比较准确的预测值
假设2. 每个模型都判断错误的概率很低
### 1.2 模型的差异性
1. 如果每个模型60%的准确率，按照少数服从多数的情况  500个模型的二分类问题：
   $$\sum_{i=251}^{500} C_{500}^{i} \cdot 0.6^{i} \cdot 0.4^{500-i}=99.999%$$
> 上述表达式成立的关键在于模型之间要有差异性（独立性）
2. 如何让模型体现差异性\
    a. 样本差异：Random samples
    b. 特征差异：Random subspaces\
    c. 样本差异和特征差异：Random Patches
> 延伸：样本差异体现在：每个模型只看样本的一部分
> 1. 有放回抽样--->bagging(bootstrap)
> 2. 无放回抽样--->pasting
### 1.3 bagging讨论
1. 随机带来的OOB问题：\
    a. 有限次的有放回取样会导致一部分样本是无法取到的：37%左右
    b. 全部数据用于训练，直接使用OOB问题中没有见过的样本测试就行
2. 并行化\
    bagging的思路是非常容易并行化的

## 二、Stacking
个人认为stacking是一种比较难讲清楚的思想，这里不尝试把它讲清楚，而是列举出自己理解的三个角度，相信看完你会有一个自己的想法。其本质就是模型叠加：第一层的输出是第二层的输入，第二层的输出才是结果（如果只有两层的话）
### 角度1: 一种soft的bagging
1. 考虑bagging的问题：不同的弱分类器拥有同样的投票权（一人一票）似乎是不合理的
2. 考虑给bagging中的分类器一个权重（是否正确率越高权重越大呢）
3. 也许可以，stacking走了另一个思路：让第二层模型学习这个权重
4. 所以stacking的第一层是训练模型，后面每一层都是对前面作为整体的模型的评价（权重）

**stacking的每一层都在学习前面作为整体模型的权重：**
> 1. soft的bagging会赋一个权重，基于模型的概率越大认为把握越大
> 2. 特定算法把这个“把握” 变成权重。stacking的后一层要去学习这个把握
> 3. 即：把所有的概率再扔进第二层模型，让第二层模型根据这些概率学习分类方法
> 4. 进一步的：第二层的结果也是概率，可以再摞一层
### 角度2: 深度学习的前传
这个角度是在李宏毅的视频中看到的，视频中进行了如下的思维推导：\
1. 逻辑回归是一种要求数据线性可分的分类方式
2. 如果数据线性不可分，可以对于数据进行某种变换使得数据线性可分
3. 此时可以再使用逻辑回归
4. 视频中李宏毅教授将这两个模型迭加起来（sigmoid就变成了激活函数，整个就变成了3层的深度学习模型）

**stacking的每一层类似深度学习的“层”**
> 1. 这个理解中，sigmoid作为激活函数的想法更加的清晰和显然
### 角度3: 数据预处理
这个角度仅举一个例子说明：\
1. 背景：一个“找不同游戏”的辅助脚本（点击两副照片中不同的地方）
2. 思路：1. 通过像素比较找到不同点  2. 通过聚类算法找到不同聚集后的不同点
3. 问题：直接K-means会被一些噪点影响，使用DBSCAN这种基于密度的算法又无法做到找出5个（游戏规定有5个不同）
4. 方案：所以采用DBSCAN先对数据进行聚类，然后K-means对 聚类后的点再次聚类

**stacking的每一层都是前一层的数据预处理：**
> 1. 这个角度和「深度学习的前传」这个思路很像
> 2. 不同点在于每一层会有相对明确的任务，后续也许可以每一层单独训练

## 三、Boosting
相比之前的Bagging和Stacking，Boosting走了一个极端，每个模型都是有依赖且基于前面的模型有“提升”的
### 3.1 提升
首先“提升”：即每个个体学习器都在弥补集成学习器的欠缺。但是对于提升的具体理解是多种多样的，也是不多说，直接从比较经典的两个算法讲起
#### 3.1.1 AdaBoost
适应性提升，相比下一种GBDT，这里的AdaBoost提供的更多的是一种思想：
1. 每次的迭代中的弱分类器虽然拟合的都是全部的数据，但是评判的标准是不同的\
    **每次迭代改变的是样本分布**
2. 当上一个弱分类器没有预测对的数据的权重会提升（影响下一个分类器的评价标准：误分率）
3. 所以每个分类器都是一个单独的分类器，重点是要拟合上一个分类器分错的样本（因为如果错了，惩罚会更高）

**提升总结**
1. 这里每次迭代的弱分类器都是独立的一个模型，只是重点关注了上一个模型错误的内容\
    a. 如果每次并没有调整样本的权重，且得到的模型没有权重的话，就是随机森林
2. 不同模型中间的“提升”在于关注的重点不同，很难讲最后一个模型就一定比第一个模型要好，但是从经验来说是这样的，AdaBoost经过多轮，似乎能够发现一批数据集中的重点样本
3. 提升：通过样本的权重记录每次模型，每次的弱分类器都可以看到历史的所有结果（之前的分类器在数据集上留下的痕迹）相当于题目还在，学生换了一届又一届，只是为了做对上一届做错的题目

#### 3.1.2 GBDT
梯度提升决策树，和AdaBoost最大的不同就是，GBDT的每一棵树都不能单独作为一个分类器，因为每一棵树（除了第一棵）看不到所有的数据
1. GBDT中将“决策树”写到了名字里（梯度提升决策树），所以相比AdaBoost，GBDT规定了若分类器是决策树（通常是CART）
2. 每次迭代的任务是：用第K个CART拟合前k-1个CART留下的残差，最终让所有的模型同时预测互相弥补\
    a. 一般来说为了防止第一棵树学到太多可能是噪声的东西，会非常早的时候就会停止\
    b. 所以GBDT的第一棵树和普通的CART比性能会差很多
3. 也就是说，最终的预测是必须要集合所有模型的结果

#### 3.1.3 番外：XGBoost
和之前的两种树相比，没有提出什么太多的思想，而是在工程上进行了大量的优化
1. 优点\
    a. 利用二阶梯度对节点进行划分，比GBDT和GBM 精度高\
    b. 利用局部近似算法对分裂节点的贪心算法优化\
    c. 损失函数中有L1、L2\
    d. 提供并行计算能力：求不同的候选的分裂点时\
    e. 树压缩，列的子采样\
2. 缺点\
    a. 需要 pre-sorted  很耗内存\
    b. 遍历次数为data*features 分裂到左右两个节点\
    c. pre-sorted还会产生大量的cache随机访问

#### 3.1.4 番外：LightGBM
2017年，微软开源，也是工程上的优化，主要是针对XGBoost的内存消耗大做出的改进。速度快，内存消耗少   核心划分时不是考虑gini系数或者熵，而是考虑叶子
1. 使用histogram 替代 XGBoost的pre-sorted，提高cache命中率
2. GOSS算法来对样本进行采样：相当于对样本进行不同权重的分配（模仿AdaBoost）