
# 线性回归
**约定：**
- <font color=F66A65>红色：</font>对名词进行解释的重点
- <font color=FDA63E>黄色：</font>各部分总结中的重点
- <font color=62D257>绿色：</font>算法基本假设与解释

**摘要：**
1. 两种得到损失函数的方法（[最小二乘法](#2.1-最小二乘法-LMS)、[极大似然估计](#2.2-极大似然估计)）
2. 两种求解损失函数的方法（[正规解](#31-正规解)、[梯度下降](#32-梯度下降)）

**名词解释：**`MSE`、`似然函数`、`极大似然估计`、`对数似然函数`、`正规解`
> 1. MSE：均方误差，线性回归常用的<font color=F66A65>损失函数</font>
> 2. 似然函数：输入 $\theta$参数空间 和 $X$输入空间，输出是样本分类概率$P$的函数: <font color=F66A65>$P(y|X,\theta)$</font>
> 3. 极大似然估计: 一种通过最大化所有样本的似然函数连乘起来的表达式，来<font color=F66A65>估计参数的取值的方法</font>
> 4. 对数似然函数：连乘的函数不好求最大，所以取对数，变成连加，<font color=F66A65>方便求最大值</font>
> 5. 正规解：线性回归常用的损失函数是可以求出解析解的，在<font color=F66A65>矩阵形式表达的“解析解”</font>称为正规解
* 注：在[番外-似然函数：一个摸小球的例子](/algorithms/ai/ai_math/Extra2.md) 中举了一个实际的例子说明了「似然函数」和「极大似然估计」的应用，建议看完这一篇，可以从另一个角度理解。

**原理实现：**
[基于梯度下降的线性回归]()

## 一、线性回归目的
找到一个线性模型对已知数据进行线性拟合，并期望能很好的泛化到未知的数据上
## 二、线性回归损失函数
$$H=\frac{1}{2}\sum^N_{i=0}(Xw-y)^2$$
以下两种理解这个公式的方法（2种推导过程）
### 2.1 最小二乘法 LMS
1. 预测值 $Xw$ 和 真实值 $y$ 的距离: $(Xw-y)$
2. 直接相加，预测的偏离方向会被掩盖（一正一负会湮灭）所以取平方 $(Xw-y)^2$ \
   不取绝对值是因为不好求梯度，原则上都喜欢使用平方进行中和
3. $\frac{1}{2}$ 也是因为求梯度方便（中和掉平方梯度的2系数） 本质上没有区别
> 注：二乘是平方的意思
### 2.2 极大似然估计
#### 2.2.1 背景补充：似然函数 likelyhood function
似然函数：是一种函数，它的输入是 $\theta$参数空间 和 $X$输入空间，它的输出是样本分类概率$P$。也就是说，似然函数是: $P(y|X,\theta)$

对以上概念的理解过程：
1. 正常求一个已知分布的概率过程，直接找到x，在坐标上描点即可\
   这样的函数我们称之为「概率密度函数」
2. 对于要求解的损失函数，我们并不知道具体的分布（因为参数需要求），所以没法画出概率密度函数\
   所以求解概率的输入还需要考虑“影响概率密度函数的参数”

**似然函数总结**
> <font color=FDA63E>似然函数是跳过得到完整分布的过程直接求概率的函数</font>（因此还需要参数作为输入）
#### 2.2.2 背景补充：极大似然估计假设
1. 通过最大化似然函数进行参数估计的方法，称之为极大似然估计
2. 上面提到的似然函数是各个样本的似然函数的乘积（相当于一个总的似然函数）

为什么最大化各样本似然函数的乘积会有用？这里其实依赖了极大似然估计的一个基本假设：
<center><font color=62D257>一件事情发生了，一定是可能性最大的那件事发生了才合理</font></center>

这句话似乎没什么问题，我们一点点带入看这个过程：
1. <font color=62D257>一件事情发生了：</font>于第$i$个样本来说，这个样本具备了所有特征$X_i$之后，成为了标签$y_i$，这个概率是它的似然函数
2. <font color=62D257>一件事情发生了：</font>对于样本空间来说，各个样本成为我们采集的样子这件事的概率，就是每个样本的似然函数的乘积（因为样本间有独立同分布的假设）
3. <font color=62D257>可能性最大的那件事发生了：</font>我们让整个乘起来的表达式最大，就是「可能性最大的那件事」
> 注：样本间的独立同分布的假设是监督学习能够通过样本学到总体分布的一个基本假设（大家得来自于一个分布）
#### 2.2.3 极大似然估计过程
1. 前提条件：$y^{(i)} = \theta^Tx^{(i)}+\epsilon^{(i)}​$      

   其中  $\epsilon^{(i)}$   是一个独立并且服从均值为0方差 $\theta^2$ 的分布

2. 服从均值为0方差 $\theta^2$ 的分布可以得到：

   $p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})​$ 

3. 带入  $\epsilon^{(i)} = y^{(i)}-\theta^Tx^{(i)} $   得

   $p\left(y^{(i)} \mid x^{(i)} ; \theta\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)​$     也就是说找到一组 $\theta​$ 和 $x^{(i)}​$  组合可以使得 $y^{i}​$ 的概率最大

4. 引入似然函数

   $L(\theta) = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)​$

5. 对似然函数取对数：方便求最大值

   $logL(\theta)=\sum_{i=1}^mlog\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)$

   $logL(\theta) = m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}$          去掉前面的常数项

   $logL(\theta)=\frac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$                                      得到最终损失函数

**损失函数总结** 
> 1. 对于线性回归来说，<font color=FDA63E>通过极大似然估计得到的损失函数也就是MSE</font>
> 2. 对于线性回归来说，<font color=FDA63E>极大似然估计需要大前提</font>：
  ​        $y^{(i)} = \theta^Tx^{(i)}+\epsilon^{(i)}$         其中 $\epsilon^{(i)}$   是一个独立并且服从均值为0方差 $\theta^2$ 的分布
## 三、损失函数求解
### 3.1 正规解
1. 使得 $H=\sum^N_{i=0}(Xw-y)^2​$   最小

2. 求导：令导函数为零                              复合函数求导

   $\frac{H}{\partial{w}} = 2(Xw-y)X=====0$

3. 想要同乘$x^{-1}​$ ：消掉左侧的X                 $XX^{-1}=0​$       注意$X^{-1}​$不一定存在  必须满秩矩阵才有逆矩阵

   $(Xw-y)XX^{-1} = 0X^{-1}$      不能直接这么写

4. 解决3的问题：$XX^{T}​$ 得到方阵，方阵就有逆矩阵

   $(Xw-y)XX^{T}\cdot (XX^{T})^{-1}=0\cdot X^{T}\cdot (XX^{T})^{-1}$

5. 化简左侧为1，右侧仍为0，得到等式

   $Xw=y​$ 

6. 此时想要同乘$x^{-1}$ ：消掉左侧的X，再次遇到3的问题，用同样的手法

   $(X^{T}X)^{-1}X^{T}\cdot Xw=(X^{T}X)^{-1}X^{T}\cdot y​$ 

7. 化简得到最后结果：

   $w=(X^{T}X)^{-1}X^{T}\cdot y​$

   ~~~python
   import numpy as np
   np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)   # np.linalg.inv 求逆
   ~~~

**正规解总结**
> 1. 对最小二乘法的损失函数进行复合函数的求导，令导数为0得到求解的入口
> 2. 为了化简掉x，外层和内层的X，需要两次使用 $X^{-1}​$  但是不一定有$X^{-1}​$ 需要引入 $X^{T}​$  和$X​$ 凑方阵
> 3. 记住最后化简的公式：$Xw=y$  <font color=FDA63E>为了消掉$X$ 得到  $w=(X^{T}X)^{-1}X^{T}\cdot y$</font>  这么复杂的结果

### 3.2 梯度下降
1. 梯度下降是一种在知道函数表达式，无法借助图像（参数量庞大）求出全局最优解的情况下尝试求解最小值的实用方法。
2. 深度学习中梯度下降十分常用，其基于微积分中的2种链式求导法则（「加法原理」和「乘法原理」）
3. 通常情况下梯度下降容易陷入局部最优的情况，所以有很多的AdaXXX优化方法避免这种情况，然而线性回归是为数不多的局部最优解也是全局最优解的常用算法
> 注：在我另一个仓库“机器学习算法手写实现”中，简单实现了[基于梯度下降的线性回归]()
## 四、总结
> 1. 作为一个入门级别的算法，线性回归也是<font color=FDA63E>为数不多存在解析解的一个算法</font>
> 2. 线性回归的总体实现较为简单，但是在梳理的过程中，本篇引入了很多很有用的工具，包括「极大似然估计」和「梯度下降」

